{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MAX_POSSIBLE_DIST]: 0.4428093360578569\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "import utils\n",
    "import torch\n",
    "import constants\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from env import Env\n",
    "from agent import Agent\n",
    "from torchsummary import summary\n",
    "from torch.distributions import Normal, Categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialise Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "workspace space: \n",
      "[[-0.26   0.04 ]\n",
      " [ 0.435  0.685]\n",
      " [ 0.     0.4  ]]\n"
     ]
    }
   ],
   "source": [
    "#initialise environment\n",
    "min_x, max_x =  -0.110 - 0.150,   -0.110 + 0.150\n",
    "min_y, max_y =   0.560 - 0.125,    0.560 + 0.125\n",
    "min_z, max_z =               0,              0.4 \n",
    "\n",
    "workspace_lim = np.asarray([[min_x, max_x], \n",
    "                            [min_y, max_y],\n",
    "                            [min_z, max_z]])\n",
    "\n",
    "print(f\"workspace space: \\n{workspace_lim}\")\n",
    "\n",
    "obj_dir = 'objects/blocks/'\n",
    "N_obj   = 5\n",
    "\n",
    "env = Env(obj_dir, N_obj, workspace_lim, cluttered_mode= True, is_debug = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialise Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n",
      "[SUCCESS] initialise environment\n",
      "[SUCCESS] initialise networks\n"
     ]
    }
   ],
   "source": [
    "agent = Agent(env, \n",
    "              max_memory_size = 100000, \n",
    "              max_memory_size_rl = 200000,\n",
    "              max_memory_size_hld = 50000,\n",
    "              is_debug = True, \n",
    "              N_batch = 512, \n",
    "              N_batch_hld = 512, \n",
    "              lr = 1e-4, \n",
    "              hld_lr = 1e-4,\n",
    "              tau = 0.05,\n",
    "              tau_hld = 0.05,\n",
    "              max_action_taken = 50,\n",
    "              max_result_window = 500,\n",
    "              max_result_window_hld = 250,\n",
    "              max_result_window_eval = 100,\n",
    "              max_stage1_episode = 200,\n",
    "              N_grasp_step = 25, #define the maximum step for low-level grasping network\n",
    "              N_push_step = 25, #define the maximum step for low-level pushing network\n",
    "              success_rate_threshold = 0.7,\n",
    "              checkpt_dir_agent=\"/media/ryan/Seagate/research_proj_backup/research_2.0/logs/agent\", \n",
    "              checkpt_dir_models=\"/media/ryan/Seagate/research_proj_backup/research_2.0/logs/models\",\n",
    "              exp_dir_expert=\"/media/ryan/Seagate/research_proj_backup/research_2.0/logs/exp_expert\", \n",
    "              exp_dir_rl=\"/media/ryan/Seagate/research_proj_backup/research_2.0/logs/exp_rl\",\n",
    "              exp_dir_hld=\"/media/ryan/Seagate/research_proj_backup/research_2.0/logs/exp_hld\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[FAIL] load agent data\n",
      "[LOAD MODEL] load hld-net best model\n",
      "[SUCCESS] load hld-net model\n",
      "[LOAD MODEL] load grasp best model\n",
      "[SUCCESS] load grasp model\n",
      "[LOAD MODEL] load push best model\n",
      "[SUCCESS] load push model\n",
      "[SUCCESS] restart environment\n",
      "['4.obj']\n",
      "item 0: shape_0, path index: 0, pose: [-0.10865292444715081, 0.5491353944575486, 0.025, 5.557250096537644, 6.226631968802069, 6.256439831366729]\n",
      "item 1: shape_1, path index: 0, pose: [-0.12634818821307028, 0.565900417477067, 0.025, 6.191640873787224, 3.3715125222725364, 5.1476114281371075]\n",
      "item 2: shape_2, path index: 0, pose: [-0.10030092645668963, 0.5425546420294677, 0.025, 1.5841684966030367, 0.9255529218462992, 4.307595578946903]\n",
      "item 3: shape_3, path index: 0, pose: [-0.12248322420313101, 0.5739700659258956, 0.025, 2.1719391272122524, 5.832415683477804, 1.1523739033974778]\n",
      "item 4: shape_4, path index: 0, pose: [-0.09935437662928209, 0.5624633182373571, 0.025, 0.20418611703964468, 0.4669409307435969, 3.7780387731519913]\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "==== episode: 0 ====\n",
      "[MIN_DISTANCE] 0.018587057427491827\n",
      "[compute_push_path]: 0.05000295694148177\n",
      "N_step_x: 4.0, N_step_y: 5.0, N_step_z: 10.0, N_step_yaw: 11.0, N_step: 11.0\n",
      "[MIN_DISTANCE] 0.02413762791581618\n",
      "[MIN_DISTANCE] 0.02413762791581618\n",
      "[MIN_DISTANCE] 0.018497884882690212\n",
      "[MIN_DISTANCE] 0.044915024310237264\n",
      "N_step_x: 6.0, N_step_y: 12.0, N_step_z: 10.0, N_step_yaw: 5.0, N_step: 12.0\n",
      "[HLD net] max output: 0\n",
      "N_step_low_level: 25\n",
      "==== AGENT MODE ENABLE BC: False ENABLE RL: False FULL TRAIN: False ====\n",
      "==== low level action taken: 0 N_pickable_item: 5 ====\n",
      "[SUCCESS] get raw data\n",
      "Qx: tensor([[0.3753, 0.3991, 0.4712]], device='cuda:0')\n",
      "Qy: tensor([[0.3254, 0.3856, 0.4383]], device='cuda:0')\n",
      "Qz: tensor([[0.5388, 0.2538, 0.2006]], device='cuda:0')\n",
      "Qyaw: tensor([[0.2588, 0.2265, 0.1846]], device='cuda:0')\n",
      "[MAX OUTPUT]\n",
      "[Q net] x: 2, y: 2, z: 0, yaw_ind: 0\n",
      "[SUCCESS] estimate actions from network\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[GRASP DISTANCE] 0.0\n",
      "[GRASP REWARD] R: 0.0\n",
      "[OVERALL REWARD] 0.0\n",
      "[ACTION TYPE]: 0\n",
      "Qx: tensor([[0.4269, 0.4950, 0.4495]], device='cuda:0')\n",
      "Qy: tensor([[0.2812, 0.3902, 0.4374]], device='cuda:0')\n",
      "Qz: tensor([[0.3949, 0.3409, 0.2090]], device='cuda:0')\n",
      "Qyaw: tensor([[0.2659, 0.2611, 0.2302]], device='cuda:0')\n",
      "[MAX OUTPUT]\n",
      "[Q net] x: 1, y: 2, z: 0, yaw_ind: 0\n",
      "[MOVE] move: [ 0.015      0.015     -0.02      -0.1308997] move index: [2, 2, 0, 0] next move index: [1, 2, 0, 0]\n",
      "target_Qx: 0.4968489706516266 Qx: 0.47120970487594604\n",
      "target_Qy: 0.43556341528892517 Qy: 0.43834519386291504\n",
      "target_Qz: 0.3997213840484619 Qz: 0.5387861728668213\n",
      "target_Qyaw: 0.25178343057632446 Qyaw: 0.25877270102500916\n",
      "loss_rank: None\n",
      "priority: 0.0050132437609136105\n",
      "[SUCCESS] append transition experience\n",
      "==== AGENT MODE ENABLE BC: False ENABLE RL: False FULL TRAIN: False ====\n",
      "==== low level action taken: 0 N_pickable_item: 5 ====\n",
      "[SUCCESS] get raw data\n",
      "Qx: tensor([[0.4255, 0.4974, 0.4545]], device='cuda:0')\n",
      "Qy: tensor([[0.2792, 0.3876, 0.4361]], device='cuda:0')\n",
      "Qz: tensor([[0.3944, 0.3399, 0.2111]], device='cuda:0')\n",
      "Qyaw: tensor([[0.2684, 0.2675, 0.2283]], device='cuda:0')\n",
      "[MAX OUTPUT]\n",
      "[Q net] x: 1, y: 2, z: 0, yaw_ind: 0\n",
      "[SUCCESS] estimate actions from network\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[GRASP DISTANCE] 0.0\n",
      "[GRASP REWARD] R: 0.0\n",
      "[OVERALL REWARD] 0.0\n",
      "[ACTION TYPE]: 0\n",
      "Qx: tensor([[0.4932, 0.5658, 0.4382]], device='cuda:0')\n",
      "Qy: tensor([[0.4158, 0.4115, 0.4218]], device='cuda:0')\n",
      "Qz: tensor([[0.4474, 0.3596, 0.3366]], device='cuda:0')\n",
      "Qyaw: tensor([[0.2099, 0.2688, 0.1499]], device='cuda:0')\n",
      "[MAX OUTPUT]\n",
      "[Q net] x: 1, y: 2, z: 0, yaw_ind: 1\n",
      "[MOVE] move: [ 0.         0.015     -0.02      -0.1308997] move index: [1, 2, 0, 0] next move index: [1, 2, 0, 1]\n",
      "target_Qx: 0.56329345703125 Qx: 0.4974276125431061\n",
      "target_Qy: 0.38279110193252563 Qy: 0.4360694885253906\n",
      "target_Qz: 0.4385669231414795 Qz: 0.3943560719490051\n",
      "target_Qyaw: 0.249888613820076 Qyaw: 0.26835504174232483\n",
      "loss_rank: None\n",
      "priority: 0.0023681260645389557\n",
      "[SUCCESS] append transition experience\n",
      "==== AGENT MODE ENABLE BC: False ENABLE RL: False FULL TRAIN: False ====\n",
      "==== low level action taken: 0 N_pickable_item: 5 ====\n",
      "[SUCCESS] get raw data\n",
      "Qx: tensor([[0.4916, 0.5677, 0.4395]], device='cuda:0')\n",
      "Qy: tensor([[0.4171, 0.4059, 0.4282]], device='cuda:0')\n",
      "Qz: tensor([[0.4475, 0.3599, 0.3332]], device='cuda:0')\n",
      "Qyaw: tensor([[0.2070, 0.2689, 0.1505]], device='cuda:0')\n",
      "[MAX OUTPUT]\n",
      "[Q net] x: 1, y: 2, z: 0, yaw_ind: 1\n",
      "[SUCCESS] estimate actions from network\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[GRASP DISTANCE] 0.0\n",
      "[GRASP REWARD] R: 0.0\n",
      "[OVERALL REWARD] 0.0\n",
      "[ACTION TYPE]: 0\n",
      "Qx: tensor([[0.6089, 0.4780, 0.4727]], device='cuda:0')\n",
      "Qy: tensor([[0.4794, 0.3175, 0.4388]], device='cuda:0')\n",
      "Qz: tensor([[0.4363, 0.4323, 0.3929]], device='cuda:0')\n",
      "Qyaw: tensor([[0.4087, 0.3874, 0.2738]], device='cuda:0')\n",
      "[MAX OUTPUT]\n",
      "[Q net] x: 0, y: 0, z: 0, yaw_ind: 0\n",
      "[MOVE] move: [ 0.     0.015 -0.02   0.   ] move index: [1, 2, 0, 1] next move index: [0, 0, 0, 0]\n",
      "target_Qx: 0.6103284358978271 Qx: 0.5676745772361755\n",
      "target_Qy: 0.47874459624290466 Qy: 0.42818737030029297\n",
      "target_Qz: 0.43948835134506226 Qz: 0.4474793076515198\n",
      "target_Qyaw: 0.42501750588417053 Qyaw: 0.2688656151294708\n",
      "loss_rank: None\n",
      "priority: 0.007205663248896599\n",
      "[SUCCESS] append transition experience\n",
      "==== AGENT MODE ENABLE BC: False ENABLE RL: False FULL TRAIN: False ====\n",
      "==== low level action taken: 0 N_pickable_item: 5 ====\n",
      "[SUCCESS] get raw data\n",
      "Qx: tensor([[0.6056, 0.4703, 0.4782]], device='cuda:0')\n",
      "Qy: tensor([[0.4757, 0.3091, 0.4320]], device='cuda:0')\n",
      "Qz: tensor([[0.4383, 0.4295, 0.3953]], device='cuda:0')\n",
      "Qyaw: tensor([[0.3988, 0.3821, 0.2699]], device='cuda:0')\n",
      "[MAX OUTPUT]\n",
      "[Q net] x: 0, y: 0, z: 0, yaw_ind: 0\n",
      "[SUCCESS] estimate actions from network\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[GRASP DISTANCE] 0.0\n",
      "[GRASP REWARD] R: 0.0\n",
      "[OVERALL REWARD] 0.0\n",
      "[ACTION TYPE]: 0\n",
      "Qx: tensor([[0.5152, 0.5380, 0.6793]], device='cuda:0')\n",
      "Qy: tensor([[0.6203, 0.5190, 0.5296]], device='cuda:0')\n",
      "Qz: tensor([[0.5415, 0.5013, 0.3121]], device='cuda:0')\n",
      "Qyaw: tensor([[0.4142, 0.4411, 0.3189]], device='cuda:0')\n",
      "[MAX OUTPUT]\n",
      "[Q net] x: 2, y: 0, z: 0, yaw_ind: 1\n",
      "[MOVE] move: [-0.015     -0.015     -0.02      -0.1308997] move index: [0, 0, 0, 0] next move index: [2, 0, 0, 1]\n",
      "target_Qx: 0.676645815372467 Qx: 0.6056253910064697\n",
      "target_Qy: 0.6300494074821472 Qy: 0.47571951150894165\n",
      "target_Qz: 0.541994035243988 Qz: 0.43828022480010986\n",
      "target_Qyaw: 0.4556303322315216 Qyaw: 0.39882561564445496\n",
      "loss_rank: None\n",
      "priority: 0.010711236856877804\n",
      "[SUCCESS] append transition experience\n",
      "==== AGENT MODE ENABLE BC: False ENABLE RL: False FULL TRAIN: False ====\n",
      "==== low level action taken: 0 N_pickable_item: 5 ====\n",
      "[SUCCESS] get raw data\n",
      "Qx: tensor([[0.5159, 0.5436, 0.6794]], device='cuda:0')\n",
      "Qy: tensor([[0.6296, 0.5457, 0.5344]], device='cuda:0')\n",
      "Qz: tensor([[0.5257, 0.4946, 0.3042]], device='cuda:0')\n",
      "Qyaw: tensor([[0.4435, 0.4550, 0.3407]], device='cuda:0')\n",
      "[MAX OUTPUT]\n",
      "[Q net] x: 2, y: 0, z: 0, yaw_ind: 1\n",
      "[SUCCESS] estimate actions from network\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[GRASP DISTANCE] 0.0\n",
      "[GRASP REWARD] R: 0.0\n",
      "[OVERALL REWARD] 0.0\n",
      "[ACTION TYPE]: 0\n",
      "Qx: tensor([[0.3278, 0.4421, 0.2725]], device='cuda:0')\n",
      "Qy: tensor([[0.3721, 0.3264, 0.3662]], device='cuda:0')\n",
      "Qz: tensor([[0.3848, 0.3533, 0.2995]], device='cuda:0')\n",
      "Qyaw: tensor([[0.2356, 0.3656, 0.2475]], device='cuda:0')\n",
      "[MAX OUTPUT]\n",
      "[Q net] x: 1, y: 0, z: 0, yaw_ind: 1\n",
      "[MOVE] move: [ 0.015 -0.015 -0.02   0.   ] move index: [2, 0, 0, 1] next move index: [1, 0, 0, 1]\n",
      "target_Qx: 0.44098135828971863 Qx: 0.6794276833534241\n",
      "target_Qy: 0.3629607856273651 Qy: 0.6296192407608032\n",
      "target_Qz: 0.3749324679374695 Qz: 0.525719404220581\n",
      "target_Qyaw: 0.3579084575176239 Qyaw: 0.45500051975250244\n",
      "loss_rank: None\n",
      "priority: 0.040031734853982925\n",
      "[SUCCESS] append transition experience\n",
      "==== AGENT MODE ENABLE BC: False ENABLE RL: False FULL TRAIN: False ====\n",
      "==== low level action taken: 0 N_pickable_item: 5 ====\n",
      "[SUCCESS] get raw data\n",
      "Qx: tensor([[0.3271, 0.4422, 0.2712]], device='cuda:0')\n",
      "Qy: tensor([[0.3724, 0.3267, 0.3661]], device='cuda:0')\n",
      "Qz: tensor([[0.3849, 0.3535, 0.2996]], device='cuda:0')\n",
      "Qyaw: tensor([[0.2371, 0.3659, 0.2473]], device='cuda:0')\n",
      "[RANDOM OUTPUT]\n",
      "[Q net] x: 2, y: 0, z: 1, yaw_ind: 2\n",
      "[SUCCESS] estimate actions from network\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[GRASP DISTANCE] 0.0\n",
      "[GRASP REWARD] R: 0.0\n",
      "[OVERALL REWARD] 0.0\n",
      "[ACTION TYPE]: 0\n",
      "Qx: tensor([[0.5829, 0.6377, 0.4935]], device='cuda:0')\n",
      "Qy: tensor([[0.5679, 0.6658, 0.6284]], device='cuda:0')\n",
      "Qz: tensor([[0.7593, 0.4218, 0.5156]], device='cuda:0')\n",
      "Qyaw: tensor([[0.6087, 0.6044, 0.4282]], device='cuda:0')\n",
      "[MAX OUTPUT]\n",
      "[Q net] x: 1, y: 1, z: 0, yaw_ind: 0\n",
      "[MOVE] move: [ 0.015     -0.015      0.         0.1308997] move index: [2, 0, 1, 2] next move index: [1, 1, 0, 0]\n",
      "target_Qx: 0.6563921570777893 Qx: 0.27122122049331665\n",
      "target_Qy: 0.6804863214492798 Qy: 0.37242579460144043\n",
      "target_Qz: 0.7598660588264465 Qz: 0.3534557521343231\n",
      "target_Qyaw: 0.6078417301177979 Qyaw: 0.24731023609638214\n",
      "loss_rank: None\n",
      "priority: 0.13460256159305573\n",
      "[SUCCESS] append transition experience\n",
      "==== AGENT MODE ENABLE BC: False ENABLE RL: False FULL TRAIN: False ====\n",
      "==== low level action taken: 0 N_pickable_item: 5 ====\n",
      "[SUCCESS] get raw data\n",
      "Qx: tensor([[0.5834, 0.6352, 0.4941]], device='cuda:0')\n",
      "Qy: tensor([[0.5692, 0.6644, 0.6237]], device='cuda:0')\n",
      "Qz: tensor([[0.7544, 0.4177, 0.5110]], device='cuda:0')\n",
      "Qyaw: tensor([[0.6088, 0.6023, 0.4278]], device='cuda:0')\n",
      "[MAX OUTPUT]\n",
      "[Q net] x: 1, y: 1, z: 0, yaw_ind: 0\n",
      "[SUCCESS] estimate actions from network\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[GRASP DISTANCE] 0.0\n",
      "[GRASP REWARD] R: 0.0\n",
      "[OVERALL REWARD] 0.0\n",
      "[ACTION TYPE]: 0\n",
      "Qx: tensor([[0.3853, 0.5497, 0.2130]], device='cuda:0')\n",
      "Qy: tensor([[0.3851, 0.6948, 0.5400]], device='cuda:0')\n",
      "Qz: tensor([[0.5654, 0.4636, 0.3155]], device='cuda:0')\n",
      "Qyaw: tensor([[0.5227, 0.3944, 0.4252]], device='cuda:0')\n",
      "[MAX OUTPUT]\n",
      "[Q net] x: 1, y: 1, z: 0, yaw_ind: 0\n",
      "[MOVE] move: [ 0.         0.        -0.02      -0.1308997] move index: [1, 1, 0, 0] next move index: [1, 1, 0, 0]\n",
      "target_Qx: 0.5468406081199646 Qx: 0.6352419257164001\n",
      "target_Qy: 0.6821561455726624 Qy: 0.6644316911697388\n",
      "target_Qz: 0.5201300978660583 Qz: 0.7544172406196594\n",
      "target_Qyaw: 0.521441638469696 Qyaw: 0.6088244915008545\n",
      "loss_rank: None\n",
      "priority: 0.01766379550099373\n",
      "[SUCCESS] append transition experience\n",
      "==== AGENT MODE ENABLE BC: False ENABLE RL: False FULL TRAIN: False ====\n",
      "==== low level action taken: 0 N_pickable_item: 5 ====\n",
      "[SUCCESS] get raw data\n",
      "Qx: tensor([[0.3963, 0.5536, 0.2196]], device='cuda:0')\n",
      "Qy: tensor([[0.4010, 0.7002, 0.5463]], device='cuda:0')\n",
      "Qz: tensor([[0.5739, 0.4738, 0.3329]], device='cuda:0')\n",
      "Qyaw: tensor([[0.5306, 0.4000, 0.4307]], device='cuda:0')\n",
      "[MAX OUTPUT]\n",
      "[Q net] x: 1, y: 1, z: 0, yaw_ind: 0\n",
      "[SUCCESS] estimate actions from network\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[GRASP DISTANCE] 0.0\n",
      "[GRASP REWARD] R: 0.0\n",
      "[OVERALL REWARD] 0.0\n",
      "[ACTION TYPE]: 0\n",
      "Qx: tensor([[0.5527, 0.6258, 0.4401]], device='cuda:0')\n",
      "Qy: tensor([[0.5960, 0.7756, 0.4991]], device='cuda:0')\n",
      "Qz: tensor([[0.6560, 0.6041, 0.4316]], device='cuda:0')\n",
      "Qyaw: tensor([[0.6253, 0.4926, 0.6186]], device='cuda:0')\n",
      "[MAX OUTPUT]\n",
      "[Q net] x: 1, y: 1, z: 0, yaw_ind: 0\n",
      "[MOVE] move: [ 0.         0.        -0.02      -0.1308997] move index: [1, 1, 0, 0] next move index: [1, 1, 0, 0]\n",
      "target_Qx: 0.6529282927513123 Qx: 0.5535553097724915\n",
      "target_Qy: 0.8020656704902649 Qy: 0.7002131938934326\n",
      "target_Qz: 0.6771792769432068 Qz: 0.5738663077354431\n",
      "target_Qyaw: 0.6222242712974548 Qyaw: 0.5306029915809631\n",
      "loss_rank: None\n",
      "priority: 0.009829236194491386\n",
      "[SUCCESS] append transition experience\n",
      "==== AGENT MODE ENABLE BC: False ENABLE RL: False FULL TRAIN: False ====\n",
      "==== low level action taken: 0 N_pickable_item: 5 ====\n",
      "[SUCCESS] get raw data\n",
      "Qx: tensor([[0.5525, 0.6252, 0.4388]], device='cuda:0')\n",
      "Qy: tensor([[0.5975, 0.7757, 0.4935]], device='cuda:0')\n",
      "Qz: tensor([[0.6535, 0.6025, 0.4325]], device='cuda:0')\n",
      "Qyaw: tensor([[0.6260, 0.4907, 0.6195]], device='cuda:0')\n",
      "[MAX OUTPUT]\n",
      "[Q net] x: 1, y: 1, z: 0, yaw_ind: 0\n",
      "[SUCCESS] estimate actions from network\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[GRASP DISTANCE] 0.0\n",
      "[GRASP REWARD] R: 0.0\n",
      "[OVERALL REWARD] 0.0\n",
      "[ACTION TYPE]: 0\n",
      "Qx: tensor([[0.5304, 0.5457, 0.3852]], device='cuda:0')\n",
      "Qy: tensor([[0.4590, 0.6546, 0.2799]], device='cuda:0')\n",
      "Qz: tensor([[0.4794, 0.6782, 0.2924]], device='cuda:0')\n",
      "Qyaw: tensor([[0.4503, 0.4494, 0.5278]], device='cuda:0')\n",
      "[MAX OUTPUT]\n",
      "[Q net] x: 1, y: 1, z: 1, yaw_ind: 2\n",
      "[MOVE] move: [ 0.         0.        -0.02      -0.1308997] move index: [1, 1, 0, 0] next move index: [1, 1, 1, 2]\n",
      "target_Qx: 0.5494624972343445 Qx: 0.6251742243766785\n",
      "target_Qy: 0.6515523791313171 Qy: 0.7756862044334412\n",
      "target_Qz: 0.6633404493331909 Qz: 0.653546929359436\n",
      "target_Qyaw: 0.5524497032165527 Qyaw: 0.6260480284690857\n",
      "loss_rank: None\n",
      "priority: 0.006663525011390448\n",
      "[SUCCESS] append transition experience\n",
      "==== AGENT MODE ENABLE BC: False ENABLE RL: False FULL TRAIN: False ====\n",
      "==== low level action taken: 0 N_pickable_item: 5 ====\n",
      "[SUCCESS] get raw data\n",
      "Qx: tensor([[0.5306, 0.5465, 0.3854]], device='cuda:0')\n",
      "Qy: tensor([[0.4588, 0.6552, 0.2797]], device='cuda:0')\n",
      "Qz: tensor([[0.4803, 0.6787, 0.2931]], device='cuda:0')\n",
      "Qyaw: tensor([[0.4509, 0.4502, 0.5286]], device='cuda:0')\n",
      "[MAX OUTPUT]\n",
      "[Q net] x: 1, y: 1, z: 1, yaw_ind: 2\n",
      "[SUCCESS] estimate actions from network\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[GRASP DISTANCE] 0.0\n",
      "[GRASP REWARD] R: 0.0\n",
      "[OVERALL REWARD] 0.0\n",
      "[ACTION TYPE]: 0\n",
      "Qx: tensor([[0.5300, 0.7075, 0.5710]], device='cuda:0')\n",
      "Qy: tensor([[0.4931, 0.7982, 0.3687]], device='cuda:0')\n",
      "Qz: tensor([[0.7084, 0.5968, 0.5016]], device='cuda:0')\n",
      "Qyaw: tensor([[0.5949, 0.7313, 0.6133]], device='cuda:0')\n",
      "[MAX OUTPUT]\n",
      "[Q net] x: 1, y: 1, z: 0, yaw_ind: 1\n",
      "[MOVE] move: [0.        0.        0.        0.1308997] move index: [1, 1, 1, 2] next move index: [1, 1, 0, 1]\n",
      "target_Qx: 0.7170054316520691 Qx: 0.5465367436408997\n",
      "target_Qy: 0.7903538346290588 Qy: 0.6551856398582458\n",
      "target_Qz: 0.7056062817573547 Qz: 0.6786877512931824\n",
      "target_Qyaw: 0.7159001231193542 Qyaw: 0.5286417007446289\n",
      "loss_rank: None\n",
      "priority: 0.020780084654688835\n",
      "[SUCCESS] append transition experience\n",
      "==== AGENT MODE ENABLE BC: False ENABLE RL: False FULL TRAIN: False ====\n",
      "==== low level action taken: 0 N_pickable_item: 5 ====\n",
      "[SUCCESS] get raw data\n",
      "Qx: tensor([[0.5307, 0.7072, 0.5724]], device='cuda:0')\n",
      "Qy: tensor([[0.4929, 0.7989, 0.3697]], device='cuda:0')\n",
      "Qz: tensor([[0.7109, 0.5967, 0.5031]], device='cuda:0')\n",
      "Qyaw: tensor([[0.5959, 0.7326, 0.6134]], device='cuda:0')\n",
      "[RANDOM OUTPUT]\n",
      "[Q net] x: 0, y: 1, z: 0, yaw_ind: 2\n",
      "[SUCCESS] estimate actions from network\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[GRASP DISTANCE] 0.0\n",
      "[GRASP REWARD] R: 0.0\n",
      "[OVERALL REWARD] 0.0\n",
      "[ACTION TYPE]: 0\n",
      "Qx: tensor([[0.5902, 0.7816, 0.7970]], device='cuda:0')\n",
      "Qy: tensor([[0.7024, 0.9491, 0.6643]], device='cuda:0')\n",
      "Qz: tensor([[0.9222, 0.7264, 0.6542]], device='cuda:0')\n",
      "Qyaw: tensor([[0.6751, 0.8771, 0.7419]], device='cuda:0')\n",
      "[MAX OUTPUT]\n",
      "[Q net] x: 2, y: 1, z: 0, yaw_ind: 1\n",
      "[MOVE] move: [-0.015      0.        -0.02       0.1308997] move index: [0, 1, 0, 2] next move index: [2, 1, 0, 1]\n",
      "target_Qx: 0.8360581994056702 Qx: 0.530693531036377\n",
      "target_Qy: 0.9427498579025269 Qy: 0.7988826036453247\n",
      "target_Qz: 0.9108009338378906 Qz: 0.7109305262565613\n",
      "target_Qyaw: 0.8787859678268433 Qyaw: 0.6134433150291443\n",
      "loss_rank: None\n",
      "priority: 0.056075066328048706\n",
      "[SUCCESS] append transition experience\n",
      "==== AGENT MODE ENABLE BC: False ENABLE RL: False FULL TRAIN: False ====\n",
      "==== low level action taken: 0 N_pickable_item: 5 ====\n",
      "[SUCCESS] get raw data\n",
      "Qx: tensor([[0.5891, 0.7780, 0.7960]], device='cuda:0')\n",
      "Qy: tensor([[0.7002, 0.9463, 0.6661]], device='cuda:0')\n",
      "Qz: tensor([[0.9187, 0.7263, 0.6520]], device='cuda:0')\n",
      "Qyaw: tensor([[0.6709, 0.8749, 0.7404]], device='cuda:0')\n",
      "[MAX OUTPUT]\n",
      "[Q net] x: 2, y: 1, z: 0, yaw_ind: 1\n",
      "[SUCCESS] estimate actions from network\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[GRASP DISTANCE] 0.0\n",
      "[GRASP REWARD] R: 0.0\n",
      "[OVERALL REWARD] 0.0\n",
      "[ACTION TYPE]: 0\n",
      "Qx: tensor([[0.7539, 0.9006, 0.6348]], device='cuda:0')\n",
      "Qy: tensor([[0.7314, 0.9910, 0.5518]], device='cuda:0')\n",
      "Qz: tensor([[0.9211, 0.7133, 0.6932]], device='cuda:0')\n",
      "Qyaw: tensor([[0.5531, 0.9535, 0.7134]], device='cuda:0')\n",
      "[MAX OUTPUT]\n",
      "[Q net] x: 1, y: 1, z: 0, yaw_ind: 1\n",
      "[MOVE] move: [ 0.015  0.    -0.02   0.   ] move index: [2, 1, 0, 1] next move index: [1, 1, 0, 1]\n",
      "target_Qx: 0.8980117440223694 Qx: 0.7960216999053955\n",
      "target_Qy: 0.9786188006401062 Qy: 0.9462786912918091\n",
      "target_Qz: 0.9161521196365356 Qz: 0.9187346696853638\n",
      "target_Qyaw: 0.9519385099411011 Qyaw: 0.8749306201934814\n",
      "loss_rank: None\n",
      "priority: 0.004346183966845274\n",
      "[SUCCESS] append transition experience\n",
      "==== AGENT MODE ENABLE BC: False ENABLE RL: False FULL TRAIN: False ====\n",
      "==== low level action taken: 0 N_pickable_item: 5 ====\n",
      "[SUCCESS] get raw data\n",
      "Qx: tensor([[0.7569, 0.9043, 0.6400]], device='cuda:0')\n",
      "Qy: tensor([[0.7373, 0.9937, 0.5526]], device='cuda:0')\n",
      "Qz: tensor([[0.9227, 0.7114, 0.6963]], device='cuda:0')\n",
      "Qyaw: tensor([[0.5566, 0.9542, 0.7137]], device='cuda:0')\n",
      "[MAX OUTPUT]\n",
      "[Q net] x: 1, y: 1, z: 0, yaw_ind: 1\n",
      "[SUCCESS] estimate actions from network\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[GRASP] grasp something\n",
      "[GRIPPER STATUS] NON_CLOSE_NON_OPEN\n",
      "[GRASP DISTANCE] 0.0\n",
      "[SUCESS] picked an item!\n",
      "[GRASP REWARD] R: 1.0\n",
      "[OVERALL REWARD] 1.0\n",
      "[ACTION TYPE]: 0\n",
      "Qx: tensor([[0.5786, 0.8075, 0.7363]], device='cuda:0')\n",
      "Qy: tensor([[0.6078, 0.8890, 0.8192]], device='cuda:0')\n",
      "Qz: tensor([[0.8883, 0.5803, 0.6837]], device='cuda:0')\n",
      "Qyaw: tensor([[0.6702, 0.8191, 0.6613]], device='cuda:0')\n",
      "[MAX OUTPUT]\n",
      "[Q net] x: 1, y: 1, z: 0, yaw_ind: 1\n",
      "[MOVE] move: [ 0.    0.   -0.02  0.  ] move index: [1, 1, 0, 1] next move index: [1, 1, 0, 1]\n",
      "target_Qx: 1.0 Qx: 0.9043390154838562\n",
      "target_Qy: 1.0 Qy: 0.9937193393707275\n",
      "target_Qz: 1.0 Qz: 0.9226590991020203\n",
      "target_Qyaw: 1.0 Qyaw: 0.9542094469070435\n",
      "loss_rank: None\n",
      "priority: 0.0043172151781618595\n",
      "[SUCCESS] append transition experience\n",
      "[SUCCESS] grasp an item successfully\n",
      "=== end of action ===\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[SUCCESS] return home\n",
      "[SUCCESS] return home\n",
      "[SUCCESS] GRASP OR PUSH ACTION\n",
      "[GRASP SUCCESS RATE] 0.2%/-inf% [500]\n",
      "[SUCCESS] save agent data\n",
      "==== episode: 0 ====\n",
      "[MIN_DISTANCE] 0.02412387466507877\n",
      "[compute_push_path]: 0.05000295694148177\n",
      "N_step_x: 4.0, N_step_y: 2.0, N_step_z: 10.0, N_step_yaw: 4.0, N_step: 10.0\n",
      "[MIN_DISTANCE] 0.02412387466507877\n",
      "[MIN_DISTANCE] 0.018502452909868204\n",
      "[MIN_DISTANCE] 0.04491790521064087\n",
      "N_step_x: 6.0, N_step_y: 12.0, N_step_z: 10.0, N_step_yaw: 5.0, N_step: 12.0\n",
      "[HLD net] max output: 0\n",
      "N_step_low_level: 25\n",
      "==== AGENT MODE ENABLE BC: False ENABLE RL: False FULL TRAIN: False ====\n",
      "==== low level action taken: 1 N_pickable_item: 4 ====\n",
      "[SUCCESS] get raw data\n",
      "Qx: tensor([[0.4445, 0.3676, 0.4512]], device='cuda:0')\n",
      "Qy: tensor([[0.3952, 0.3826, 0.4175]], device='cuda:0')\n",
      "Qz: tensor([[0.5580, 0.2788, 0.2373]], device='cuda:0')\n",
      "Qyaw: tensor([[0.3210, 0.2687, 0.2631]], device='cuda:0')\n",
      "[MAX OUTPUT]\n",
      "[Q net] x: 2, y: 2, z: 0, yaw_ind: 0\n",
      "[SUCCESS] estimate actions from network\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[GRASP DISTANCE] 0.0\n",
      "[GRASP REWARD] R: 0.0\n",
      "[OVERALL REWARD] 0.0\n",
      "[ACTION TYPE]: 0\n",
      "Qx: tensor([[0.4523, 0.5322, 0.5154]], device='cuda:0')\n",
      "Qy: tensor([[0.2007, 0.3715, 0.3699]], device='cuda:0')\n",
      "Qz: tensor([[0.4553, 0.3819, 0.2910]], device='cuda:0')\n",
      "Qyaw: tensor([[0.3319, 0.3065, 0.1974]], device='cuda:0')\n",
      "[MAX OUTPUT]\n",
      "[Q net] x: 1, y: 1, z: 0, yaw_ind: 0\n",
      "[MOVE] move: [ 0.015      0.015     -0.02      -0.1308997] move index: [2, 2, 0, 0] next move index: [1, 1, 0, 0]\n",
      "target_Qx: 0.5325993299484253 Qx: 0.4512425661087036\n",
      "target_Qy: 0.3866467773914337 Qy: 0.41751396656036377\n",
      "target_Qz: 0.448261559009552 Qz: 0.5579728484153748\n",
      "target_Qyaw: 0.31998640298843384 Qyaw: 0.3210415542125702\n",
      "loss_rank: None\n",
      "priority: 0.00490234699100256\n",
      "[SUCCESS] append transition experience\n",
      "==== AGENT MODE ENABLE BC: False ENABLE RL: False FULL TRAIN: False ====\n",
      "==== low level action taken: 1 N_pickable_item: 4 ====\n",
      "[SUCCESS] get raw data\n",
      "Qx: tensor([[0.4482, 0.5321, 0.5149]], device='cuda:0')\n",
      "Qy: tensor([[0.2019, 0.3734, 0.3694]], device='cuda:0')\n",
      "Qz: tensor([[0.4503, 0.3825, 0.2867]], device='cuda:0')\n",
      "Qyaw: tensor([[0.3302, 0.3056, 0.1965]], device='cuda:0')\n",
      "[MAX OUTPUT]\n",
      "[Q net] x: 1, y: 1, z: 0, yaw_ind: 0\n",
      "[SUCCESS] estimate actions from network\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[GRASP DISTANCE] 0.0\n",
      "[GRASP REWARD] R: 0.0\n",
      "[OVERALL REWARD] 0.0\n",
      "[ACTION TYPE]: 0\n",
      "Qx: tensor([[0.4502, 0.5106, 0.4617]], device='cuda:0')\n",
      "Qy: tensor([[0.2302, 0.3875, 0.3431]], device='cuda:0')\n",
      "Qz: tensor([[0.4701, 0.2719, 0.3055]], device='cuda:0')\n",
      "Qyaw: tensor([[0.2407, 0.2117, 0.2367]], device='cuda:0')\n",
      "[MAX OUTPUT]\n",
      "[Q net] x: 1, y: 1, z: 0, yaw_ind: 0\n",
      "[MOVE] move: [ 0.         0.        -0.02      -0.1308997] move index: [1, 1, 0, 0] next move index: [1, 1, 0, 0]\n",
      "target_Qx: 0.5041155815124512 Qx: 0.532072126865387\n",
      "target_Qy: 0.39390265941619873 Qy: 0.3733632266521454\n",
      "target_Qz: 0.4728153347969055 Qz: 0.4503070116043091\n",
      "target_Qyaw: 0.2385684996843338 Qyaw: 0.3301697373390198\n",
      "loss_rank: None\n",
      "priority: 0.002525212010368705\n",
      "[SUCCESS] append transition experience\n",
      "==== AGENT MODE ENABLE BC: False ENABLE RL: False FULL TRAIN: False ====\n",
      "==== low level action taken: 1 N_pickable_item: 4 ====\n",
      "[SUCCESS] get raw data\n",
      "Qx: tensor([[0.4507, 0.5113, 0.4626]], device='cuda:0')\n",
      "Qy: tensor([[0.2302, 0.3874, 0.3421]], device='cuda:0')\n",
      "Qz: tensor([[0.4707, 0.2731, 0.3061]], device='cuda:0')\n",
      "Qyaw: tensor([[0.2419, 0.2122, 0.2377]], device='cuda:0')\n",
      "[RANDOM OUTPUT]\n",
      "[Q net] x: 1, y: 2, z: 1, yaw_ind: 0\n",
      "[SUCCESS] estimate actions from network\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[GRASP DISTANCE] 0.0\n",
      "[GRASP REWARD] R: 0.0\n",
      "[OVERALL REWARD] 0.0\n",
      "[ACTION TYPE]: 0\n",
      "Qx: tensor([[0.6071, 0.5353, 0.4732]], device='cuda:0')\n",
      "Qy: tensor([[0.3407, 0.4605, 0.4282]], device='cuda:0')\n",
      "Qz: tensor([[0.6158, 0.4883, 0.4462]], device='cuda:0')\n",
      "Qyaw: tensor([[0.2792, 0.4332, 0.2579]], device='cuda:0')\n",
      "[MAX OUTPUT]\n",
      "[Q net] x: 0, y: 1, z: 0, yaw_ind: 1\n",
      "[MOVE] move: [ 0.         0.015      0.        -0.1308997] move index: [1, 2, 1, 0] next move index: [0, 1, 0, 1]\n",
      "target_Qx: 0.6007658243179321 Qx: 0.511320173740387\n",
      "target_Qy: 0.4598705768585205 Qy: 0.34209901094436646\n",
      "target_Qz: 0.6102628111839294 Qz: 0.27306023240089417\n",
      "target_Qyaw: 0.41872745752334595 Qyaw: 0.24192121624946594\n",
      "loss_rank: None\n",
      "priority: 0.04170917347073555\n",
      "[SUCCESS] append transition experience\n",
      "==== AGENT MODE ENABLE BC: False ENABLE RL: False FULL TRAIN: False ====\n",
      "==== low level action taken: 1 N_pickable_item: 4 ====\n",
      "[SUCCESS] get raw data\n",
      "Qx: tensor([[0.6063, 0.5372, 0.4719]], device='cuda:0')\n",
      "Qy: tensor([[0.3354, 0.4509, 0.4337]], device='cuda:0')\n",
      "Qz: tensor([[0.6168, 0.4850, 0.4474]], device='cuda:0')\n",
      "Qyaw: tensor([[0.2762, 0.4399, 0.2602]], device='cuda:0')\n",
      "[MAX OUTPUT]\n",
      "[Q net] x: 0, y: 1, z: 0, yaw_ind: 1\n",
      "[SUCCESS] estimate actions from network\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[GRASP DISTANCE] 0.0\n",
      "[GRASP REWARD] R: 0.0\n",
      "[OVERALL REWARD] 0.0\n",
      "[ACTION TYPE]: 0\n",
      "Qx: tensor([[0.5818, 0.5877, 0.4818]], device='cuda:0')\n",
      "Qy: tensor([[0.3972, 0.4585, 0.5147]], device='cuda:0')\n",
      "Qz: tensor([[0.6290, 0.4504, 0.5036]], device='cuda:0')\n",
      "Qyaw: tensor([[0.2990, 0.4058, 0.3538]], device='cuda:0')\n",
      "[MAX OUTPUT]\n",
      "[Q net] x: 1, y: 2, z: 0, yaw_ind: 1\n",
      "[MOVE] move: [-0.015  0.    -0.02   0.   ] move index: [0, 1, 0, 1] next move index: [1, 2, 0, 1]\n",
      "target_Qx: 0.5804486870765686 Qx: 0.6063305139541626\n",
      "target_Qy: 0.49511054158210754 Qy: 0.45094695687294006\n",
      "target_Qz: 0.6103313565254211 Qz: 0.6168413162231445\n",
      "target_Qyaw: 0.4001323878765106 Qyaw: 0.43989163637161255\n",
      "loss_rank: None\n",
      "priority: 0.001060867216438055\n",
      "[SUCCESS] append transition experience\n",
      "==== AGENT MODE ENABLE BC: False ENABLE RL: False FULL TRAIN: False ====\n",
      "==== low level action taken: 1 N_pickable_item: 4 ====\n",
      "[SUCCESS] get raw data\n",
      "Qx: tensor([[0.5797, 0.5889, 0.4837]], device='cuda:0')\n",
      "Qy: tensor([[0.3931, 0.4552, 0.5144]], device='cuda:0')\n",
      "Qz: tensor([[0.6279, 0.4545, 0.5031]], device='cuda:0')\n",
      "Qyaw: tensor([[0.3011, 0.4044, 0.3524]], device='cuda:0')\n",
      "[MAX OUTPUT]\n",
      "[Q net] x: 1, y: 2, z: 0, yaw_ind: 1\n",
      "[SUCCESS] estimate actions from network\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[GRASP DISTANCE] 0.0\n",
      "[GRASP REWARD] R: 0.0\n",
      "[OVERALL REWARD] 0.0\n",
      "[ACTION TYPE]: 0\n",
      "Qx: tensor([[0.5475, 0.5487, 0.3956]], device='cuda:0')\n",
      "Qy: tensor([[0.3650, 0.3736, 0.4802]], device='cuda:0')\n",
      "Qz: tensor([[0.5589, 0.4154, 0.4388]], device='cuda:0')\n",
      "Qyaw: tensor([[0.2798, 0.3411, 0.2698]], device='cuda:0')\n",
      "[MAX OUTPUT]\n",
      "[Q net] x: 1, y: 2, z: 0, yaw_ind: 1\n",
      "[MOVE] move: [ 0.     0.015 -0.02   0.   ] move index: [1, 2, 0, 1] next move index: [1, 2, 0, 1]\n",
      "target_Qx: 0.5389031767845154 Qx: 0.5888968706130981\n",
      "target_Qy: 0.4436250627040863 Qy: 0.5144010782241821\n",
      "target_Qz: 0.5292931795120239 Qz: 0.6278501152992249\n",
      "target_Qyaw: 0.3137410581111908 Qyaw: 0.4043809175491333\n",
      "loss_rank: None\n",
      "priority: 0.006359416991472244\n",
      "[SUCCESS] append transition experience\n",
      "==== AGENT MODE ENABLE BC: False ENABLE RL: False FULL TRAIN: False ====\n",
      "==== low level action taken: 1 N_pickable_item: 4 ====\n",
      "[SUCCESS] get raw data\n",
      "Qx: tensor([[0.5411, 0.5451, 0.4003]], device='cuda:0')\n",
      "Qy: tensor([[0.3629, 0.3662, 0.4816]], device='cuda:0')\n",
      "Qz: tensor([[0.5567, 0.4261, 0.4369]], device='cuda:0')\n",
      "Qyaw: tensor([[0.2844, 0.3412, 0.2729]], device='cuda:0')\n",
      "[MAX OUTPUT]\n",
      "[Q net] x: 1, y: 2, z: 0, yaw_ind: 1\n",
      "[SUCCESS] estimate actions from network\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[GRASP DISTANCE] 0.0\n",
      "[GRASP REWARD] R: 0.0\n",
      "[OVERALL REWARD] 0.0\n",
      "[ACTION TYPE]: 0\n",
      "Qx: tensor([[0.6110, 0.6430, 0.5065]], device='cuda:0')\n",
      "Qy: tensor([[0.6302, 0.4904, 0.6551]], device='cuda:0')\n",
      "Qz: tensor([[0.6015, 0.5563, 0.4628]], device='cuda:0')\n",
      "Qyaw: tensor([[0.4250, 0.5267, 0.3224]], device='cuda:0')\n",
      "[MAX OUTPUT]\n",
      "[Q net] x: 1, y: 2, z: 0, yaw_ind: 1\n",
      "[MOVE] move: [ 0.     0.015 -0.02   0.   ] move index: [1, 2, 0, 1] next move index: [1, 2, 0, 1]\n",
      "target_Qx: 0.64430171251297 Qx: 0.545062780380249\n",
      "target_Qy: 0.65309739112854 Qy: 0.4815974235534668\n",
      "target_Qz: 0.601154088973999 Qz: 0.5567274689674377\n",
      "target_Qyaw: 0.5275835990905762 Qyaw: 0.34116828441619873\n",
      "loss_rank: None\n",
      "priority: 0.01899624988436699\n",
      "[SUCCESS] append transition experience\n",
      "==== AGENT MODE ENABLE BC: False ENABLE RL: False FULL TRAIN: False ====\n",
      "==== low level action taken: 1 N_pickable_item: 4 ====\n",
      "[SUCCESS] get raw data\n",
      "Qx: tensor([[0.6098, 0.6418, 0.5049]], device='cuda:0')\n",
      "Qy: tensor([[0.6285, 0.4880, 0.6549]], device='cuda:0')\n",
      "Qz: tensor([[0.6022, 0.5567, 0.4635]], device='cuda:0')\n",
      "Qyaw: tensor([[0.4226, 0.5270, 0.3218]], device='cuda:0')\n",
      "[MAX OUTPUT]\n",
      "[Q net] x: 1, y: 2, z: 0, yaw_ind: 1\n",
      "[SUCCESS] estimate actions from network\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[GRASP DISTANCE] 0.0\n",
      "[GRASP REWARD] R: 0.0\n",
      "[OVERALL REWARD] 0.0\n",
      "[ACTION TYPE]: 0\n",
      "Qx: tensor([[0.7231, 0.6943, 0.5938]], device='cuda:0')\n",
      "Qy: tensor([[0.6626, 0.5659, 0.7534]], device='cuda:0')\n",
      "Qz: tensor([[0.6619, 0.4053, 0.5012]], device='cuda:0')\n",
      "Qyaw: tensor([[0.6215, 0.5439, 0.5818]], device='cuda:0')\n",
      "[MAX OUTPUT]\n",
      "[Q net] x: 0, y: 2, z: 0, yaw_ind: 0\n",
      "[MOVE] move: [ 0.     0.015 -0.02   0.   ] move index: [1, 2, 0, 1] next move index: [0, 2, 0, 0]\n",
      "target_Qx: 0.6857112050056458 Qx: 0.6417593955993652\n",
      "target_Qy: 0.7412089109420776 Qy: 0.6548683047294617\n",
      "target_Qz: 0.6589136719703674 Qz: 0.602180540561676\n",
      "target_Qyaw: 0.60983806848526 Qyaw: 0.5270194411277771\n",
      "loss_rank: None\n",
      "priority: 0.004866008646786213\n",
      "[SUCCESS] append transition experience\n",
      "==== AGENT MODE ENABLE BC: False ENABLE RL: False FULL TRAIN: False ====\n",
      "==== low level action taken: 1 N_pickable_item: 4 ====\n",
      "[SUCCESS] get raw data\n",
      "Qx: tensor([[0.7241, 0.6955, 0.5957]], device='cuda:0')\n",
      "Qy: tensor([[0.6658, 0.5654, 0.7566]], device='cuda:0')\n",
      "Qz: tensor([[0.6623, 0.4033, 0.4998]], device='cuda:0')\n",
      "Qyaw: tensor([[0.6258, 0.5460, 0.5824]], device='cuda:0')\n",
      "[MAX OUTPUT]\n",
      "[Q net] x: 0, y: 2, z: 0, yaw_ind: 0\n",
      "[SUCCESS] estimate actions from network\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[GRASP DISTANCE] 0.0\n",
      "[GRASP REWARD] R: 0.0\n",
      "[OVERALL REWARD] 0.0\n",
      "[ACTION TYPE]: 0\n",
      "Qx: tensor([[0.5931, 0.6657, 0.4261]], device='cuda:0')\n",
      "Qy: tensor([[0.5847, 0.4504, 0.7655]], device='cuda:0')\n",
      "Qz: tensor([[0.6378, 0.4989, 0.4837]], device='cuda:0')\n",
      "Qyaw: tensor([[0.4903, 0.6975, 0.5161]], device='cuda:0')\n",
      "[MAX OUTPUT]\n",
      "[Q net] x: 1, y: 2, z: 0, yaw_ind: 1\n",
      "[MOVE] move: [-0.015      0.015     -0.02      -0.1308997] move index: [0, 2, 0, 0] next move index: [1, 2, 0, 1]\n",
      "target_Qx: 0.6588549613952637 Qx: 0.7240949869155884\n",
      "target_Qy: 0.7279543876647949 Qy: 0.7565742135047913\n",
      "target_Qz: 0.5933660864830017 Qz: 0.662259578704834\n",
      "target_Qyaw: 0.6445966362953186 Qyaw: 0.6257762312889099\n",
      "loss_rank: None\n",
      "priority: 0.002543969079852104\n",
      "[SUCCESS] append transition experience\n",
      "==== AGENT MODE ENABLE BC: False ENABLE RL: False FULL TRAIN: False ====\n",
      "==== low level action taken: 1 N_pickable_item: 4 ====\n",
      "[SUCCESS] get raw data\n",
      "Qx: tensor([[0.5966, 0.6662, 0.4295]], device='cuda:0')\n",
      "Qy: tensor([[0.5857, 0.4452, 0.7654]], device='cuda:0')\n",
      "Qz: tensor([[0.6331, 0.4983, 0.4804]], device='cuda:0')\n",
      "Qyaw: tensor([[0.4851, 0.6952, 0.5134]], device='cuda:0')\n",
      "[MAX OUTPUT]\n",
      "[Q net] x: 1, y: 2, z: 0, yaw_ind: 1\n",
      "[SUCCESS] estimate actions from network\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[GRASP DISTANCE] 0.0\n",
      "[GRASP REWARD] R: 0.0\n",
      "[OVERALL REWARD] 0.0\n",
      "[ACTION TYPE]: 0\n",
      "Qx: tensor([[0.4963, 0.5948, 0.5344]], device='cuda:0')\n",
      "Qy: tensor([[0.4952, 0.3491, 0.6059]], device='cuda:0')\n",
      "Qz: tensor([[0.5685, 0.2841, 0.3066]], device='cuda:0')\n",
      "Qyaw: tensor([[0.5697, 0.3002, 0.3568]], device='cuda:0')\n",
      "[MAX OUTPUT]\n",
      "[Q net] x: 1, y: 2, z: 0, yaw_ind: 0\n",
      "[MOVE] move: [ 0.     0.015 -0.02   0.   ] move index: [1, 2, 0, 1] next move index: [1, 2, 0, 0]\n",
      "target_Qx: 0.6798246502876282 Qx: 0.6662405729293823\n",
      "target_Qy: 0.6163697242736816 Qy: 0.7654321193695068\n",
      "target_Qz: 0.6003097295761108 Qz: 0.6331095099449158\n",
      "target_Qyaw: 0.5747323632240295 Qyaw: 0.6951916217803955\n",
      "loss_rank: None\n",
      "priority: 0.009497595950961113\n",
      "[SUCCESS] append transition experience\n",
      "==== AGENT MODE ENABLE BC: False ENABLE RL: False FULL TRAIN: False ====\n",
      "==== low level action taken: 1 N_pickable_item: 4 ====\n",
      "[SUCCESS] get raw data\n",
      "Qx: tensor([[0.5024, 0.6004, 0.5381]], device='cuda:0')\n",
      "Qy: tensor([[0.4990, 0.3570, 0.6068]], device='cuda:0')\n",
      "Qz: tensor([[0.5753, 0.2856, 0.3113]], device='cuda:0')\n",
      "Qyaw: tensor([[0.5709, 0.3060, 0.3576]], device='cuda:0')\n",
      "[MAX OUTPUT]\n",
      "[Q net] x: 1, y: 2, z: 0, yaw_ind: 0\n",
      "[SUCCESS] estimate actions from network\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[GRASP DISTANCE] 0.0\n",
      "[GRASP REWARD] R: 0.0\n",
      "[OVERALL REWARD] 0.0\n",
      "[ACTION TYPE]: 0\n",
      "Qx: tensor([[0.6430, 0.7393, 0.6876]], device='cuda:0')\n",
      "Qy: tensor([[0.5531, 0.6489, 0.6527]], device='cuda:0')\n",
      "Qz: tensor([[0.7092, 0.4919, 0.3335]], device='cuda:0')\n",
      "Qyaw: tensor([[0.3794, 0.6336, 0.3682]], device='cuda:0')\n",
      "[MAX OUTPUT]\n",
      "[Q net] x: 1, y: 2, z: 0, yaw_ind: 1\n",
      "[MOVE] move: [ 0.         0.015     -0.02      -0.1308997] move index: [1, 2, 0, 0] next move index: [1, 2, 0, 1]\n",
      "target_Qx: 0.7588203549385071 Qx: 0.600350558757782\n",
      "target_Qy: 0.6155451536178589 Qy: 0.6068228483200073\n",
      "target_Qz: 0.668612539768219 Qz: 0.5753198266029358\n",
      "target_Qyaw: 0.6231963634490967 Qyaw: 0.5709068179130554\n",
      "loss_rank: None\n",
      "priority: 0.009156620129942894\n",
      "[SUCCESS] append transition experience\n",
      "==== AGENT MODE ENABLE BC: False ENABLE RL: False FULL TRAIN: False ====\n",
      "==== low level action taken: 1 N_pickable_item: 4 ====\n",
      "[SUCCESS] get raw data\n",
      "Qx: tensor([[0.6549, 0.7539, 0.6959]], device='cuda:0')\n",
      "Qy: tensor([[0.5674, 0.6608, 0.6798]], device='cuda:0')\n",
      "Qz: tensor([[0.7193, 0.4983, 0.3549]], device='cuda:0')\n",
      "Qyaw: tensor([[0.3982, 0.6400, 0.3720]], device='cuda:0')\n",
      "[MAX OUTPUT]\n",
      "[Q net] x: 1, y: 2, z: 0, yaw_ind: 1\n",
      "[SUCCESS] estimate actions from network\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[GRASP DISTANCE] 0.0\n",
      "[GRASP REWARD] R: 0.0\n",
      "[OVERALL REWARD] 0.0\n",
      "[ACTION TYPE]: 0\n",
      "Qx: tensor([[0.6742, 0.7582, 0.6810]], device='cuda:0')\n",
      "Qy: tensor([[0.6267, 0.8500, 0.5830]], device='cuda:0')\n",
      "Qz: tensor([[0.8654, 0.6305, 0.6908]], device='cuda:0')\n",
      "Qyaw: tensor([[0.6181, 0.7707, 0.5904]], device='cuda:0')\n",
      "[MAX OUTPUT]\n",
      "[Q net] x: 1, y: 1, z: 0, yaw_ind: 1\n",
      "[MOVE] move: [ 0.     0.015 -0.02   0.   ] move index: [1, 2, 0, 1] next move index: [1, 1, 0, 1]\n",
      "target_Qx: 0.6964756846427917 Qx: 0.7538887858390808\n",
      "target_Qy: 0.8104511499404907 Qy: 0.6798405647277832\n",
      "target_Qz: 0.7935145497322083 Qz: 0.7192623019218445\n",
      "target_Qyaw: 0.7054142951965332 Qyaw: 0.6399660706520081\n",
      "loss_rank: None\n",
      "priority: 0.0075380634516477585\n",
      "[SUCCESS] append transition experience\n",
      "==== AGENT MODE ENABLE BC: False ENABLE RL: False FULL TRAIN: False ====\n",
      "==== low level action taken: 1 N_pickable_item: 4 ====\n",
      "[SUCCESS] get raw data\n",
      "Qx: tensor([[0.6795, 0.7633, 0.6894]], device='cuda:0')\n",
      "Qy: tensor([[0.6287, 0.8502, 0.5834]], device='cuda:0')\n",
      "Qz: tensor([[0.8658, 0.6404, 0.6983]], device='cuda:0')\n",
      "Qyaw: tensor([[0.6249, 0.7747, 0.5947]], device='cuda:0')\n",
      "[MAX OUTPUT]\n",
      "[Q net] x: 1, y: 1, z: 0, yaw_ind: 1\n",
      "[SUCCESS] estimate actions from network\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[GRASP] grasp something\n",
      "[GRIPPER STATUS] NON_CLOSE_NON_OPEN\n",
      "[GRASP DISTANCE] 0.0\n",
      "[SUCESS] picked an item!\n",
      "[GRASP REWARD] R: 1.0\n",
      "[OVERALL REWARD] 1.0\n",
      "[ACTION TYPE]: 0\n",
      "Qx: tensor([[ 0.0178, -0.0217,  0.1040]], device='cuda:0')\n",
      "Qy: tensor([[-0.0810,  0.0500, -0.1762]], device='cuda:0')\n",
      "Qz: tensor([[-0.1533,  0.0670,  0.1245]], device='cuda:0')\n",
      "Qyaw: tensor([[-0.3067, -0.0510, -0.1673]], device='cuda:0')\n",
      "[MAX OUTPUT]\n",
      "[Q net] x: 2, y: 1, z: 2, yaw_ind: 1\n",
      "[MOVE] move: [ 0.    0.   -0.02  0.  ] move index: [1, 1, 0, 1] next move index: [2, 1, 2, 1]\n",
      "target_Qx: 1.0 Qx: 0.763251781463623\n",
      "target_Qy: 1.0 Qy: 0.8501744270324707\n",
      "target_Qz: 1.0 Qz: 0.8658438324928284\n",
      "target_Qyaw: 1.0 Qyaw: 0.7747347354888916\n",
      "loss_rank: None\n",
      "priority: 0.03680993616580963\n",
      "[SUCCESS] append transition experience\n",
      "[SUCCESS] grasp an item successfully\n",
      "=== end of action ===\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[SUCCESS] return home\n",
      "[SUCCESS] return home\n",
      "[SUCCESS] GRASP OR PUSH ACTION\n",
      "[GRASP SUCCESS RATE] 0.4%/-inf% [500]\n",
      "[SUCCESS] save agent data\n",
      "==== episode: 0 ====\n",
      "[MIN_DISTANCE] 0.02412892290597977\n",
      "[compute_push_path]: 0.05000295694148177\n",
      "N_step_x: 4.0, N_step_y: 2.0, N_step_z: 10.0, N_step_yaw: 4.0, N_step: 10.0\n",
      "[MIN_DISTANCE] 0.02412892290597977\n",
      "[MIN_DISTANCE] 0.08497198703317928\n",
      "N_step_x: 6.0, N_step_y: 12.0, N_step_z: 10.0, N_step_yaw: 5.0, N_step: 12.0\n",
      "[HLD net] max output: 0\n",
      "N_step_low_level: 25\n",
      "==== AGENT MODE ENABLE BC: False ENABLE RL: False FULL TRAIN: False ====\n",
      "==== low level action taken: 2 N_pickable_item: 3 ====\n",
      "[SUCCESS] get raw data\n",
      "Qx: tensor([[0.3807, 0.4219, 0.4048]], device='cuda:0')\n",
      "Qy: tensor([[0.3730, 0.4228, 0.3852]], device='cuda:0')\n",
      "Qz: tensor([[0.5114, 0.3075, 0.2020]], device='cuda:0')\n",
      "Qyaw: tensor([[0.2672, 0.2563, 0.2376]], device='cuda:0')\n",
      "[MAX OUTPUT]\n",
      "[Q net] x: 1, y: 1, z: 0, yaw_ind: 0\n",
      "[SUCCESS] estimate actions from network\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[GRASP DISTANCE] 0.0\n",
      "[GRASP REWARD] R: 0.0\n",
      "[OVERALL REWARD] 0.0\n",
      "[ACTION TYPE]: 0\n",
      "Qx: tensor([[0.3864, 0.3846, 0.3431]], device='cuda:0')\n",
      "Qy: tensor([[0.3736, 0.3874, 0.4713]], device='cuda:0')\n",
      "Qz: tensor([[0.4675, 0.4577, 0.2209]], device='cuda:0')\n",
      "Qyaw: tensor([[0.1163, 0.1300, 0.2667]], device='cuda:0')\n",
      "[MAX OUTPUT]\n",
      "[Q net] x: 0, y: 2, z: 0, yaw_ind: 2\n",
      "[MOVE] move: [ 0.         0.        -0.02      -0.1308997] move index: [1, 1, 0, 0] next move index: [0, 2, 0, 2]\n",
      "target_Qx: 0.3787517547607422 Qx: 0.4219266474246979\n",
      "target_Qy: 0.4521045982837677 Qy: 0.4227801263332367\n",
      "target_Qz: 0.46356725692749023 Qz: 0.5114172697067261\n",
      "target_Qyaw: 0.29069504141807556 Qyaw: 0.2671688199043274\n",
      "loss_rank: None\n",
      "priority: 0.0013917756732553244\n",
      "[SUCCESS] append transition experience\n",
      "==== AGENT MODE ENABLE BC: False ENABLE RL: False FULL TRAIN: False ====\n",
      "==== low level action taken: 2 N_pickable_item: 3 ====\n",
      "[SUCCESS] get raw data\n",
      "Qx: tensor([[0.3858, 0.3835, 0.3429]], device='cuda:0')\n",
      "Qy: tensor([[0.3729, 0.3870, 0.4705]], device='cuda:0')\n",
      "Qz: tensor([[0.4668, 0.4588, 0.2195]], device='cuda:0')\n",
      "Qyaw: tensor([[0.1163, 0.1308, 0.2686]], device='cuda:0')\n",
      "[MAX OUTPUT]\n",
      "[Q net] x: 0, y: 2, z: 0, yaw_ind: 2\n",
      "[SUCCESS] estimate actions from network\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[GRASP DISTANCE] 0.0\n",
      "[GRASP REWARD] R: 0.0\n",
      "[OVERALL REWARD] 0.0\n",
      "[ACTION TYPE]: 0\n",
      "Qx: tensor([[0.4498, 0.4324, 0.5698]], device='cuda:0')\n",
      "Qy: tensor([[0.2954, 0.4059, 0.3966]], device='cuda:0')\n",
      "Qz: tensor([[0.4948, 0.3558, 0.2527]], device='cuda:0')\n",
      "Qyaw: tensor([[0.2721, 0.2637, 0.2122]], device='cuda:0')\n",
      "[MAX OUTPUT]\n",
      "[Q net] x: 2, y: 1, z: 0, yaw_ind: 0\n",
      "[MOVE] move: [-0.015      0.015     -0.02       0.1308997] move index: [0, 2, 0, 2] next move index: [2, 1, 0, 0]\n",
      "target_Qx: 0.5567534565925598 Qx: 0.385765939950943\n",
      "target_Qy: 0.37896814942359924 Qy: 0.4705316126346588\n",
      "target_Qz: 0.4621499180793762 Qz: 0.4668434262275696\n",
      "target_Qyaw: 0.2593314051628113 Qyaw: 0.2685728669166565\n",
      "loss_rank: None\n",
      "priority: 0.009432007558643818\n",
      "[SUCCESS] append transition experience\n",
      "==== AGENT MODE ENABLE BC: False ENABLE RL: False FULL TRAIN: False ====\n",
      "==== low level action taken: 2 N_pickable_item: 3 ====\n",
      "[SUCCESS] get raw data\n",
      "Qx: tensor([[0.4441, 0.4040, 0.5544]], device='cuda:0')\n",
      "Qy: tensor([[0.2810, 0.3956, 0.3974]], device='cuda:0')\n",
      "Qz: tensor([[0.4933, 0.3307, 0.2369]], device='cuda:0')\n",
      "Qyaw: tensor([[0.2724, 0.2685, 0.1938]], device='cuda:0')\n",
      "[MAX OUTPUT]\n",
      "[Q net] x: 2, y: 2, z: 0, yaw_ind: 0\n",
      "[SUCCESS] estimate actions from network\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[GRASP DISTANCE] 0.0\n",
      "[GRASP REWARD] R: 0.0\n",
      "[OVERALL REWARD] 0.0\n",
      "[ACTION TYPE]: 0\n",
      "Qx: tensor([[0.4940, 0.5457, 0.4234]], device='cuda:0')\n",
      "Qy: tensor([[0.3827, 0.4771, 0.4160]], device='cuda:0')\n",
      "Qz: tensor([[0.5177, 0.4242, 0.3075]], device='cuda:0')\n",
      "Qyaw: tensor([[0.3416, 0.2418, 0.2517]], device='cuda:0')\n",
      "[MAX OUTPUT]\n",
      "[Q net] x: 1, y: 1, z: 0, yaw_ind: 0\n",
      "[MOVE] move: [ 0.015      0.015     -0.02      -0.1308997] move index: [2, 2, 0, 0] next move index: [1, 1, 0, 0]\n",
      "target_Qx: 0.5669531226158142 Qx: 0.5543925166130066\n",
      "target_Qy: 0.4906575381755829 Qy: 0.3974245488643646\n",
      "target_Qz: 0.5176659226417542 Qz: 0.4932619333267212\n",
      "target_Qyaw: 0.3404558002948761 Qyaw: 0.2724132239818573\n",
      "loss_rank: None\n",
      "priority: 0.0035188766196370125\n",
      "[SUCCESS] append transition experience\n",
      "==== AGENT MODE ENABLE BC: False ENABLE RL: False FULL TRAIN: False ====\n",
      "==== low level action taken: 2 N_pickable_item: 3 ====\n",
      "[SUCCESS] get raw data\n",
      "Qx: tensor([[0.4948, 0.5452, 0.4237]], device='cuda:0')\n",
      "Qy: tensor([[0.3822, 0.4760, 0.4150]], device='cuda:0')\n",
      "Qz: tensor([[0.5172, 0.4240, 0.3085]], device='cuda:0')\n",
      "Qyaw: tensor([[0.3414, 0.2420, 0.2525]], device='cuda:0')\n",
      "[MAX OUTPUT]\n",
      "[Q net] x: 1, y: 1, z: 0, yaw_ind: 0\n",
      "[SUCCESS] estimate actions from network\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[GRASP DISTANCE] 0.0\n",
      "[GRASP REWARD] R: 0.0\n",
      "[OVERALL REWARD] 0.0\n",
      "[ACTION TYPE]: 0\n",
      "Qx: tensor([[0.3817, 0.5427, 0.4136]], device='cuda:0')\n",
      "Qy: tensor([[0.3787, 0.3932, 0.5089]], device='cuda:0')\n",
      "Qz: tensor([[0.5062, 0.4265, 0.3052]], device='cuda:0')\n",
      "Qyaw: tensor([[0.3640, 0.3842, 0.2621]], device='cuda:0')\n",
      "[MAX OUTPUT]\n",
      "[Q net] x: 1, y: 2, z: 0, yaw_ind: 1\n",
      "[MOVE] move: [ 0.         0.        -0.02      -0.1308997] move index: [1, 1, 0, 0] next move index: [1, 2, 0, 1]\n",
      "target_Qx: 0.5545966029167175 Qx: 0.5452467203140259\n",
      "target_Qy: 0.5198543667793274 Qy: 0.4759843945503235\n",
      "target_Qz: 0.5075773596763611 Qz: 0.5172154307365417\n",
      "target_Qyaw: 0.3878747522830963 Qyaw: 0.3413728177547455\n",
      "loss_rank: None\n",
      "priority: 0.0010668293107300997\n",
      "[SUCCESS] append transition experience\n",
      "==== AGENT MODE ENABLE BC: False ENABLE RL: False FULL TRAIN: False ====\n",
      "==== low level action taken: 2 N_pickable_item: 3 ====\n",
      "[SUCCESS] get raw data\n",
      "Qx: tensor([[0.3689, 0.5369, 0.4085]], device='cuda:0')\n",
      "Qy: tensor([[0.3767, 0.3875, 0.5054]], device='cuda:0')\n",
      "Qz: tensor([[0.5071, 0.4258, 0.2984]], device='cuda:0')\n",
      "Qyaw: tensor([[0.3579, 0.3826, 0.2547]], device='cuda:0')\n",
      "[MAX OUTPUT]\n",
      "[Q net] x: 1, y: 2, z: 0, yaw_ind: 1\n",
      "[SUCCESS] estimate actions from network\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[GRASP DISTANCE] 0.0\n",
      "[GRASP REWARD] R: 0.0\n",
      "[OVERALL REWARD] 0.0\n",
      "[ACTION TYPE]: 0\n",
      "Qx: tensor([[0.4134, 0.4401, 0.4716]], device='cuda:0')\n",
      "Qy: tensor([[0.4672, 0.4806, 0.4708]], device='cuda:0')\n",
      "Qz: tensor([[0.4834, 0.4500, 0.2348]], device='cuda:0')\n",
      "Qyaw: tensor([[0.4563, 0.4873, 0.4683]], device='cuda:0')\n",
      "[MAX OUTPUT]\n",
      "[Q net] x: 2, y: 1, z: 0, yaw_ind: 1\n",
      "[MOVE] move: [ 0.     0.015 -0.02   0.   ] move index: [1, 2, 0, 1] next move index: [2, 1, 0, 1]\n",
      "target_Qx: 0.46295392513275146 Qx: 0.5369136929512024\n",
      "target_Qy: 0.46132317185401917 Qy: 0.5053908824920654\n",
      "target_Qz: 0.4683682918548584 Qz: 0.507114589214325\n",
      "target_Qyaw: 0.4822308123111725 Qyaw: 0.38261502981185913\n",
      "loss_rank: None\n",
      "priority: 0.004709147848188877\n",
      "[SUCCESS] append transition experience\n",
      "==== AGENT MODE ENABLE BC: False ENABLE RL: False FULL TRAIN: False ====\n",
      "==== low level action taken: 2 N_pickable_item: 3 ====\n",
      "[SUCCESS] get raw data\n",
      "Qx: tensor([[0.4148, 0.4436, 0.4683]], device='cuda:0')\n",
      "Qy: tensor([[0.4614, 0.4767, 0.4659]], device='cuda:0')\n",
      "Qz: tensor([[0.4808, 0.4456, 0.2307]], device='cuda:0')\n",
      "Qyaw: tensor([[0.4573, 0.4892, 0.4650]], device='cuda:0')\n",
      "[MAX OUTPUT]\n",
      "[Q net] x: 2, y: 1, z: 0, yaw_ind: 1\n",
      "[SUCCESS] estimate actions from network\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[GRASP DISTANCE] 0.0\n",
      "[GRASP REWARD] R: 0.0\n",
      "[OVERALL REWARD] 0.0\n",
      "[ACTION TYPE]: 0\n",
      "Qx: tensor([[0.6866, 0.5432, 0.4721]], device='cuda:0')\n",
      "Qy: tensor([[0.5300, 0.6296, 0.5205]], device='cuda:0')\n",
      "Qz: tensor([[0.5690, 0.6248, 0.3794]], device='cuda:0')\n",
      "Qyaw: tensor([[0.5844, 0.6448, 0.4136]], device='cuda:0')\n",
      "[MAX OUTPUT]\n",
      "[Q net] x: 0, y: 1, z: 1, yaw_ind: 1\n",
      "[MOVE] move: [ 0.015  0.    -0.02   0.   ] move index: [2, 1, 0, 1] next move index: [0, 1, 1, 1]\n",
      "target_Qx: 0.6634912490844727 Qx: 0.46826449036598206\n",
      "target_Qy: 0.6270745396614075 Qy: 0.4767359793186188\n",
      "target_Qz: 0.6212636232376099 Qz: 0.48075205087661743\n",
      "target_Qyaw: 0.6497383117675781 Qyaw: 0.48920130729675293\n",
      "loss_rank: None\n",
      "priority: 0.02655770070850849\n",
      "[SUCCESS] append transition experience\n",
      "==== AGENT MODE ENABLE BC: False ENABLE RL: False FULL TRAIN: False ====\n",
      "==== low level action taken: 2 N_pickable_item: 3 ====\n",
      "[SUCCESS] get raw data\n",
      "Qx: tensor([[0.6923, 0.5342, 0.4729]], device='cuda:0')\n",
      "Qy: tensor([[0.5327, 0.6297, 0.5200]], device='cuda:0')\n",
      "Qz: tensor([[0.5759, 0.6279, 0.3817]], device='cuda:0')\n",
      "Qyaw: tensor([[0.5917, 0.6440, 0.4208]], device='cuda:0')\n",
      "[MAX OUTPUT]\n",
      "[Q net] x: 0, y: 1, z: 1, yaw_ind: 1\n",
      "[SUCCESS] estimate actions from network\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[GRASP DISTANCE] 0.0\n",
      "[GRASP REWARD] R: 0.0\n",
      "[OVERALL REWARD] 0.0\n",
      "[ACTION TYPE]: 0\n",
      "Qx: tensor([[0.3616, 0.4430, 0.3651]], device='cuda:0')\n",
      "Qy: tensor([[0.3435, 0.3964, 0.4657]], device='cuda:0')\n",
      "Qz: tensor([[0.4931, 0.3542, 0.2606]], device='cuda:0')\n",
      "Qyaw: tensor([[0.4708, 0.4077, 0.4526]], device='cuda:0')\n",
      "[MAX OUTPUT]\n",
      "[Q net] x: 1, y: 2, z: 0, yaw_ind: 0\n",
      "[MOVE] move: [-0.015  0.     0.     0.   ] move index: [0, 1, 1, 1] next move index: [1, 2, 0, 0]\n",
      "target_Qx: 0.44033393263816833 Qx: 0.6922621726989746\n",
      "target_Qy: 0.45054513216018677 Qy: 0.6297030448913574\n",
      "target_Qz: 0.48166412115097046 Qz: 0.6279414892196655\n",
      "target_Qyaw: 0.46774715185165405 Qyaw: 0.6440450549125671\n",
      "loss_rank: None\n",
      "priority: 0.03701085224747658\n",
      "[SUCCESS] append transition experience\n",
      "==== AGENT MODE ENABLE BC: False ENABLE RL: False FULL TRAIN: False ====\n",
      "==== low level action taken: 2 N_pickable_item: 3 ====\n",
      "[SUCCESS] get raw data\n",
      "Qx: tensor([[0.3601, 0.4522, 0.3529]], device='cuda:0')\n",
      "Qy: tensor([[0.3384, 0.3947, 0.4607]], device='cuda:0')\n",
      "Qz: tensor([[0.4961, 0.3436, 0.2443]], device='cuda:0')\n",
      "Qyaw: tensor([[0.4774, 0.4128, 0.4556]], device='cuda:0')\n",
      "[MAX OUTPUT]\n",
      "[Q net] x: 1, y: 2, z: 0, yaw_ind: 0\n",
      "[SUCCESS] estimate actions from network\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[GRASP DISTANCE] 0.0\n",
      "[GRASP REWARD] R: 0.0\n",
      "[OVERALL REWARD] 0.0\n",
      "[ACTION TYPE]: 0\n",
      "Qx: tensor([[0.4126, 0.4175, 0.3411]], device='cuda:0')\n",
      "Qy: tensor([[0.4142, 0.5094, 0.3614]], device='cuda:0')\n",
      "Qz: tensor([[0.5053, 0.4250, 0.2297]], device='cuda:0')\n",
      "Qyaw: tensor([[0.4546, 0.4099, 0.2617]], device='cuda:0')\n",
      "[MAX OUTPUT]\n",
      "[Q net] x: 1, y: 1, z: 0, yaw_ind: 0\n",
      "[MOVE] move: [ 0.         0.015     -0.02      -0.1308997] move index: [1, 2, 0, 0] next move index: [1, 1, 0, 0]\n",
      "target_Qx: 0.4660262167453766 Qx: 0.45216402411460876\n",
      "target_Qy: 0.5156955122947693 Qy: 0.4607333540916443\n",
      "target_Qz: 0.50677889585495 Qz: 0.4961382746696472\n",
      "target_Qyaw: 0.46657341718673706 Qyaw: 0.47742214798927307\n",
      "loss_rank: None\n",
      "priority: 0.0008609792566858232\n",
      "[SUCCESS] append transition experience\n",
      "==== AGENT MODE ENABLE BC: False ENABLE RL: False FULL TRAIN: False ====\n",
      "==== low level action taken: 2 N_pickable_item: 3 ====\n",
      "[SUCCESS] get raw data\n",
      "Qx: tensor([[0.4183, 0.4210, 0.3498]], device='cuda:0')\n",
      "Qy: tensor([[0.4160, 0.5136, 0.3619]], device='cuda:0')\n",
      "Qz: tensor([[0.5124, 0.4265, 0.2422]], device='cuda:0')\n",
      "Qyaw: tensor([[0.4536, 0.4154, 0.2685]], device='cuda:0')\n",
      "[MAX OUTPUT]\n",
      "[Q net] x: 1, y: 1, z: 0, yaw_ind: 0\n",
      "[SUCCESS] estimate actions from network\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[GRASP DISTANCE] 0.0\n",
      "[GRASP REWARD] R: 0.0\n",
      "[OVERALL REWARD] 0.0\n",
      "[ACTION TYPE]: 0\n",
      "Qx: tensor([[0.4023, 0.3587, 0.2831]], device='cuda:0')\n",
      "Qy: tensor([[0.2034, 0.3473, 0.0774]], device='cuda:0')\n",
      "Qz: tensor([[0.4650, 0.3356, 0.2308]], device='cuda:0')\n",
      "Qyaw: tensor([[0.2005, 0.3218, 0.1338]], device='cuda:0')\n",
      "[MAX OUTPUT]\n",
      "[Q net] x: 0, y: 1, z: 0, yaw_ind: 1\n",
      "[MOVE] move: [ 0.         0.        -0.02      -0.1308997] move index: [1, 1, 0, 0] next move index: [0, 1, 0, 1]\n",
      "target_Qx: 0.3893095850944519 Qx: 0.42099079489707947\n",
      "target_Qy: 0.32995128631591797 Qy: 0.5136077404022217\n",
      "target_Qz: 0.4530828595161438 Qz: 0.5123535990715027\n",
      "target_Qyaw: 0.29244956374168396 Qyaw: 0.45358893275260925\n",
      "loss_rank: None\n",
      "priority: 0.016053076833486557\n",
      "[SUCCESS] append transition experience\n",
      "==== AGENT MODE ENABLE BC: False ENABLE RL: False FULL TRAIN: False ====\n",
      "==== low level action taken: 2 N_pickable_item: 3 ====\n",
      "[SUCCESS] get raw data\n",
      "Qx: tensor([[0.4023, 0.3572, 0.2818]], device='cuda:0')\n",
      "Qy: tensor([[0.2025, 0.3489, 0.0767]], device='cuda:0')\n",
      "Qz: tensor([[0.4666, 0.3397, 0.2318]], device='cuda:0')\n",
      "Qyaw: tensor([[0.2023, 0.3235, 0.1351]], device='cuda:0')\n",
      "[MAX OUTPUT]\n",
      "[Q net] x: 0, y: 1, z: 0, yaw_ind: 1\n",
      "[SUCCESS] estimate actions from network\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[GRASP DISTANCE] 0.0\n",
      "[GRASP REWARD] R: 0.0\n",
      "[OVERALL REWARD] 0.0\n",
      "[ACTION TYPE]: 0\n",
      "Qx: tensor([[0.2726, 0.3803, 0.3590]], device='cuda:0')\n",
      "Qy: tensor([[0.2249, 0.3803, 0.1304]], device='cuda:0')\n",
      "Qz: tensor([[0.3089, 0.1147, 0.1379]], device='cuda:0')\n",
      "Qyaw: tensor([[ 0.1359,  0.2060, -0.0374]], device='cuda:0')\n",
      "[MAX OUTPUT]\n",
      "[Q net] x: 1, y: 1, z: 0, yaw_ind: 1\n",
      "[MOVE] move: [-0.015  0.    -0.02   0.   ] move index: [0, 1, 0, 1] next move index: [1, 1, 0, 1]\n",
      "target_Qx: 0.3428782522678375 Qx: 0.40231361985206604\n",
      "target_Qy: 0.3436555564403534 Qy: 0.34886887669563293\n",
      "target_Qz: 0.28067654371261597 Qz: 0.4666326642036438\n",
      "target_Qyaw: 0.1503421813249588 Qyaw: 0.3235480785369873\n",
      "loss_rank: None\n",
      "priority: 0.017034925520420074\n",
      "[SUCCESS] append transition experience\n",
      "==== AGENT MODE ENABLE BC: False ENABLE RL: False FULL TRAIN: False ====\n",
      "==== low level action taken: 2 N_pickable_item: 3 ====\n",
      "[SUCCESS] get raw data\n",
      "Qx: tensor([[0.2657, 0.3804, 0.3592]], device='cuda:0')\n",
      "Qy: tensor([[0.2259, 0.3931, 0.1497]], device='cuda:0')\n",
      "Qz: tensor([[0.3120, 0.1386, 0.1436]], device='cuda:0')\n",
      "Qyaw: tensor([[ 0.1236,  0.2152, -0.0453]], device='cuda:0')\n",
      "[RANDOM OUTPUT]\n",
      "[Q net] x: 1, y: 1, z: 0, yaw_ind: 0\n",
      "[SUCCESS] estimate actions from network\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[GRASP DISTANCE] 0.0\n",
      "[GRASP REWARD] R: 0.0\n",
      "[OVERALL REWARD] 0.0\n",
      "[ACTION TYPE]: 0\n",
      "Qx: tensor([[0.1077, 0.1445, 0.1993]], device='cuda:0')\n",
      "Qy: tensor([[-0.0197,  0.1041, -0.0443]], device='cuda:0')\n",
      "Qz: tensor([[0.1836, 0.0752, 0.1884]], device='cuda:0')\n",
      "Qyaw: tensor([[-0.0320,  0.0096, -0.0827]], device='cuda:0')\n",
      "[MAX OUTPUT]\n",
      "[Q net] x: 2, y: 1, z: 2, yaw_ind: 1\n",
      "[MOVE] move: [ 0.         0.        -0.02      -0.1308997] move index: [1, 1, 0, 0] next move index: [2, 1, 2, 1]\n",
      "target_Qx: 0.12876783311367035 Qx: 0.3804064691066742\n",
      "target_Qy: -0.0046365163289010525 Qy: 0.3930620849132538\n",
      "target_Qz: 0.0729430541396141 Qz: 0.3119964301586151\n",
      "target_Qyaw: -0.09465815126895905 Qyaw: 0.12362929433584213\n",
      "loss_rank: None\n",
      "priority: 0.08157052844762802\n",
      "[SUCCESS] append transition experience\n",
      "==== AGENT MODE ENABLE BC: False ENABLE RL: False FULL TRAIN: False ====\n",
      "==== low level action taken: 2 N_pickable_item: 3 ====\n",
      "[SUCCESS] get raw data\n",
      "Qx: tensor([[0.1130, 0.1365, 0.1945]], device='cuda:0')\n",
      "Qy: tensor([[-0.0199,  0.0982, -0.0417]], device='cuda:0')\n",
      "Qz: tensor([[0.1858, 0.0810, 0.1945]], device='cuda:0')\n",
      "Qyaw: tensor([[-0.0341,  0.0059, -0.0910]], device='cuda:0')\n",
      "[RANDOM OUTPUT]\n",
      "[Q net] x: 1, y: 0, z: 1, yaw_ind: 2\n",
      "[SUCCESS] estimate actions from network\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[GRASP DISTANCE] 0.0\n",
      "[GRASP REWARD] R: 0.0\n",
      "[OVERALL REWARD] 0.0\n",
      "[ACTION TYPE]: 0\n",
      "Qx: tensor([[0.5817, 0.6445, 0.7421]], device='cuda:0')\n",
      "Qy: tensor([[0.5431, 0.7666, 0.7582]], device='cuda:0')\n",
      "Qz: tensor([[0.7762, 0.8096, 0.6834]], device='cuda:0')\n",
      "Qyaw: tensor([[0.6611, 0.7026, 0.5721]], device='cuda:0')\n",
      "[MAX OUTPUT]\n",
      "[Q net] x: 2, y: 1, z: 1, yaw_ind: 1\n",
      "[MOVE] move: [ 0.        -0.015      0.         0.1308997] move index: [1, 0, 1, 2] next move index: [2, 1, 1, 1]\n",
      "target_Qx: 0.7733043432235718 Qx: 0.1365443617105484\n",
      "target_Qy: 0.7936203479766846 Qy: -0.01992456242442131\n",
      "target_Qz: 0.8089309930801392 Qz: 0.08102503418922424\n",
      "target_Qyaw: 0.7396540641784668 Qyaw: -0.09102748334407806\n",
      "loss_rank: None\n",
      "priority: 0.5717993974685669\n",
      "[SUCCESS] append transition experience\n",
      "==== AGENT MODE ENABLE BC: False ENABLE RL: False FULL TRAIN: False ====\n",
      "==== low level action taken: 2 N_pickable_item: 3 ====\n",
      "[SUCCESS] get raw data\n",
      "Qx: tensor([[0.5817, 0.6443, 0.7420]], device='cuda:0')\n",
      "Qy: tensor([[0.5428, 0.7662, 0.7580]], device='cuda:0')\n",
      "Qz: tensor([[0.7760, 0.8093, 0.6831]], device='cuda:0')\n",
      "Qyaw: tensor([[0.6608, 0.7022, 0.5716]], device='cuda:0')\n",
      "[MAX OUTPUT]\n",
      "[Q net] x: 2, y: 1, z: 1, yaw_ind: 1\n",
      "[SUCCESS] estimate actions from network\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[GRASP DISTANCE] 0.0\n",
      "[GRASP REWARD] R: 0.0\n",
      "[OVERALL REWARD] 0.0\n",
      "[ACTION TYPE]: 0\n",
      "Qx: tensor([[0.8115, 0.8287, 0.5251]], device='cuda:0')\n",
      "Qy: tensor([[0.8416, 0.8571, 0.8665]], device='cuda:0')\n",
      "Qz: tensor([[0.9057, 0.7466, 0.7394]], device='cuda:0')\n",
      "Qyaw: tensor([[0.8117, 0.8594, 0.7372]], device='cuda:0')\n",
      "[MAX OUTPUT]\n",
      "[Q net] x: 1, y: 2, z: 0, yaw_ind: 1\n",
      "[MOVE] move: [0.015 0.    0.    0.   ] move index: [2, 1, 1, 1] next move index: [1, 2, 0, 1]\n",
      "target_Qx: 0.8407959342002869 Qx: 0.7420271039009094\n",
      "target_Qy: 0.8052095770835876 Qy: 0.7662386298179626\n",
      "target_Qz: 0.9465711712837219 Qz: 0.8092857599258423\n",
      "target_Qyaw: 0.8917096853256226 Qyaw: 0.7021788954734802\n",
      "loss_rank: None\n",
      "priority: 0.01651080511510372\n",
      "[SUCCESS] append transition experience\n",
      "==== AGENT MODE ENABLE BC: False ENABLE RL: False FULL TRAIN: False ====\n",
      "==== low level action taken: 2 N_pickable_item: 3 ====\n",
      "[SUCCESS] get raw data\n",
      "Qx: tensor([[0.8110, 0.8268, 0.5266]], device='cuda:0')\n",
      "Qy: tensor([[0.8454, 0.8592, 0.8672]], device='cuda:0')\n",
      "Qz: tensor([[0.9079, 0.7502, 0.7424]], device='cuda:0')\n",
      "Qyaw: tensor([[0.8123, 0.8612, 0.7418]], device='cuda:0')\n",
      "[MAX OUTPUT]\n",
      "[Q net] x: 1, y: 2, z: 0, yaw_ind: 1\n",
      "[SUCCESS] estimate actions from network\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[GRASP] grasp something\n",
      "[GRIPPER STATUS] NON_CLOSE_NON_OPEN\n",
      "[GRASP DISTANCE] 0.0\n",
      "[SUCESS] picked an item!\n",
      "[GRASP REWARD] R: 1.0\n",
      "[OVERALL REWARD] 1.0\n",
      "[ACTION TYPE]: 0\n",
      "Qx: tensor([[0.1409, 0.1074, 0.0022]], device='cuda:0')\n",
      "Qy: tensor([[0.0447, 0.1232, 0.0817]], device='cuda:0')\n",
      "Qz: tensor([[0.0719, 0.0553, 0.1524]], device='cuda:0')\n",
      "Qyaw: tensor([[ 0.0303, -0.0058, -0.1427]], device='cuda:0')\n",
      "[MAX OUTPUT]\n",
      "[Q net] x: 0, y: 1, z: 2, yaw_ind: 0\n",
      "[MOVE] move: [ 0.     0.015 -0.02   0.   ] move index: [1, 2, 0, 1] next move index: [0, 1, 2, 0]\n",
      "target_Qx: 1.0 Qx: 0.8267955183982849\n",
      "target_Qy: 1.0 Qy: 0.8671618700027466\n",
      "target_Qz: 1.0 Qz: 0.9078899025917053\n",
      "target_Qyaw: 1.0 Qyaw: 0.8612483739852905\n",
      "loss_rank: None\n",
      "priority: 0.018845511600375175\n",
      "[SUCCESS] append transition experience\n",
      "[SUCCESS] grasp an item successfully\n",
      "=== end of action ===\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[SUCCESS] return home\n",
      "[SUCCESS] return home\n",
      "[SUCCESS] GRASP OR PUSH ACTION\n",
      "[GRASP SUCCESS RATE] 0.6%/-inf% [500]\n",
      "[SUCCESS] save agent data\n",
      "==== episode: 0 ====\n",
      "[MIN_DISTANCE] 0.11387873719771588\n",
      "N_step_x: 5.0, N_step_y: 6.0, N_step_z: 10.0, N_step_yaw: 5.0, N_step: 10.0\n",
      "[MIN_DISTANCE] 0.11387873719771588\n",
      "[HLD net] max output: 0\n",
      "N_step_low_level: 25\n",
      "==== AGENT MODE ENABLE BC: False ENABLE RL: False FULL TRAIN: False ====\n",
      "==== low level action taken: 3 N_pickable_item: 2 ====\n",
      "[SUCCESS] get raw data\n",
      "Qx: tensor([[0.4891, 0.4612, 0.4906]], device='cuda:0')\n",
      "Qy: tensor([[0.3955, 0.4304, 0.4570]], device='cuda:0')\n",
      "Qz: tensor([[0.5735, 0.3281, 0.2479]], device='cuda:0')\n",
      "Qyaw: tensor([[0.3752, 0.3121, 0.1742]], device='cuda:0')\n",
      "[MAX OUTPUT]\n",
      "[Q net] x: 2, y: 2, z: 0, yaw_ind: 0\n",
      "[SUCCESS] estimate actions from network\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[GRASP DISTANCE] 0.0\n",
      "[GRASP REWARD] R: 0.0\n",
      "[OVERALL REWARD] 0.0\n",
      "[ACTION TYPE]: 0\n",
      "Qx: tensor([[0.4845, 0.4801, 0.5316]], device='cuda:0')\n",
      "Qy: tensor([[0.2738, 0.3827, 0.4894]], device='cuda:0')\n",
      "Qz: tensor([[0.4801, 0.3502, 0.2218]], device='cuda:0')\n",
      "Qyaw: tensor([[0.2522, 0.3323, 0.2499]], device='cuda:0')\n",
      "[MAX OUTPUT]\n",
      "[Q net] x: 2, y: 2, z: 0, yaw_ind: 1\n",
      "[MOVE] move: [ 0.015      0.015     -0.02      -0.1308997] move index: [2, 2, 0, 0] next move index: [2, 2, 0, 1]\n",
      "target_Qx: 0.5286579132080078 Qx: 0.4905760586261749\n",
      "target_Qy: 0.4821772873401642 Qy: 0.4569520354270935\n",
      "target_Qz: 0.4921233057975769 Qz: 0.5734705924987793\n",
      "target_Qyaw: 0.31822118163108826 Qyaw: 0.37517526745796204\n",
      "loss_rank: None\n",
      "priority: 0.002986922627314925\n",
      "[SUCCESS] append transition experience\n",
      "==== AGENT MODE ENABLE BC: False ENABLE RL: False FULL TRAIN: False ====\n",
      "==== low level action taken: 3 N_pickable_item: 2 ====\n",
      "[SUCCESS] get raw data\n",
      "Qx: tensor([[0.4858, 0.4784, 0.5348]], device='cuda:0')\n",
      "Qy: tensor([[0.2745, 0.3764, 0.4876]], device='cuda:0')\n",
      "Qz: tensor([[0.4820, 0.3546, 0.2201]], device='cuda:0')\n",
      "Qyaw: tensor([[0.2490, 0.3299, 0.2502]], device='cuda:0')\n",
      "[MAX OUTPUT]\n",
      "[Q net] x: 2, y: 2, z: 0, yaw_ind: 1\n",
      "[SUCCESS] estimate actions from network\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[GRASP DISTANCE] 0.0\n",
      "[GRASP REWARD] R: 0.0\n",
      "[OVERALL REWARD] 0.0\n",
      "[ACTION TYPE]: 0\n",
      "Qx: tensor([[0.4795, 0.4123, 0.4541]], device='cuda:0')\n",
      "Qy: tensor([[0.2577, 0.4560, 0.4061]], device='cuda:0')\n",
      "Qz: tensor([[0.4712, 0.3453, 0.3487]], device='cuda:0')\n",
      "Qyaw: tensor([[0.1603, 0.2948, 0.2880]], device='cuda:0')\n",
      "[MAX OUTPUT]\n",
      "[Q net] x: 0, y: 1, z: 0, yaw_ind: 1\n",
      "[MOVE] move: [ 0.015  0.015 -0.02   0.   ] move index: [2, 2, 0, 1] next move index: [0, 1, 0, 1]\n",
      "target_Qx: 0.4732905328273773 Qx: 0.5347561240196228\n",
      "target_Qy: 0.45410701632499695 Qy: 0.48761606216430664\n",
      "target_Qz: 0.46841078996658325 Qz: 0.48197704553604126\n",
      "target_Qyaw: 0.30090460181236267 Qyaw: 0.3299095630645752\n",
      "loss_rank: None\n",
      "priority: 0.0014815515605732799\n",
      "[SUCCESS] append transition experience\n",
      "==== AGENT MODE ENABLE BC: False ENABLE RL: False FULL TRAIN: False ====\n",
      "==== low level action taken: 3 N_pickable_item: 2 ====\n",
      "[SUCCESS] get raw data\n",
      "Qx: tensor([[0.4796, 0.4123, 0.4543]], device='cuda:0')\n",
      "Qy: tensor([[0.2577, 0.4563, 0.4060]], device='cuda:0')\n",
      "Qz: tensor([[0.4713, 0.3453, 0.3491]], device='cuda:0')\n",
      "Qyaw: tensor([[0.1603, 0.2951, 0.2882]], device='cuda:0')\n",
      "[MAX OUTPUT]\n",
      "[Q net] x: 0, y: 1, z: 0, yaw_ind: 1\n",
      "[SUCCESS] estimate actions from network\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[GRASP DISTANCE] 0.0\n",
      "[GRASP REWARD] R: 0.0\n",
      "[OVERALL REWARD] 0.0\n",
      "[ACTION TYPE]: 0\n",
      "Qx: tensor([[0.5656, 0.5620, 0.5517]], device='cuda:0')\n",
      "Qy: tensor([[0.2468, 0.4826, 0.4588]], device='cuda:0')\n",
      "Qz: tensor([[0.5288, 0.4001, 0.3719]], device='cuda:0')\n",
      "Qyaw: tensor([[0.3509, 0.3306, 0.2969]], device='cuda:0')\n",
      "[MAX OUTPUT]\n",
      "[Q net] x: 0, y: 1, z: 0, yaw_ind: 0\n",
      "[MOVE] move: [-0.015  0.    -0.02   0.   ] move index: [0, 1, 0, 1] next move index: [0, 1, 0, 0]\n",
      "target_Qx: 0.5638391375541687 Qx: 0.4796063005924225\n",
      "target_Qy: 0.466420978307724 Qy: 0.456316739320755\n",
      "target_Qz: 0.5360536575317383 Qz: 0.4712895154953003\n",
      "target_Qyaw: 0.3604868948459625 Qyaw: 0.2950543165206909\n",
      "loss_rank: None\n",
      "priority: 0.003918270580470562\n",
      "[SUCCESS] append transition experience\n",
      "==== AGENT MODE ENABLE BC: False ENABLE RL: False FULL TRAIN: False ====\n",
      "==== low level action taken: 3 N_pickable_item: 2 ====\n",
      "[SUCCESS] get raw data\n",
      "Qx: tensor([[0.5623, 0.5424, 0.5564]], device='cuda:0')\n",
      "Qy: tensor([[0.2530, 0.4888, 0.4651]], device='cuda:0')\n",
      "Qz: tensor([[0.5343, 0.3877, 0.3858]], device='cuda:0')\n",
      "Qyaw: tensor([[0.3578, 0.3142, 0.3090]], device='cuda:0')\n",
      "[MAX OUTPUT]\n",
      "[Q net] x: 0, y: 1, z: 0, yaw_ind: 0\n",
      "[SUCCESS] estimate actions from network\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[GRASP DISTANCE] 0.0\n",
      "[GRASP REWARD] R: 0.0\n",
      "[OVERALL REWARD] 0.0\n",
      "[ACTION TYPE]: 0\n",
      "Qx: tensor([[0.5759, 0.5407, 0.5013]], device='cuda:0')\n",
      "Qy: tensor([[0.4943, 0.4721, 0.5953]], device='cuda:0')\n",
      "Qz: tensor([[0.6085, 0.5494, 0.3648]], device='cuda:0')\n",
      "Qyaw: tensor([[0.4123, 0.4998, 0.4116]], device='cuda:0')\n",
      "[MAX OUTPUT]\n",
      "[Q net] x: 0, y: 2, z: 0, yaw_ind: 1\n",
      "[MOVE] move: [-0.015      0.        -0.02      -0.1308997] move index: [0, 1, 0, 0] next move index: [0, 2, 0, 1]\n",
      "target_Qx: 0.5734942555427551 Qx: 0.5623213052749634\n",
      "target_Qy: 0.5864656567573547 Qy: 0.488781601190567\n",
      "target_Qz: 0.6105807423591614 Qz: 0.534343957901001\n",
      "target_Qyaw: 0.49682459235191345 Qyaw: 0.35782986879348755\n",
      "loss_rank: None\n",
      "priority: 0.008699647150933743\n",
      "[SUCCESS] append transition experience\n",
      "==== AGENT MODE ENABLE BC: False ENABLE RL: False FULL TRAIN: False ====\n",
      "==== low level action taken: 3 N_pickable_item: 2 ====\n",
      "[SUCCESS] get raw data\n",
      "Qx: tensor([[0.5787, 0.5340, 0.5025]], device='cuda:0')\n",
      "Qy: tensor([[0.4898, 0.4704, 0.5936]], device='cuda:0')\n",
      "Qz: tensor([[0.6097, 0.5500, 0.3684]], device='cuda:0')\n",
      "Qyaw: tensor([[0.4203, 0.5004, 0.4130]], device='cuda:0')\n",
      "[MAX OUTPUT]\n",
      "[Q net] x: 0, y: 2, z: 0, yaw_ind: 1\n",
      "[SUCCESS] estimate actions from network\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[GRASP DISTANCE] 0.0\n",
      "[GRASP REWARD] R: 0.0\n",
      "[OVERALL REWARD] 0.0\n",
      "[ACTION TYPE]: 0\n",
      "Qx: tensor([[0.7791, 0.7166, 0.6758]], device='cuda:0')\n",
      "Qy: tensor([[0.5755, 0.7102, 0.7006]], device='cuda:0')\n",
      "Qz: tensor([[0.7517, 0.6880, 0.5790]], device='cuda:0')\n",
      "Qyaw: tensor([[0.7107, 0.7484, 0.5582]], device='cuda:0')\n",
      "[MAX OUTPUT]\n",
      "[Q net] x: 0, y: 1, z: 0, yaw_ind: 1\n",
      "[MOVE] move: [-0.015  0.015 -0.02   0.   ] move index: [0, 2, 0, 1] next move index: [0, 1, 0, 1]\n",
      "target_Qx: 0.7485783100128174 Qx: 0.5786716938018799\n",
      "target_Qy: 0.6998278498649597 Qy: 0.5935871601104736\n",
      "target_Qz: 0.751035749912262 Qz: 0.6097338199615479\n",
      "target_Qyaw: 0.7300180196762085 Qyaw: 0.5004167556762695\n",
      "loss_rank: None\n",
      "priority: 0.02820958010852337\n",
      "[SUCCESS] append transition experience\n",
      "==== AGENT MODE ENABLE BC: False ENABLE RL: False FULL TRAIN: False ====\n",
      "==== low level action taken: 3 N_pickable_item: 2 ====\n",
      "[SUCCESS] get raw data\n",
      "Qx: tensor([[0.7746, 0.7049, 0.6828]], device='cuda:0')\n",
      "Qy: tensor([[0.5705, 0.6986, 0.7170]], device='cuda:0')\n",
      "Qz: tensor([[0.7406, 0.6864, 0.5707]], device='cuda:0')\n",
      "Qyaw: tensor([[0.6856, 0.7286, 0.5428]], device='cuda:0')\n",
      "[RANDOM OUTPUT]\n",
      "[Q net] x: 0, y: 1, z: 0, yaw_ind: 0\n",
      "[SUCCESS] estimate actions from network\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[GRASP DISTANCE] 0.0\n",
      "[GRASP REWARD] R: 0.0\n",
      "[OVERALL REWARD] 0.0\n",
      "[ACTION TYPE]: 0\n",
      "Qx: tensor([[0.7282, 0.6349, 0.4445]], device='cuda:0')\n",
      "Qy: tensor([[0.6460, 0.7772, 0.7534]], device='cuda:0')\n",
      "Qz: tensor([[0.7534, 0.6922, 0.5990]], device='cuda:0')\n",
      "Qyaw: tensor([[0.7223, 0.7449, 0.6264]], device='cuda:0')\n",
      "[MAX OUTPUT]\n",
      "[Q net] x: 0, y: 1, z: 0, yaw_ind: 1\n",
      "[MOVE] move: [-0.015      0.        -0.02      -0.1308997] move index: [0, 1, 0, 0] next move index: [0, 1, 0, 1]\n",
      "target_Qx: 0.7096021771430969 Qx: 0.7746458053588867\n",
      "target_Qy: 0.7743772864341736 Qy: 0.69859379529953\n",
      "target_Qz: 0.7631083726882935 Qz: 0.7406134009361267\n",
      "target_Qyaw: 0.752808690071106 Qyaw: 0.6856479048728943\n",
      "loss_rank: None\n",
      "priority: 0.0037476015277206898\n",
      "[SUCCESS] append transition experience\n",
      "==== AGENT MODE ENABLE BC: False ENABLE RL: False FULL TRAIN: False ====\n",
      "==== low level action taken: 3 N_pickable_item: 2 ====\n",
      "[SUCCESS] get raw data\n",
      "Qx: tensor([[0.7282, 0.6349, 0.4441]], device='cuda:0')\n",
      "Qy: tensor([[0.6463, 0.7775, 0.7537]], device='cuda:0')\n",
      "Qz: tensor([[0.7535, 0.6924, 0.5990]], device='cuda:0')\n",
      "Qyaw: tensor([[0.7225, 0.7450, 0.6264]], device='cuda:0')\n",
      "[MAX OUTPUT]\n",
      "[Q net] x: 0, y: 1, z: 0, yaw_ind: 1\n",
      "[SUCCESS] estimate actions from network\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[GRASP DISTANCE] 0.0\n",
      "[GRASP REWARD] R: 0.0\n",
      "[OVERALL REWARD] 0.0\n",
      "[ACTION TYPE]: 0\n",
      "Qx: tensor([[0.9159, 0.8557, 0.6438]], device='cuda:0')\n",
      "Qy: tensor([[0.7535, 0.7687, 0.9268]], device='cuda:0')\n",
      "Qz: tensor([[0.8222, 0.6719, 0.6391]], device='cuda:0')\n",
      "Qyaw: tensor([[0.8200, 0.8558, 0.7217]], device='cuda:0')\n",
      "[MAX OUTPUT]\n",
      "[Q net] x: 0, y: 2, z: 0, yaw_ind: 1\n",
      "[MOVE] move: [-0.015  0.    -0.02   0.   ] move index: [0, 1, 0, 1] next move index: [0, 2, 0, 1]\n",
      "target_Qx: 0.8911076784133911 Qx: 0.728234052658081\n",
      "target_Qy: 0.899313747882843 Qy: 0.7775079011917114\n",
      "target_Qz: 0.8258495926856995 Qz: 0.7534583806991577\n",
      "target_Qyaw: 0.8667323589324951 Qyaw: 0.7449730634689331\n",
      "loss_rank: None\n",
      "priority: 0.015357574447989464\n",
      "[SUCCESS] append transition experience\n",
      "==== AGENT MODE ENABLE BC: False ENABLE RL: False FULL TRAIN: False ====\n",
      "==== low level action taken: 3 N_pickable_item: 2 ====\n",
      "[SUCCESS] get raw data\n",
      "Qx: tensor([[0.9143, 0.8602, 0.6524]], device='cuda:0')\n",
      "Qy: tensor([[0.7513, 0.7707, 0.9282]], device='cuda:0')\n",
      "Qz: tensor([[0.8226, 0.6766, 0.6485]], device='cuda:0')\n",
      "Qyaw: tensor([[0.8163, 0.8572, 0.7231]], device='cuda:0')\n",
      "[MAX OUTPUT]\n",
      "[Q net] x: 0, y: 2, z: 0, yaw_ind: 1\n",
      "[SUCCESS] estimate actions from network\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[GRASP DISTANCE] 0.0\n",
      "[GRASP REWARD] R: 0.0\n",
      "[OVERALL REWARD] 0.0\n",
      "[ACTION TYPE]: 0\n",
      "Qx: tensor([[0.8533, 0.8695, 0.7683]], device='cuda:0')\n",
      "Qy: tensor([[0.8504, 0.9084, 0.8868]], device='cuda:0')\n",
      "Qz: tensor([[0.8520, 0.7591, 0.7655]], device='cuda:0')\n",
      "Qyaw: tensor([[0.8217, 0.8485, 0.7945]], device='cuda:0')\n",
      "[MAX OUTPUT]\n",
      "[Q net] x: 1, y: 1, z: 0, yaw_ind: 1\n",
      "[MOVE] move: [-0.015  0.015 -0.02   0.   ] move index: [0, 2, 0, 1] next move index: [1, 1, 0, 1]\n",
      "target_Qx: 0.8677484393119812 Qx: 0.914263129234314\n",
      "target_Qy: 0.8995260000228882 Qy: 0.9281712770462036\n",
      "target_Qz: 0.8418165445327759 Qz: 0.8225832581520081\n",
      "target_Qyaw: 0.8483964800834656 Qyaw: 0.857210636138916\n",
      "loss_rank: None\n",
      "priority: 0.0008579442510381341\n",
      "[SUCCESS] append transition experience\n",
      "==== AGENT MODE ENABLE BC: False ENABLE RL: False FULL TRAIN: False ====\n",
      "==== low level action taken: 3 N_pickable_item: 2 ====\n",
      "[SUCCESS] get raw data\n",
      "Qx: tensor([[0.8404, 0.8509, 0.7447]], device='cuda:0')\n",
      "Qy: tensor([[0.8536, 0.9047, 0.8722]], device='cuda:0')\n",
      "Qz: tensor([[0.8418, 0.7657, 0.7591]], device='cuda:0')\n",
      "Qyaw: tensor([[0.8177, 0.8170, 0.7751]], device='cuda:0')\n",
      "[MAX OUTPUT]\n",
      "[Q net] x: 1, y: 1, z: 0, yaw_ind: 0\n",
      "[SUCCESS] estimate actions from network\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[GRASP DISTANCE] 0.0\n",
      "[GRASP REWARD] R: 0.0\n",
      "[OVERALL REWARD] 0.0\n",
      "[ACTION TYPE]: 0\n",
      "Qx: tensor([[0.8978, 0.6692, 0.4976]], device='cuda:0')\n",
      "Qy: tensor([[0.7172, 0.7465, 0.7491]], device='cuda:0')\n",
      "Qz: tensor([[0.7997, 0.7683, 0.6560]], device='cuda:0')\n",
      "Qyaw: tensor([[0.6309, 0.8576, 0.6786]], device='cuda:0')\n",
      "[MAX OUTPUT]\n",
      "[Q net] x: 0, y: 2, z: 0, yaw_ind: 1\n",
      "[MOVE] move: [ 0.         0.        -0.02      -0.1308997] move index: [1, 1, 0, 0] next move index: [0, 2, 0, 1]\n",
      "target_Qx: 0.8908810615539551 Qx: 0.8508787155151367\n",
      "target_Qy: 0.7555462718009949 Qy: 0.9046550989151001\n",
      "target_Qz: 0.8206292390823364 Qz: 0.841808021068573\n",
      "target_Qyaw: 0.8884597420692444 Qyaw: 0.8176777958869934\n",
      "loss_rank: None\n",
      "priority: 0.0073230634443461895\n",
      "[SUCCESS] append transition experience\n",
      "==== AGENT MODE ENABLE BC: False ENABLE RL: False FULL TRAIN: False ====\n",
      "==== low level action taken: 3 N_pickable_item: 2 ====\n",
      "[SUCCESS] get raw data\n",
      "Qx: tensor([[0.8938, 0.6731, 0.4938]], device='cuda:0')\n",
      "Qy: tensor([[0.7176, 0.7618, 0.7447]], device='cuda:0')\n",
      "Qz: tensor([[0.8006, 0.7664, 0.6624]], device='cuda:0')\n",
      "Qyaw: tensor([[0.6362, 0.8566, 0.6789]], device='cuda:0')\n",
      "[MAX OUTPUT]\n",
      "[Q net] x: 0, y: 1, z: 0, yaw_ind: 1\n",
      "[SUCCESS] estimate actions from network\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[GRASP DISTANCE] 0.0\n",
      "[GRASP REWARD] R: 0.0\n",
      "[OVERALL REWARD] 0.0\n",
      "[ACTION TYPE]: 0\n",
      "Qx: tensor([[0.8875, 0.8779, 0.7462]], device='cuda:0')\n",
      "Qy: tensor([[0.6363, 0.7551, 0.9012]], device='cuda:0')\n",
      "Qz: tensor([[0.9258, 0.6918, 0.7395]], device='cuda:0')\n",
      "Qyaw: tensor([[0.5762, 0.9454, 0.8446]], device='cuda:0')\n",
      "[MAX OUTPUT]\n",
      "[Q net] x: 0, y: 2, z: 0, yaw_ind: 1\n",
      "[MOVE] move: [-0.015  0.    -0.02   0.   ] move index: [0, 1, 0, 1] next move index: [0, 2, 0, 1]\n",
      "target_Qx: 0.9077809453010559 Qx: 0.8938400745391846\n",
      "target_Qy: 0.9159789681434631 Qy: 0.7617506980895996\n",
      "target_Qz: 0.9424618482589722 Qz: 0.800603985786438\n",
      "target_Qyaw: 0.9381610155105591 Qyaw: 0.856552004814148\n",
      "loss_rank: None\n",
      "priority: 0.01269109733402729\n",
      "[SUCCESS] append transition experience\n",
      "==== AGENT MODE ENABLE BC: False ENABLE RL: False FULL TRAIN: False ====\n",
      "==== low level action taken: 3 N_pickable_item: 2 ====\n",
      "[SUCCESS] get raw data\n",
      "Qx: tensor([[0.9052, 0.8883, 0.7483]], device='cuda:0')\n",
      "Qy: tensor([[0.6632, 0.7735, 0.9301]], device='cuda:0')\n",
      "Qz: tensor([[0.9386, 0.6909, 0.7554]], device='cuda:0')\n",
      "Qyaw: tensor([[0.5923, 0.9411, 0.8587]], device='cuda:0')\n",
      "[MAX OUTPUT]\n",
      "[Q net] x: 0, y: 2, z: 0, yaw_ind: 1\n",
      "[SUCCESS] estimate actions from network\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[GRASP] grasp something\n",
      "[GRIPPER STATUS] NON_CLOSE_NON_OPEN\n",
      "[GRASP DISTANCE] 0.0\n",
      "[SUCESS] picked an item!\n",
      "[GRASP REWARD] R: 1.0\n",
      "[OVERALL REWARD] 1.0\n",
      "[ACTION TYPE]: 0\n",
      "Qx: tensor([[0.1466, 0.2906, 0.5149]], device='cuda:0')\n",
      "Qy: tensor([[0.4569, 0.4472, 0.6123]], device='cuda:0')\n",
      "Qz: tensor([[0.5423, 0.4452, 0.4658]], device='cuda:0')\n",
      "Qyaw: tensor([[0.3282, 0.4979, 0.2808]], device='cuda:0')\n",
      "[MAX OUTPUT]\n",
      "[Q net] x: 2, y: 2, z: 0, yaw_ind: 1\n",
      "[MOVE] move: [-0.015  0.015 -0.02   0.   ] move index: [0, 2, 0, 1] next move index: [2, 2, 0, 1]\n",
      "target_Qx: 1.0 Qx: 0.9051836729049683\n",
      "target_Qy: 1.0 Qy: 0.9301145076751709\n",
      "target_Qz: 1.0 Qz: 0.9385812878608704\n",
      "target_Qyaw: 1.0 Qyaw: 0.9410939812660217\n",
      "loss_rank: None\n",
      "priority: 0.00527907395735383\n",
      "[SUCCESS] append transition experience\n",
      "[SUCCESS] grasp an item successfully\n",
      "=== end of action ===\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[SUCCESS] return home\n",
      "[SUCCESS] return home\n",
      "[SUCCESS] GRASP OR PUSH ACTION\n",
      "[GRASP SUCCESS RATE] 0.8%/-inf% [500]\n",
      "[SUCCESS] save agent data\n",
      "==== episode: 0 ====\n",
      "[MIN_DISTANCE] inf\n",
      "N_step_x: 6.0, N_step_y: 12.0, N_step_z: 10.0, N_step_yaw: 5.0, N_step: 12.0\n",
      "[HLD net] max output: 0\n",
      "N_step_low_level: 25\n",
      "==== AGENT MODE ENABLE BC: False ENABLE RL: False FULL TRAIN: False ====\n",
      "==== low level action taken: 4 N_pickable_item: 1 ====\n",
      "[SUCCESS] get raw data\n",
      "Qx: tensor([[0.4503, 0.4134, 0.4393]], device='cuda:0')\n",
      "Qy: tensor([[0.4065, 0.4216, 0.5019]], device='cuda:0')\n",
      "Qz: tensor([[0.5112, 0.1973, 0.1039]], device='cuda:0')\n",
      "Qyaw: tensor([[0.2278, 0.3319, 0.2680]], device='cuda:0')\n",
      "[MAX OUTPUT]\n",
      "[Q net] x: 0, y: 2, z: 0, yaw_ind: 1\n",
      "[SUCCESS] estimate actions from network\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[GRASP DISTANCE] 0.0\n",
      "[GRASP REWARD] R: 0.0\n",
      "[OVERALL REWARD] 0.0\n",
      "[ACTION TYPE]: 0\n",
      "Qx: tensor([[0.4551, 0.4995, 0.4122]], device='cuda:0')\n",
      "Qy: tensor([[0.4153, 0.4605, 0.4724]], device='cuda:0')\n",
      "Qz: tensor([[0.4538, 0.2898, 0.2024]], device='cuda:0')\n",
      "Qyaw: tensor([[0.2444, 0.3265, 0.3489]], device='cuda:0')\n",
      "[MAX OUTPUT]\n",
      "[Q net] x: 1, y: 2, z: 0, yaw_ind: 2\n",
      "[MOVE] move: [-0.015  0.015 -0.02   0.   ] move index: [0, 2, 0, 1] next move index: [1, 2, 0, 2]\n",
      "target_Qx: 0.4709075391292572 Qx: 0.450334757566452\n",
      "target_Qy: 0.499947190284729 Qy: 0.5019009709358215\n",
      "target_Qz: 0.3773505687713623 Qz: 0.5112374424934387\n",
      "target_Qyaw: 0.3568848967552185 Qyaw: 0.3319126069545746\n",
      "loss_rank: None\n",
      "priority: 0.004744091536849737\n",
      "[SUCCESS] append transition experience\n",
      "==== AGENT MODE ENABLE BC: False ENABLE RL: False FULL TRAIN: False ====\n",
      "==== low level action taken: 4 N_pickable_item: 1 ====\n",
      "[SUCCESS] get raw data\n",
      "Qx: tensor([[0.4550, 0.5065, 0.3985]], device='cuda:0')\n",
      "Qy: tensor([[0.4073, 0.4445, 0.4744]], device='cuda:0')\n",
      "Qz: tensor([[0.4572, 0.3075, 0.2194]], device='cuda:0')\n",
      "Qyaw: tensor([[0.2400, 0.3411, 0.3510]], device='cuda:0')\n",
      "[MAX OUTPUT]\n",
      "[Q net] x: 1, y: 2, z: 0, yaw_ind: 2\n",
      "[SUCCESS] estimate actions from network\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[GRASP DISTANCE] 0.0\n",
      "[GRASP REWARD] R: 0.0\n",
      "[OVERALL REWARD] 0.0\n",
      "[ACTION TYPE]: 0\n",
      "Qx: tensor([[0.4505, 0.5557, 0.5844]], device='cuda:0')\n",
      "Qy: tensor([[0.4422, 0.5642, 0.5460]], device='cuda:0')\n",
      "Qz: tensor([[0.5479, 0.4094, 0.3887]], device='cuda:0')\n",
      "Qyaw: tensor([[0.2687, 0.3835, 0.1898]], device='cuda:0')\n",
      "[MAX OUTPUT]\n",
      "[Q net] x: 2, y: 1, z: 0, yaw_ind: 1\n",
      "[MOVE] move: [ 0.         0.015     -0.02       0.1308997] move index: [1, 2, 0, 2] next move index: [2, 1, 0, 1]\n",
      "target_Qx: 0.5692617297172546 Qx: 0.5065401792526245\n",
      "target_Qy: 0.5474601984024048 Qy: 0.4743879437446594\n",
      "target_Qz: 0.5316480994224548 Qz: 0.4572013020515442\n",
      "target_Qyaw: 0.39625999331474304 Qyaw: 0.35097289085388184\n",
      "loss_rank: None\n",
      "priority: 0.004216698929667473\n",
      "[SUCCESS] append transition experience\n",
      "==== AGENT MODE ENABLE BC: False ENABLE RL: False FULL TRAIN: False ====\n",
      "==== low level action taken: 4 N_pickable_item: 1 ====\n",
      "[SUCCESS] get raw data\n",
      "Qx: tensor([[0.4474, 0.5546, 0.5814]], device='cuda:0')\n",
      "Qy: tensor([[0.4383, 0.5541, 0.5482]], device='cuda:0')\n",
      "Qz: tensor([[0.5541, 0.3975, 0.3933]], device='cuda:0')\n",
      "Qyaw: tensor([[0.2805, 0.3886, 0.1826]], device='cuda:0')\n",
      "[MAX OUTPUT]\n",
      "[Q net] x: 2, y: 1, z: 0, yaw_ind: 1\n",
      "[SUCCESS] estimate actions from network\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[GRASP DISTANCE] 0.0\n",
      "[GRASP REWARD] R: 0.0\n",
      "[OVERALL REWARD] 0.0\n",
      "[ACTION TYPE]: 0\n",
      "Qx: tensor([[0.4203, 0.5455, 0.5042]], device='cuda:0')\n",
      "Qy: tensor([[0.3627, 0.3361, 0.5119]], device='cuda:0')\n",
      "Qz: tensor([[0.5086, 0.4856, 0.4856]], device='cuda:0')\n",
      "Qyaw: tensor([[0.3221, 0.4732, 0.4023]], device='cuda:0')\n",
      "[MAX OUTPUT]\n",
      "[Q net] x: 1, y: 2, z: 0, yaw_ind: 1\n",
      "[MOVE] move: [ 0.015  0.    -0.02   0.   ] move index: [2, 1, 0, 1] next move index: [1, 2, 0, 1]\n",
      "target_Qx: 0.5591127276420593 Qx: 0.5814236402511597\n",
      "target_Qy: 0.5061477422714233 Qy: 0.5541127920150757\n",
      "target_Qz: 0.4953227639198303 Qz: 0.5540902018547058\n",
      "target_Qyaw: 0.4661860764026642 Qyaw: 0.38856756687164307\n",
      "loss_rank: None\n",
      "priority: 0.003069167025387287\n",
      "[SUCCESS] append transition experience\n",
      "==== AGENT MODE ENABLE BC: False ENABLE RL: False FULL TRAIN: False ====\n",
      "==== low level action taken: 4 N_pickable_item: 1 ====\n",
      "[SUCCESS] get raw data\n",
      "Qx: tensor([[0.4200, 0.5417, 0.5040]], device='cuda:0')\n",
      "Qy: tensor([[0.3593, 0.3254, 0.5113]], device='cuda:0')\n",
      "Qz: tensor([[0.5058, 0.4813, 0.4844]], device='cuda:0')\n",
      "Qyaw: tensor([[0.3234, 0.4741, 0.4017]], device='cuda:0')\n",
      "[MAX OUTPUT]\n",
      "[Q net] x: 1, y: 2, z: 0, yaw_ind: 1\n",
      "[SUCCESS] estimate actions from network\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[GRASP DISTANCE] 0.0\n",
      "[GRASP REWARD] R: 0.0\n",
      "[OVERALL REWARD] 0.0\n",
      "[ACTION TYPE]: 0\n",
      "Qx: tensor([[0.4175, 0.5124, 0.5615]], device='cuda:0')\n",
      "Qy: tensor([[0.3552, 0.3457, 0.4950]], device='cuda:0')\n",
      "Qz: tensor([[0.5205, 0.5992, 0.5333]], device='cuda:0')\n",
      "Qyaw: tensor([[0.3557, 0.4042, 0.4351]], device='cuda:0')\n",
      "[MAX OUTPUT]\n",
      "[Q net] x: 2, y: 2, z: 1, yaw_ind: 2\n",
      "[MOVE] move: [ 0.     0.015 -0.02   0.   ] move index: [1, 2, 0, 1] next move index: [2, 2, 1, 2]\n",
      "target_Qx: 0.5569076538085938 Qx: 0.5417087078094482\n",
      "target_Qy: 0.480731725692749 Qy: 0.5112994313240051\n",
      "target_Qz: 0.5624340772628784 Qz: 0.5058043003082275\n",
      "target_Qyaw: 0.4234012961387634 Qyaw: 0.47408968210220337\n",
      "loss_rank: None\n",
      "priority: 0.0017354092560708523\n",
      "[SUCCESS] append transition experience\n",
      "==== AGENT MODE ENABLE BC: False ENABLE RL: False FULL TRAIN: False ====\n",
      "==== low level action taken: 4 N_pickable_item: 1 ====\n",
      "[SUCCESS] get raw data\n",
      "Qx: tensor([[0.4202, 0.5114, 0.5587]], device='cuda:0')\n",
      "Qy: tensor([[0.3548, 0.3459, 0.4971]], device='cuda:0')\n",
      "Qz: tensor([[0.5134, 0.5936, 0.5313]], device='cuda:0')\n",
      "Qyaw: tensor([[0.3542, 0.3999, 0.4395]], device='cuda:0')\n",
      "[MAX OUTPUT]\n",
      "[Q net] x: 2, y: 2, z: 1, yaw_ind: 2\n",
      "[SUCCESS] estimate actions from network\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[GRASP DISTANCE] 0.0\n",
      "[GRASP REWARD] R: 0.0\n",
      "[OVERALL REWARD] 0.0\n",
      "[ACTION TYPE]: 0\n",
      "Qx: tensor([[0.5236, 0.4784, 0.5847]], device='cuda:0')\n",
      "Qy: tensor([[0.3209, 0.4018, 0.5398]], device='cuda:0')\n",
      "Qz: tensor([[0.5832, 0.5137, 0.4500]], device='cuda:0')\n",
      "Qyaw: tensor([[0.3897, 0.5249, 0.4059]], device='cuda:0')\n",
      "[MAX OUTPUT]\n",
      "[Q net] x: 2, y: 2, z: 0, yaw_ind: 1\n",
      "[MOVE] move: [0.015     0.015     0.        0.1308997] move index: [2, 2, 1, 2] next move index: [2, 2, 0, 1]\n",
      "target_Qx: 0.5989375114440918 Qx: 0.5586827397346497\n",
      "target_Qy: 0.5454654693603516 Qy: 0.4970664381980896\n",
      "target_Qz: 0.5752999186515808 Qz: 0.5935898423194885\n",
      "target_Qyaw: 0.4866810142993927 Qyaw: 0.43947267532348633\n",
      "loss_rank: None\n",
      "priority: 0.001631515333428979\n",
      "[SUCCESS] append transition experience\n",
      "==== AGENT MODE ENABLE BC: False ENABLE RL: False FULL TRAIN: False ====\n",
      "==== low level action taken: 4 N_pickable_item: 1 ====\n",
      "[SUCCESS] get raw data\n",
      "Qx: tensor([[0.5198, 0.4889, 0.5819]], device='cuda:0')\n",
      "Qy: tensor([[0.3291, 0.3890, 0.5392]], device='cuda:0')\n",
      "Qz: tensor([[0.5842, 0.5193, 0.4440]], device='cuda:0')\n",
      "Qyaw: tensor([[0.3930, 0.5129, 0.3792]], device='cuda:0')\n",
      "[RANDOM OUTPUT]\n",
      "[Q net] x: 1, y: 0, z: 0, yaw_ind: 0\n",
      "[SUCCESS] estimate actions from network\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[GRASP DISTANCE] 0.0\n",
      "[GRASP REWARD] R: 0.0\n",
      "[OVERALL REWARD] 0.0\n",
      "[ACTION TYPE]: 0\n",
      "Qx: tensor([[0.4524, 0.4197, 0.5543]], device='cuda:0')\n",
      "Qy: tensor([[0.4060, 0.3820, 0.5369]], device='cuda:0')\n",
      "Qz: tensor([[0.5949, 0.6020, 0.5567]], device='cuda:0')\n",
      "Qyaw: tensor([[0.3853, 0.4376, 0.3143]], device='cuda:0')\n",
      "[MAX OUTPUT]\n",
      "[Q net] x: 2, y: 2, z: 1, yaw_ind: 1\n",
      "[MOVE] move: [ 0.        -0.015     -0.02      -0.1308997] move index: [1, 0, 0, 0] next move index: [2, 2, 1, 1]\n",
      "target_Qx: 0.5511309504508972 Qx: 0.48887088894844055\n",
      "target_Qy: 0.5447509288787842 Qy: 0.3291410803794861\n",
      "target_Qz: 0.568312406539917 Qz: 0.584178626537323\n",
      "target_Qyaw: 0.441799134016037 Qyaw: 0.39299634099006653\n",
      "loss_rank: None\n",
      "priority: 0.013249342329800129\n",
      "[SUCCESS] append transition experience\n",
      "==== AGENT MODE ENABLE BC: False ENABLE RL: False FULL TRAIN: False ====\n",
      "==== low level action taken: 4 N_pickable_item: 1 ====\n",
      "[SUCCESS] get raw data\n",
      "Qx: tensor([[0.4381, 0.4231, 0.5506]], device='cuda:0')\n",
      "Qy: tensor([[0.4204, 0.3851, 0.5381]], device='cuda:0')\n",
      "Qz: tensor([[0.5870, 0.6086, 0.5582]], device='cuda:0')\n",
      "Qyaw: tensor([[0.3727, 0.4289, 0.3353]], device='cuda:0')\n",
      "[MAX OUTPUT]\n",
      "[Q net] x: 2, y: 2, z: 1, yaw_ind: 1\n",
      "[SUCCESS] estimate actions from network\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[GRASP DISTANCE] 0.0\n",
      "[GRASP REWARD] R: 0.0\n",
      "[OVERALL REWARD] 0.0\n",
      "[ACTION TYPE]: 0\n",
      "Qx: tensor([[0.4750, 0.5096, 0.5375]], device='cuda:0')\n",
      "Qy: tensor([[0.4100, 0.4480, 0.5142]], device='cuda:0')\n",
      "Qz: tensor([[0.5922, 0.5944, 0.5779]], device='cuda:0')\n",
      "Qyaw: tensor([[0.3751, 0.4215, 0.3460]], device='cuda:0')\n",
      "[MAX OUTPUT]\n",
      "[Q net] x: 2, y: 2, z: 1, yaw_ind: 1\n",
      "[MOVE] move: [0.015 0.015 0.    0.   ] move index: [2, 2, 1, 1] next move index: [2, 2, 1, 1]\n",
      "target_Qx: 0.5302878618240356 Qx: 0.550621509552002\n",
      "target_Qy: 0.5053470134735107 Qy: 0.5380572080612183\n",
      "target_Qz: 0.5852063894271851 Qz: 0.6086359620094299\n",
      "target_Qyaw: 0.41341251134872437 Qyaw: 0.4289452135562897\n",
      "loss_rank: None\n",
      "priority: 0.0005684059578925371\n",
      "[SUCCESS] append transition experience\n",
      "==== AGENT MODE ENABLE BC: False ENABLE RL: False FULL TRAIN: False ====\n",
      "==== low level action taken: 4 N_pickable_item: 1 ====\n",
      "[SUCCESS] get raw data\n",
      "Qx: tensor([[0.4705, 0.5092, 0.5351]], device='cuda:0')\n",
      "Qy: tensor([[0.4097, 0.4472, 0.5149]], device='cuda:0')\n",
      "Qz: tensor([[0.5918, 0.5903, 0.5732]], device='cuda:0')\n",
      "Qyaw: tensor([[0.3764, 0.4242, 0.3422]], device='cuda:0')\n",
      "[MAX OUTPUT]\n",
      "[Q net] x: 2, y: 2, z: 0, yaw_ind: 1\n",
      "[SUCCESS] estimate actions from network\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[GRASP DISTANCE] 0.0\n",
      "[GRASP REWARD] R: 0.0\n",
      "[OVERALL REWARD] 0.0\n",
      "[ACTION TYPE]: 0\n",
      "Qx: tensor([[0.6732, 0.4589, 0.5195]], device='cuda:0')\n",
      "Qy: tensor([[0.4581, 0.4803, 0.5927]], device='cuda:0')\n",
      "Qz: tensor([[0.7289, 0.5008, 0.5456]], device='cuda:0')\n",
      "Qyaw: tensor([[0.4308, 0.5454, 0.3967]], device='cuda:0')\n",
      "[MAX OUTPUT]\n",
      "[Q net] x: 0, y: 2, z: 0, yaw_ind: 1\n",
      "[MOVE] move: [ 0.015  0.015 -0.02   0.   ] move index: [2, 2, 0, 1] next move index: [0, 2, 0, 1]\n",
      "target_Qx: 0.6615777015686035 Qx: 0.5350595116615295\n",
      "target_Qy: 0.5731449723243713 Qy: 0.5149282813072205\n",
      "target_Qz: 0.7160108685493469 Qz: 0.5918331146240234\n",
      "target_Qyaw: 0.532112181186676 Qyaw: 0.4242475926876068\n",
      "loss_rank: None\n",
      "priority: 0.011612730100750923\n",
      "[SUCCESS] append transition experience\n",
      "==== AGENT MODE ENABLE BC: False ENABLE RL: False FULL TRAIN: False ====\n",
      "==== low level action taken: 4 N_pickable_item: 1 ====\n",
      "[SUCCESS] get raw data\n",
      "Qx: tensor([[0.6789, 0.4566, 0.5197]], device='cuda:0')\n",
      "Qy: tensor([[0.4553, 0.4817, 0.5842]], device='cuda:0')\n",
      "Qz: tensor([[0.7192, 0.5024, 0.5507]], device='cuda:0')\n",
      "Qyaw: tensor([[0.4049, 0.5567, 0.3796]], device='cuda:0')\n",
      "[RANDOM OUTPUT]\n",
      "[Q net] x: 2, y: 1, z: 1, yaw_ind: 2\n",
      "[SUCCESS] estimate actions from network\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[GRASP DISTANCE] 0.0\n",
      "[GRASP REWARD] R: 0.0\n",
      "[OVERALL REWARD] 0.0\n",
      "[ACTION TYPE]: 0\n",
      "Qx: tensor([[0.4983, 0.5009, 0.5991]], device='cuda:0')\n",
      "Qy: tensor([[0.4718, 0.4755, 0.6231]], device='cuda:0')\n",
      "Qz: tensor([[0.6608, 0.6376, 0.5941]], device='cuda:0')\n",
      "Qyaw: tensor([[0.3918, 0.6151, 0.3855]], device='cuda:0')\n",
      "[MAX OUTPUT]\n",
      "[Q net] x: 2, y: 2, z: 0, yaw_ind: 1\n",
      "[MOVE] move: [0.015     0.        0.        0.1308997] move index: [2, 1, 1, 2] next move index: [2, 2, 0, 1]\n",
      "target_Qx: 0.586807131767273 Qx: 0.5197449922561646\n",
      "target_Qy: 0.6179306507110596 Qy: 0.48169174790382385\n",
      "target_Qz: 0.6676779389381409 Qz: 0.5024300813674927\n",
      "target_Qyaw: 0.5980230569839478 Qyaw: 0.3795933723449707\n",
      "loss_rank: None\n",
      "priority: 0.024519188329577446\n",
      "[SUCCESS] append transition experience\n",
      "==== AGENT MODE ENABLE BC: False ENABLE RL: False FULL TRAIN: False ====\n",
      "==== low level action taken: 4 N_pickable_item: 1 ====\n",
      "[SUCCESS] get raw data\n",
      "Qx: tensor([[0.4963, 0.5011, 0.5989]], device='cuda:0')\n",
      "Qy: tensor([[0.4704, 0.4754, 0.6207]], device='cuda:0')\n",
      "Qz: tensor([[0.6595, 0.6351, 0.5898]], device='cuda:0')\n",
      "Qyaw: tensor([[0.3916, 0.6148, 0.3904]], device='cuda:0')\n",
      "[MAX OUTPUT]\n",
      "[Q net] x: 2, y: 2, z: 0, yaw_ind: 1\n",
      "[SUCCESS] estimate actions from network\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[GRASP DISTANCE] 0.0\n",
      "[GRASP REWARD] R: 0.0\n",
      "[OVERALL REWARD] 0.0\n",
      "[ACTION TYPE]: 0\n",
      "Qx: tensor([[0.4185, 0.5438, 0.5421]], device='cuda:0')\n",
      "Qy: tensor([[0.4069, 0.4364, 0.5973]], device='cuda:0')\n",
      "Qz: tensor([[0.6369, 0.7224, 0.5840]], device='cuda:0')\n",
      "Qyaw: tensor([[0.3592, 0.6267, 0.5488]], device='cuda:0')\n",
      "[MAX OUTPUT]\n",
      "[Q net] x: 1, y: 2, z: 1, yaw_ind: 1\n",
      "[MOVE] move: [ 0.015  0.015 -0.02   0.   ] move index: [2, 2, 0, 1] next move index: [1, 2, 1, 1]\n",
      "target_Qx: 0.560348391532898 Qx: 0.5988962054252625\n",
      "target_Qy: 0.626767098903656 Qy: 0.6206739544868469\n",
      "target_Qz: 0.7216668128967285 Qz: 0.6594966650009155\n",
      "target_Qyaw: 0.6422221064567566 Qyaw: 0.614774227142334\n",
      "loss_rank: None\n",
      "priority: 0.0015353932976722717\n",
      "[SUCCESS] append transition experience\n",
      "==== AGENT MODE ENABLE BC: False ENABLE RL: False FULL TRAIN: False ====\n",
      "==== low level action taken: 4 N_pickable_item: 1 ====\n",
      "[SUCCESS] get raw data\n",
      "Qx: tensor([[0.4331, 0.5462, 0.5440]], device='cuda:0')\n",
      "Qy: tensor([[0.4130, 0.4350, 0.5963]], device='cuda:0')\n",
      "Qz: tensor([[0.6387, 0.7270, 0.5883]], device='cuda:0')\n",
      "Qyaw: tensor([[0.3570, 0.6261, 0.5343]], device='cuda:0')\n",
      "[MAX OUTPUT]\n",
      "[Q net] x: 1, y: 2, z: 1, yaw_ind: 1\n",
      "[SUCCESS] estimate actions from network\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[GRASP DISTANCE] 0.0\n",
      "[GRASP REWARD] R: 0.0\n",
      "[OVERALL REWARD] 0.0\n",
      "[ACTION TYPE]: 0\n",
      "Qx: tensor([[0.4229, 0.6147, 0.5935]], device='cuda:0')\n",
      "Qy: tensor([[0.5968, 0.4348, 0.6360]], device='cuda:0')\n",
      "Qz: tensor([[0.6538, 0.5747, 0.6043]], device='cuda:0')\n",
      "Qyaw: tensor([[0.4576, 0.6131, 0.5247]], device='cuda:0')\n",
      "[MAX OUTPUT]\n",
      "[Q net] x: 1, y: 2, z: 0, yaw_ind: 1\n",
      "[MOVE] move: [0.    0.015 0.    0.   ] move index: [1, 2, 1, 1] next move index: [1, 2, 0, 1]\n",
      "target_Qx: 0.5917184948921204 Qx: 0.5462470650672913\n",
      "target_Qy: 0.6438025832176208 Qy: 0.5963237881660461\n",
      "target_Qz: 0.6516750454902649 Qz: 0.726966917514801\n",
      "target_Qyaw: 0.5856451392173767 Qyaw: 0.6261350512504578\n",
      "loss_rank: None\n",
      "priority: 0.002907546702772379\n",
      "[SUCCESS] append transition experience\n",
      "==== AGENT MODE ENABLE BC: False ENABLE RL: False FULL TRAIN: False ====\n",
      "==== low level action taken: 4 N_pickable_item: 1 ====\n",
      "[SUCCESS] get raw data\n",
      "Qx: tensor([[0.4407, 0.6043, 0.5669]], device='cuda:0')\n",
      "Qy: tensor([[0.6098, 0.4680, 0.6514]], device='cuda:0')\n",
      "Qz: tensor([[0.6563, 0.6037, 0.6099]], device='cuda:0')\n",
      "Qyaw: tensor([[0.4707, 0.6338, 0.5599]], device='cuda:0')\n",
      "[MAX OUTPUT]\n",
      "[Q net] x: 1, y: 2, z: 0, yaw_ind: 1\n",
      "[SUCCESS] estimate actions from network\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[GRASP DISTANCE] 0.0\n",
      "[GRASP REWARD] R: 0.0\n",
      "[OVERALL REWARD] 0.0\n",
      "[ACTION TYPE]: 0\n",
      "Qx: tensor([[0.5894, 0.6781, 0.6878]], device='cuda:0')\n",
      "Qy: tensor([[0.6480, 0.5693, 0.7563]], device='cuda:0')\n",
      "Qz: tensor([[0.7963, 0.6849, 0.6575]], device='cuda:0')\n",
      "Qyaw: tensor([[0.4754, 0.5946, 0.6448]], device='cuda:0')\n",
      "[MAX OUTPUT]\n",
      "[Q net] x: 2, y: 2, z: 0, yaw_ind: 2\n",
      "[MOVE] move: [ 0.     0.015 -0.02   0.   ] move index: [1, 2, 0, 1] next move index: [2, 2, 0, 2]\n",
      "target_Qx: 0.6878217458724976 Qx: 0.6042508482933044\n",
      "target_Qy: 0.7448420524597168 Qy: 0.6513661742210388\n",
      "target_Qz: 0.7842557430267334 Qz: 0.6562515497207642\n",
      "target_Qyaw: 0.629688024520874 Qyaw: 0.6338144540786743\n",
      "loss_rank: None\n",
      "priority: 0.008030983619391918\n",
      "[SUCCESS] append transition experience\n",
      "==== AGENT MODE ENABLE BC: False ENABLE RL: False FULL TRAIN: False ====\n",
      "==== low level action taken: 4 N_pickable_item: 1 ====\n",
      "[SUCCESS] get raw data\n",
      "Qx: tensor([[0.5874, 0.6668, 0.6890]], device='cuda:0')\n",
      "Qy: tensor([[0.6516, 0.5567, 0.7619]], device='cuda:0')\n",
      "Qz: tensor([[0.7888, 0.6809, 0.6472]], device='cuda:0')\n",
      "Qyaw: tensor([[0.4653, 0.5778, 0.6476]], device='cuda:0')\n",
      "[MAX OUTPUT]\n",
      "[Q net] x: 2, y: 2, z: 0, yaw_ind: 2\n",
      "[SUCCESS] estimate actions from network\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[GRASP DISTANCE] 0.0\n",
      "[GRASP REWARD] R: 0.0\n",
      "[OVERALL REWARD] 0.0\n",
      "[ACTION TYPE]: 0\n",
      "Qx: tensor([[0.5223, 0.5262, 0.4603]], device='cuda:0')\n",
      "Qy: tensor([[0.4590, 0.3975, 0.5869]], device='cuda:0')\n",
      "Qz: tensor([[0.5825, 0.6133, 0.5736]], device='cuda:0')\n",
      "Qyaw: tensor([[0.4324, 0.4010, 0.6029]], device='cuda:0')\n",
      "[MAX OUTPUT]\n",
      "[Q net] x: 1, y: 2, z: 1, yaw_ind: 2\n",
      "[MOVE] move: [ 0.015      0.015     -0.02       0.1308997] move index: [2, 2, 0, 2] next move index: [1, 2, 1, 2]\n",
      "target_Qx: 0.5557735562324524 Qx: 0.6890354156494141\n",
      "target_Qy: 0.5228123068809509 Qy: 0.7619099617004395\n",
      "target_Qz: 0.6114339828491211 Qz: 0.7888479828834534\n",
      "target_Qyaw: 0.5740318298339844 Qyaw: 0.6475762128829956\n",
      "loss_rank: None\n",
      "priority: 0.027952730655670166\n",
      "[SUCCESS] append transition experience\n",
      "==== AGENT MODE ENABLE BC: False ENABLE RL: False FULL TRAIN: False ====\n",
      "==== low level action taken: 4 N_pickable_item: 1 ====\n",
      "[SUCCESS] get raw data\n",
      "Qx: tensor([[0.5093, 0.5350, 0.4919]], device='cuda:0')\n",
      "Qy: tensor([[0.4566, 0.3809, 0.6014]], device='cuda:0')\n",
      "Qz: tensor([[0.5938, 0.6316, 0.5602]], device='cuda:0')\n",
      "Qyaw: tensor([[0.4063, 0.3908, 0.5654]], device='cuda:0')\n",
      "[MAX OUTPUT]\n",
      "[Q net] x: 1, y: 2, z: 1, yaw_ind: 2\n",
      "[SUCCESS] estimate actions from network\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[GRASP DISTANCE] 0.0\n",
      "[GRASP REWARD] R: 0.0\n",
      "[OVERALL REWARD] 0.0\n",
      "[ACTION TYPE]: 0\n",
      "Qx: tensor([[0.8346, 0.8859, 0.8822]], device='cuda:0')\n",
      "Qy: tensor([[0.7157, 0.7296, 0.9242]], device='cuda:0')\n",
      "Qz: tensor([[0.9228, 0.8080, 0.8311]], device='cuda:0')\n",
      "Qyaw: tensor([[0.7291, 0.9394, 0.7747]], device='cuda:0')\n",
      "[MAX OUTPUT]\n",
      "[Q net] x: 1, y: 2, z: 0, yaw_ind: 1\n",
      "[MOVE] move: [0.        0.015     0.        0.1308997] move index: [1, 2, 1, 2] next move index: [1, 2, 0, 1]\n",
      "target_Qx: 0.8879486322402954 Qx: 0.5350013375282288\n",
      "target_Qy: 0.8958768844604492 Qy: 0.601433515548706\n",
      "target_Qz: 0.9007722735404968 Qz: 0.631618320941925\n",
      "target_Qyaw: 0.9140505194664001 Qyaw: 0.5653882622718811\n",
      "loss_rank: None\n",
      "priority: 0.10131947696208954\n",
      "[SUCCESS] append transition experience\n",
      "==== AGENT MODE ENABLE BC: False ENABLE RL: False FULL TRAIN: False ====\n",
      "==== low level action taken: 4 N_pickable_item: 1 ====\n",
      "[SUCCESS] get raw data\n",
      "Qx: tensor([[0.8513, 0.8922, 0.8942]], device='cuda:0')\n",
      "Qy: tensor([[0.7194, 0.7369, 0.9216]], device='cuda:0')\n",
      "Qz: tensor([[0.9256, 0.8185, 0.8270]], device='cuda:0')\n",
      "Qyaw: tensor([[0.7493, 0.9415, 0.7613]], device='cuda:0')\n",
      "[MAX OUTPUT]\n",
      "[Q net] x: 2, y: 2, z: 0, yaw_ind: 1\n",
      "[SUCCESS] estimate actions from network\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[GRASP DISTANCE] 0.0\n",
      "[GRASP REWARD] R: 0.0\n",
      "[OVERALL REWARD] 0.0\n",
      "[ACTION TYPE]: 0\n",
      "Qx: tensor([[0.7531, 0.8460, 0.6816]], device='cuda:0')\n",
      "Qy: tensor([[0.7369, 0.8940, 0.8064]], device='cuda:0')\n",
      "Qz: tensor([[0.8792, 0.8472, 0.6810]], device='cuda:0')\n",
      "Qyaw: tensor([[0.7619, 0.8709, 0.7957]], device='cuda:0')\n",
      "[MAX OUTPUT]\n",
      "[Q net] x: 1, y: 1, z: 0, yaw_ind: 1\n",
      "[MOVE] move: [ 0.015  0.015 -0.02   0.   ] move index: [2, 2, 0, 1] next move index: [1, 1, 0, 1]\n",
      "target_Qx: 0.8328197598457336 Qx: 0.8942106366157532\n",
      "target_Qy: 0.8680338859558105 Qy: 0.9216360449790955\n",
      "target_Qz: 0.8612181544303894 Qz: 0.9256446957588196\n",
      "target_Qyaw: 0.8435110449790955 Qyaw: 0.9415357112884521\n",
      "loss_rank: None\n",
      "priority: 0.0051004113629460335\n",
      "[SUCCESS] append transition experience\n",
      "==== AGENT MODE ENABLE BC: False ENABLE RL: False FULL TRAIN: False ====\n",
      "==== low level action taken: 4 N_pickable_item: 1 ====\n",
      "[SUCCESS] get raw data\n",
      "Qx: tensor([[0.6830, 0.7976, 0.6125]], device='cuda:0')\n",
      "Qy: tensor([[0.6449, 0.8480, 0.7275]], device='cuda:0')\n",
      "Qz: tensor([[0.8028, 0.7639, 0.6365]], device='cuda:0')\n",
      "Qyaw: tensor([[0.7347, 0.8433, 0.7513]], device='cuda:0')\n",
      "[MAX OUTPUT]\n",
      "[Q net] x: 1, y: 1, z: 0, yaw_ind: 1\n",
      "[SUCCESS] estimate actions from network\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[GRASP] grasp something\n",
      "[GRIPPER STATUS] NON_CLOSE_NON_OPEN\n",
      "[GRASP DISTANCE] 0.0\n",
      "[SUCESS] picked an item!\n",
      "[GRASP REWARD] R: 1.0\n",
      "[OVERALL REWARD] 1.0\n",
      "[ACTION TYPE]: 0\n",
      "Qx: tensor([[0.5420, 0.6905, 0.4322]], device='cuda:0')\n",
      "Qy: tensor([[0.4922, 0.6545, 0.6063]], device='cuda:0')\n",
      "Qz: tensor([[0.6623, 0.4485, 0.5816]], device='cuda:0')\n",
      "Qyaw: tensor([[0.2794, 0.5722, 0.4591]], device='cuda:0')\n",
      "[MAX OUTPUT]\n",
      "[Q net] x: 1, y: 1, z: 0, yaw_ind: 1\n",
      "[MOVE] move: [ 0.    0.   -0.02  0.  ] move index: [1, 1, 0, 1] next move index: [1, 1, 0, 1]\n",
      "target_Qx: 1.0 Qx: 0.7975986003875732\n",
      "target_Qy: 1.0 Qy: 0.8479512929916382\n",
      "target_Qz: 1.0 Qz: 0.802794873714447\n",
      "target_Qyaw: 1.0 Qyaw: 0.843271017074585\n",
      "loss_rank: None\n",
      "priority: 0.03188474476337433\n",
      "[SUCCESS] append transition experience\n",
      "[SUCCESS] finish one episode\n",
      "=== end of action ===\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[SUCCESS] return home\n",
      "[SUCCESS] return home\n",
      "[SUCCESS] GRASP OR PUSH ACTION\n",
      "[GRASP SUCCESS RATE] 1.0%/-inf% [500]\n",
      "[SUCCESS] save agent data\n",
      "=== end of episode ===\n",
      "[SUCCESS] save agent data\n",
      "[SUCCESS] save agent data\n"
     ]
    }
   ],
   "source": [
    "# agent.interact(max_episode = 1, \n",
    "#                lla_mode = constants.BC_RL,\n",
    "#                hld_mode = constants.HLD_MODE,\n",
    "#                is_eval = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.hld_mode = constants.HLD_MODE\n",
    "agent.is_eval  = True\n",
    "agent.load_agent_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "completion rate (eval): 1.0%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7eaf9e14af40>]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "CR_rate = np.sum(agent.CR_eval)/agent.max_result_window_eval\n",
    "print(f\"completion rate (eval): {CR_rate*100.}%\")\n",
    "\n",
    "plt.plot(agent.CR_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AGS rate (eval): 100.0%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7eaf9d910730>]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "AGS_rate = np.sum(np.array(agent.AGS_eval)*np.array(agent.CR_eval))/(np.array(agent.CR_eval) > 0).sum()\n",
    "print(f\"AGS rate (eval): {AGS_rate*100.}%\")\n",
    "\n",
    "plt.plot(agent.AGS_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ATC rate (eval): 5.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.0, 25.0)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ATC_rate = np.sum(np.array(agent.ATC_eval)*np.array(agent.CR_eval))/(np.array(agent.CR_eval) > 0).sum()\n",
    "print(f\"ATC rate (eval): {ATC_rate}\")\n",
    "\n",
    "plt.plot(agent.ATC_eval, '--')\n",
    "plt.ylim([0, 25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(np.array(agent.CR_eval) > 0).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
