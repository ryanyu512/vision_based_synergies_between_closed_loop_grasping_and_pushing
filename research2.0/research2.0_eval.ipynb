{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MAX_POSSIBLE_DIST]: 0.4428093360578569\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "import utils\n",
    "import torch\n",
    "import constants\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from env import Env\n",
    "from agent import Agent\n",
    "from torchsummary import summary\n",
    "from torch.distributions import Normal, Categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialise Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "workspace space: \n",
      "[[-0.26   0.04 ]\n",
      " [ 0.435  0.685]\n",
      " [ 0.     0.4  ]]\n"
     ]
    }
   ],
   "source": [
    "#initialise environment\n",
    "min_x, max_x =  -0.110 - 0.150,   -0.110 + 0.150\n",
    "min_y, max_y =   0.560 - 0.125,    0.560 + 0.125\n",
    "min_z, max_z =               0,              0.4 \n",
    "\n",
    "workspace_lim = np.asarray([[min_x, max_x], \n",
    "                            [min_y, max_y],\n",
    "                            [min_z, max_z]])\n",
    "\n",
    "print(f\"workspace space: \\n{workspace_lim}\")\n",
    "\n",
    "obj_dir = 'objects/blocks/'\n",
    "N_obj   = 5\n",
    "\n",
    "env = Env(obj_dir, N_obj, workspace_lim, cluttered_mode= True, is_debug = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialise Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n",
      "[SUCCESS] initialise environment\n",
      "[SUCCESS] initialise networks\n"
     ]
    }
   ],
   "source": [
    "agent = Agent(env, \n",
    "              max_memory_size = 100000, \n",
    "              max_memory_size_rl = 200000,\n",
    "              max_memory_size_hld = 50000,\n",
    "              is_debug = True, \n",
    "              N_batch = 512, \n",
    "              N_batch_hld = 512, \n",
    "              lr = 1e-4, \n",
    "              hld_lr = 1e-4,\n",
    "              tau = 0.05,\n",
    "              tau_hld = 0.05,\n",
    "              max_action_taken = 50,\n",
    "              max_result_window = 500,\n",
    "              max_result_window_hld = 250,\n",
    "              max_result_window_eval = 100,\n",
    "              max_stage1_episode = 200,\n",
    "              N_grasp_step = 25, #define the maximum step for low-level grasping network\n",
    "              N_push_step = 25, #define the maximum step for low-level pushing network\n",
    "              success_rate_threshold = 0.7,\n",
    "              checkpt_dir_agent=\"/media/ryan/Seagate/research_proj_backup/research_2.0/logs/agent\", \n",
    "              checkpt_dir_models=\"/media/ryan/Seagate/research_proj_backup/research_2.0/logs/models\",\n",
    "              exp_dir_expert=\"/media/ryan/Seagate/research_proj_backup/research_2.0/logs/exp_expert\", \n",
    "              exp_dir_rl=\"/media/ryan/Seagate/research_proj_backup/research_2.0/logs/exp_rl\",\n",
    "              exp_dir_hld=\"/media/ryan/Seagate/research_proj_backup/research_2.0/logs/exp_hld\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[FAIL] load agent data\n",
      "[LOAD MODEL] load hld-net best model\n",
      "[SUCCESS] load hld-net model\n",
      "[LOAD MODEL] load grasp best model\n",
      "[SUCCESS] load grasp model\n",
      "[LOAD MODEL] load push best model\n",
      "[SUCCESS] load push model\n",
      "[SUCCESS] restart environment\n",
      "['4.obj']\n",
      "item 0: shape_0, path index: 0, pose: [-0.11730983942470956, 0.5715850001222249, 0.025, 3.2787621502348907, 5.307921774744696, 3.9732943339674485]\n",
      "item 1: shape_1, path index: 0, pose: [-0.12245787476808549, 0.5492379108922224, 0.025, 4.528027987496574, 0.04842365333390224, 6.0052344809520894]\n",
      "item 2: shape_2, path index: 0, pose: [-0.10298928329072685, 0.5488547009253889, 0.025, 4.399928070594247, 5.828672411107015, 4.673738393954171]\n",
      "item 3: shape_3, path index: 0, pose: [-0.10121218467719272, 0.5656672097544398, 0.025, 4.921709228580442, 1.7229663656724368, 5.107444008538859]\n",
      "item 4: shape_4, path index: 0, pose: [-0.11227127571555368, 0.5724147660049947, 0.025, 1.6128470710759497, 3.1136979015071873, 5.7609100230518235]\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "==== episode: 0 ====\n",
      "[MIN_DISTANCE] 0.012447500427175578\n",
      "[compute_push_path]: 0.05000295694148177\n",
      "N_step_x: 5.0, N_step_y: 1.0, N_step_z: 10.0, N_step_yaw: 5.0, N_step: 10.0\n",
      "[MIN_DISTANCE] 0.012447500427175578\n",
      "[MIN_DISTANCE] 0.0212146500208138\n",
      "[MIN_DISTANCE] 0.0212146500208138\n",
      "[MIN_DISTANCE] 0.021658823544153286\n",
      "N_step_low_level: 12\n",
      "==== EXPERT MODE ENABLE BC: False ENABLE RL: False FULL TRAIN: False ====\n",
      "==== low level action taken: 0 N_pickable_item: 5 ====\n",
      "[SUCCESS] get raw data\n",
      "Qx: tensor([[0.5299, 0.4676, 0.4924]], device='cuda:0')\n",
      "Qy: tensor([[0.4623, 0.3923, 0.5287]], device='cuda:0')\n",
      "Qz: tensor([[0.5175, 0.4828, 0.4135]], device='cuda:0')\n",
      "Qyaw: tensor([[0.5482, 0.4063, 0.4669]], device='cuda:0')\n",
      "[MAX OUTPUT]\n",
      "[Q net] x: 0, y: 2, z: 0, yaw_ind: 0\n",
      "[SUCCESS] estimate actions from network\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "gripper tip height: 0.24916064127397114\n",
      "[PUSH REWARD] R: 0.0\n",
      "[OVERALL REWARD] 0.0\n",
      "[ACTION TYPE]: 1\n",
      "[MOVE] move: [-0.015     -0.015     -0.02      -0.2617994] move index: [0, 0, 0, 0]\n",
      "next move: tensor([ 0.0000,  0.0000, -0.0200,  0.0000], device='cuda:0') next move index: [1, 1, 0, 1]\n",
      "target_Qx: 0.5907436609268188 Qx: 0.5298884510993958\n",
      "target_Qy: 0.5837634801864624 Qy: 0.46228915452957153\n",
      "target_Qz: 0.5479486584663391 Qz: 0.5174688696861267\n",
      "target_Qyaw: 0.5828313231468201 Qyaw: 0.5481652617454529\n",
      "loss_rank: 0.03173080459237099\n",
      "priority: 0.03687833622097969\n",
      "[SUCCESS] append transition experience\n",
      "==== EXPERT MODE ENABLE BC: False ENABLE RL: False FULL TRAIN: False ====\n",
      "==== low level action taken: 0 N_pickable_item: 5 ====\n",
      "[SUCCESS] get raw data\n",
      "Qx: tensor([[0.4580, 0.5892, 0.5661]], device='cuda:0')\n",
      "Qy: tensor([[0.5382, 0.6119, 0.6012]], device='cuda:0')\n",
      "Qz: tensor([[0.5678, 0.5621, 0.4923]], device='cuda:0')\n",
      "Qyaw: tensor([[0.5470, 0.5579, 0.5547]], device='cuda:0')\n",
      "[MAX OUTPUT]\n",
      "[Q net] x: 1, y: 1, z: 0, yaw_ind: 1\n",
      "[SUCCESS] estimate actions from network\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "gripper tip height: 0.2275246372116419\n",
      "[PUSH REWARD] R: 0.0\n",
      "[OVERALL REWARD] 0.0\n",
      "[ACTION TYPE]: 1\n",
      "[MOVE] move: [ 0.    0.   -0.02  0.  ] move index: [1, 1, 0, 1]\n",
      "next move: tensor([-0.0150,  0.0000, -0.0200, -0.2618], device='cuda:0') next move index: [0, 1, 0, 0]\n",
      "target_Qx: 0.6393640637397766 Qx: 0.5891565084457397\n",
      "target_Qy: 0.7437088489532471 Qy: 0.6119316816329956\n",
      "target_Qz: 0.6980029344558716 Qz: 0.5677870512008667\n",
      "target_Qyaw: 0.6239723563194275 Qyaw: 0.557887613773346\n",
      "loss_rank: 0.04143165424466133\n",
      "priority: 0.05173400044441223\n",
      "[SUCCESS] append transition experience\n",
      "==== EXPERT MODE ENABLE BC: False ENABLE RL: False FULL TRAIN: False ====\n",
      "==== low level action taken: 0 N_pickable_item: 5 ====\n",
      "[SUCCESS] get raw data\n",
      "Qx: tensor([[0.6467, 0.7703, 0.6630]], device='cuda:0')\n",
      "Qy: tensor([[0.7311, 0.7426, 0.6102]], device='cuda:0')\n",
      "Qz: tensor([[0.6931, 0.6938, 0.5703]], device='cuda:0')\n",
      "Qyaw: tensor([[0.6199, 0.6529, 0.6305]], device='cuda:0')\n",
      "[MAX OUTPUT]\n",
      "[Q net] x: 1, y: 1, z: 1, yaw_ind: 1\n",
      "[SUCCESS] estimate actions from network\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "gripper tip height: 0.20800099931289612\n",
      "[PUSH REWARD] R: 0.0\n",
      "[OVERALL REWARD] 0.0\n",
      "[ACTION TYPE]: 1\n",
      "[MOVE] move: [-0.015      0.        -0.02      -0.2617994] move index: [0, 1, 0, 0]\n",
      "next move: tensor([ 0.0000,  0.0000, -0.0200,  0.0000], device='cuda:0') next move index: [1, 1, 0, 1]\n",
      "target_Qx: 0.6759670972824097 Qx: 0.6467054486274719\n",
      "target_Qy: 0.759635329246521 Qy: 0.7426356673240662\n",
      "target_Qz: 0.7138628959655762 Qz: 0.6930733919143677\n",
      "target_Qyaw: 0.6976540684700012 Qyaw: 0.6199257969856262\n",
      "loss_rank: 0.06438903510570526\n",
      "priority: 0.06629381328821182\n",
      "[SUCCESS] append transition experience\n",
      "==== EXPERT MODE ENABLE BC: False ENABLE RL: False FULL TRAIN: False ====\n",
      "==== low level action taken: 0 N_pickable_item: 5 ====\n",
      "[SUCCESS] get raw data\n",
      "Qx: tensor([[0.7012, 0.6712, 0.6744]], device='cuda:0')\n",
      "Qy: tensor([[0.6199, 0.7653, 0.7275]], device='cuda:0')\n",
      "Qz: tensor([[0.6800, 0.6538, 0.6413]], device='cuda:0')\n",
      "Qyaw: tensor([[0.6923, 0.6671, 0.6485]], device='cuda:0')\n",
      "[MAX OUTPUT]\n",
      "[Q net] x: 0, y: 1, z: 0, yaw_ind: 0\n",
      "[SUCCESS] estimate actions from network\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "gripper tip height: 0.18757936729636948\n",
      "[PUSH REWARD] R: 0.0\n",
      "[OVERALL REWARD] 0.0\n",
      "[ACTION TYPE]: 1\n",
      "[MOVE] move: [ 0.    0.   -0.02  0.  ] move index: [1, 1, 0, 1]\n",
      "next move: tensor([ 0.0000,  0.0000, -0.0200,  0.0000], device='cuda:0') next move index: [1, 1, 0, 1]\n",
      "target_Qx: 0.714261531829834 Qx: 0.6711742877960205\n",
      "target_Qy: 0.7844274640083313 Qy: 0.7652552723884583\n",
      "target_Qz: 0.7452152371406555 Qz: 0.6799817085266113\n",
      "target_Qyaw: 0.7226473689079285 Qyaw: 0.6671121716499329\n",
      "loss_rank: 0.05310109257698059\n",
      "priority: 0.05549200624227524\n",
      "[SUCCESS] append transition experience\n",
      "==== EXPERT MODE ENABLE BC: False ENABLE RL: False FULL TRAIN: False ====\n",
      "==== low level action taken: 0 N_pickable_item: 5 ====\n",
      "[SUCCESS] get raw data\n",
      "Qx: tensor([[0.7275, 0.6970, 0.6411]], device='cuda:0')\n",
      "Qy: tensor([[0.6596, 0.7750, 0.7963]], device='cuda:0')\n",
      "Qz: tensor([[0.7350, 0.6747, 0.6539]], device='cuda:0')\n",
      "Qyaw: tensor([[0.7590, 0.6960, 0.6443]], device='cuda:0')\n",
      "[MAX OUTPUT]\n",
      "[Q net] x: 0, y: 2, z: 0, yaw_ind: 0\n",
      "[SUCCESS] estimate actions from network\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "gripper tip height: 0.16626779506063732\n",
      "[PUSH REWARD] R: 0.0\n",
      "[OVERALL REWARD] 0.0\n",
      "[ACTION TYPE]: 1\n",
      "[MOVE] move: [ 0.    0.   -0.02  0.  ] move index: [1, 1, 0, 1]\n",
      "next move: tensor([-0.0150,  0.0000, -0.0200, -0.2618], device='cuda:0') next move index: [0, 1, 0, 0]\n",
      "target_Qx: 0.7687486410140991 Qx: 0.697032630443573\n",
      "target_Qy: 0.7783129215240479 Qy: 0.7749673128128052\n",
      "target_Qz: 0.8011091947555542 Qz: 0.7350202798843384\n",
      "target_Qyaw: 0.7938328981399536 Qyaw: 0.6960133910179138\n",
      "loss_rank: 0.047138430178165436\n",
      "priority: 0.0519111230969429\n",
      "[SUCCESS] append transition experience\n",
      "==== EXPERT MODE ENABLE BC: False ENABLE RL: False FULL TRAIN: False ====\n",
      "==== low level action taken: 0 N_pickable_item: 5 ====\n",
      "[SUCCESS] get raw data\n",
      "Qx: tensor([[0.7587, 0.7486, 0.7449]], device='cuda:0')\n",
      "Qy: tensor([[0.7114, 0.7683, 0.7834]], device='cuda:0')\n",
      "Qz: tensor([[0.7805, 0.6671, 0.6909]], device='cuda:0')\n",
      "Qyaw: tensor([[0.7995, 0.7105, 0.6000]], device='cuda:0')\n",
      "[MAX OUTPUT]\n",
      "[Q net] x: 0, y: 2, z: 0, yaw_ind: 0\n",
      "[SUCCESS] estimate actions from network\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "gripper tip height: 0.1456878052946511\n",
      "[PUSH REWARD] R: 0.0\n",
      "[OVERALL REWARD] 0.0\n",
      "[ACTION TYPE]: 1\n",
      "[MOVE] move: [-0.015      0.        -0.02      -0.2617994] move index: [0, 1, 0, 0]\n",
      "next move: tensor([ 0.0000,  0.0000, -0.0200,  0.0000], device='cuda:0') next move index: [1, 1, 0, 1]\n",
      "target_Qx: 0.838714599609375 Qx: 0.7586655616760254\n",
      "target_Qy: 0.805394172668457 Qy: 0.7683420181274414\n",
      "target_Qz: 0.8404502272605896 Qz: 0.7805485725402832\n",
      "target_Qyaw: 0.8519287109375 Qyaw: 0.7995287775993347\n",
      "loss_rank: 0.029627740383148193\n",
      "priority: 0.03315640985965729\n",
      "[SUCCESS] append transition experience\n",
      "==== EXPERT MODE ENABLE BC: False ENABLE RL: False FULL TRAIN: False ====\n",
      "==== low level action taken: 0 N_pickable_item: 5 ====\n",
      "[SUCCESS] get raw data\n",
      "Qx: tensor([[0.8000, 0.8482, 0.7217]], device='cuda:0')\n",
      "Qy: tensor([[0.7703, 0.7810, 0.7090]], device='cuda:0')\n",
      "Qz: tensor([[0.8364, 0.6957, 0.7119]], device='cuda:0')\n",
      "Qyaw: tensor([[0.8302, 0.8367, 0.7264]], device='cuda:0')\n",
      "[MAX OUTPUT]\n",
      "[Q net] x: 1, y: 1, z: 0, yaw_ind: 1\n",
      "[SUCCESS] estimate actions from network\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "gripper tip height: 0.12616386071817334\n",
      "[PUSH REWARD] R: 0.0\n",
      "[OVERALL REWARD] 0.0\n",
      "[ACTION TYPE]: 1\n",
      "[MOVE] move: [ 0.    0.   -0.02  0.  ] move index: [1, 1, 0, 1]\n",
      "next move: tensor([-0.0150,  0.0000, -0.0200, -0.2618], device='cuda:0') next move index: [0, 1, 0, 0]\n",
      "target_Qx: 0.7589438557624817 Qx: 0.8482245206832886\n",
      "target_Qy: 0.7658589482307434 Qy: 0.781039834022522\n",
      "target_Qz: 0.7871978878974915 Qz: 0.8364386558532715\n",
      "target_Qyaw: 0.787767231464386 Qyaw: 0.8366721272468567\n",
      "loss_rank: 0.02188071422278881\n",
      "priority: 0.02513517439365387\n",
      "[SUCCESS] append transition experience\n",
      "==== EXPERT MODE ENABLE BC: False ENABLE RL: False FULL TRAIN: False ====\n",
      "==== low level action taken: 0 N_pickable_item: 5 ====\n",
      "[SUCCESS] get raw data\n",
      "Qx: tensor([[0.7583, 0.7465, 0.7089]], device='cuda:0')\n",
      "Qy: tensor([[0.7380, 0.7541, 0.7894]], device='cuda:0')\n",
      "Qz: tensor([[0.7728, 0.7142, 0.7098]], device='cuda:0')\n",
      "Qyaw: tensor([[0.7707, 0.8065, 0.7060]], device='cuda:0')\n",
      "[MAX OUTPUT]\n",
      "[Q net] x: 0, y: 2, z: 0, yaw_ind: 1\n",
      "[SUCCESS] estimate actions from network\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "gripper tip height: 0.105794671379147\n",
      "[PUSH REWARD] R: 0.0\n",
      "[OVERALL REWARD] 0.0\n",
      "[ACTION TYPE]: 1\n",
      "[MOVE] move: [-0.015      0.        -0.02      -0.2617994] move index: [0, 1, 0, 0]\n",
      "next move: tensor([ 0.0000,  0.0000, -0.0200,  0.0000], device='cuda:0') next move index: [1, 1, 0, 1]\n",
      "target_Qx: 0.7962597608566284 Qx: 0.7582839131355286\n",
      "target_Qy: 0.8542467951774597 Qy: 0.7540557980537415\n",
      "target_Qz: 0.8568567037582397 Qz: 0.7728182077407837\n",
      "target_Qyaw: 0.7982295751571655 Qyaw: 0.7707414031028748\n",
      "loss_rank: 0.05063294619321823\n",
      "priority: 0.05545756220817566\n",
      "[SUCCESS] append transition experience\n",
      "==== EXPERT MODE ENABLE BC: False ENABLE RL: False FULL TRAIN: False ====\n",
      "==== low level action taken: 0 N_pickable_item: 5 ====\n",
      "[SUCCESS] get raw data\n",
      "Qx: tensor([[0.8069, 0.7958, 0.7460]], device='cuda:0')\n",
      "Qy: tensor([[0.7073, 0.8265, 0.7998]], device='cuda:0')\n",
      "Qz: tensor([[0.8589, 0.7994, 0.8962]], device='cuda:0')\n",
      "Qyaw: tensor([[0.8485, 0.7136, 0.7917]], device='cuda:0')\n",
      "[MAX OUTPUT]\n",
      "[Q net] x: 0, y: 1, z: 2, yaw_ind: 0\n",
      "[SUCCESS] estimate actions from network\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[GRIPPER STATUS] FULL ClOSE\n",
      "gripper tip height: 0.08520049634473037\n",
      "[PUSH REWARD] R: 0.0\n",
      "[OVERALL REWARD] 0.0\n",
      "[ACTION TYPE]: 1\n",
      "[MOVE] move: [ 0.    0.   -0.02  0.  ] move index: [1, 1, 0, 1]\n",
      "next move: tensor([ 0.0000,  0.0000, -0.0200,  0.0000], device='cuda:0') next move index: [1, 1, 0, 1]\n",
      "target_Qx: 0.93602055311203 Qx: 0.795820951461792\n",
      "target_Qy: 0.9224836826324463 Qy: 0.8265441656112671\n",
      "target_Qz: 0.9593073129653931 Qz: 0.8589112162590027\n",
      "target_Qyaw: 0.8196132779121399 Qyaw: 0.7135936617851257\n",
      "loss_rank: 0.06877994537353516\n",
      "priority: 0.08132490515708923\n",
      "[SUCCESS] append transition experience\n",
      "==== EXPERT MODE ENABLE BC: False ENABLE RL: False FULL TRAIN: False ====\n",
      "==== low level action taken: 0 N_pickable_item: 5 ====\n",
      "[SUCCESS] get raw data\n",
      "Qx: tensor([[0.7396, 0.9159, 0.7826]], device='cuda:0')\n",
      "Qy: tensor([[0.7499, 0.8961, 0.8166]], device='cuda:0')\n",
      "Qz: tensor([[0.9447, 0.8645, 0.8240]], device='cuda:0')\n",
      "Qyaw: tensor([[0.7204, 0.8135, 0.7195]], device='cuda:0')\n",
      "[MAX OUTPUT]\n",
      "[Q net] x: 1, y: 1, z: 0, yaw_ind: 1\n",
      "[SUCCESS] estimate actions from network\n",
      "[GRIPPER STATUS] FULL CLOSE\n",
      "[GRIPPER STATUS] FULL CLOSE\n",
      "gripper tip height: 0.06448080870461453\n",
      "[PUSH REWARD] R: 0.0\n",
      "[OVERALL REWARD] 0.0\n",
      "[ACTION TYPE]: 1\n",
      "[MOVE] move: [ 0.    0.   -0.02  0.  ] move index: [1, 1, 0, 1]\n",
      "next move: tensor([ 0.0000,  0.0000, -0.0200,  0.0000], device='cuda:0') next move index: [1, 1, 0, 1]\n",
      "target_Qx: 0.9504480957984924 Qx: 0.9158698320388794\n",
      "target_Qy: 0.9067379236221313 Qy: 0.896099328994751\n",
      "target_Qz: 0.9679785370826721 Qz: 0.9446508884429932\n",
      "target_Qyaw: 0.9684918522834778 Qyaw: 0.8134687542915344\n",
      "loss_rank: 0.004447313956916332\n",
      "priority: 0.01091860793530941\n",
      "[SUCCESS] append transition experience\n",
      "==== EXPERT MODE ENABLE BC: False ENABLE RL: False FULL TRAIN: False ====\n",
      "==== low level action taken: 0 N_pickable_item: 5 ====\n",
      "[SUCCESS] get raw data\n",
      "Qx: tensor([[0.8254, 0.9337, 0.8214]], device='cuda:0')\n",
      "Qy: tensor([[0.7916, 0.9001, 0.8474]], device='cuda:0')\n",
      "Qz: tensor([[0.9565, 0.8866, 0.8896]], device='cuda:0')\n",
      "Qyaw: tensor([[0.8946, 0.9511, 0.8939]], device='cuda:0')\n",
      "[MAX OUTPUT]\n",
      "[Q net] x: 1, y: 1, z: 0, yaw_ind: 1\n",
      "[SUCCESS] estimate actions from network\n",
      "[GRIPPER STATUS] FULL CLOSE\n",
      "[GRIPPER STATUS] FULL CLOSE\n",
      "gripper tip height: 0.04407749200967703\n",
      "[PUSH REWARD] R: 1.0\n",
      "[OVERALL REWARD] 1.0\n",
      "[ACTION TYPE]: 1\n",
      "[MOVE] move: [ 0.    0.   -0.02  0.  ] move index: [1, 1, 0, 1]\n",
      "next move: tensor([ 0.0000,  0.0000, -0.0200,  0.0000], device='cuda:0') next move index: [1, 1, 0, 1]\n",
      "target_Qx: 1.0 Qx: 0.9337388873100281\n",
      "target_Qy: 1.0 Qy: 0.900062084197998\n",
      "target_Qz: 1.0 Qz: 0.9564679861068726\n",
      "target_Qyaw: 1.0 Qyaw: 0.951103925704956\n",
      "loss_rank: 0.01641269214451313\n",
      "priority: 0.0210786871612072\n",
      "[SUCCESS] append transition experience\n",
      "[SUCCESS] perform push action\n",
      "=== end of action ===\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[SUCCESS] return home\n",
      "[SUCCESS] return home\n",
      "[SUCCESS] GRASP OR PUSH ACTION\n",
      "[PUSH SUCCESS RATE] 0.2%/-inf% [500]\n",
      "[SUCCESS] save agent data\n",
      "==== episode: 0 ====\n",
      "[MIN_DISTANCE] 0.03826336870751752\n",
      "N_step_x: 2.0, N_step_y: 3.0, N_step_z: 10.0, N_step_yaw: 2.0, N_step: 10.0\n",
      "[MIN_DISTANCE] 0.003872461183765675\n",
      "[compute_push_path]: 0.05000295694148177\n",
      "N_step_x: 1.0, N_step_y: 7.0, N_step_z: 10.0, N_step_yaw: 10.0, N_step: 10.0\n",
      "[MIN_DISTANCE] 0.014258333685656422\n",
      "[MIN_DISTANCE] 0.014258333685656422\n",
      "[MIN_DISTANCE] 0.022889562562164355\n",
      "N_step_low_level: 12\n",
      "==== EXPERT MODE ENABLE BC: False ENABLE RL: False FULL TRAIN: False ====\n",
      "==== low level action taken: 1 N_pickable_item: 5 ====\n",
      "[SUCCESS] get raw data\n",
      "Qx: tensor([[0.3356, 0.4210, 0.3834]], device='cuda:0')\n",
      "Qy: tensor([[0.5059, 0.5112, 0.4301]], device='cuda:0')\n",
      "Qz: tensor([[0.6609, 0.2888, 0.1612]], device='cuda:0')\n",
      "Qyaw: tensor([[0.2494, 0.3230, 0.3562]], device='cuda:0')\n",
      "[RANDOM OUTPUT]\n",
      "[Q net] x: 2, y: 2, z: 1, yaw_ind: 1\n",
      "[SUCCESS] estimate actions from network\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[GRASP DISTANCE] 0.0\n",
      "[GRASP REWARD] R: 0.0\n",
      "[OVERALL REWARD] 0.0\n",
      "[ACTION TYPE]: 0\n",
      "[MOVE] move: [-0.015     -0.015     -0.02      -0.1308997] move index: [0, 0, 0, 0]\n",
      "next move: tensor([ 0.0000,  0.0000, -0.0200,  0.0000], device='cuda:0') next move index: [1, 1, 0, 1]\n",
      "target_Qx: 0.36565470695495605 Qx: 0.33557435870170593\n",
      "target_Qy: 0.5159755945205688 Qy: 0.5059393048286438\n",
      "target_Qz: 0.5149593949317932 Qz: 0.6609033942222595\n",
      "target_Qyaw: 0.30463525652885437 Qyaw: 0.24943724274635315\n",
      "loss_rank: 0.07024586200714111\n",
      "priority: 0.0765838697552681\n",
      "[SUCCESS] append transition experience\n",
      "==== EXPERT MODE ENABLE BC: False ENABLE RL: False FULL TRAIN: False ====\n",
      "==== low level action taken: 1 N_pickable_item: 5 ====\n",
      "[SUCCESS] get raw data\n",
      "Qx: tensor([[0.4718, 0.3771, 0.3885]], device='cuda:0')\n",
      "Qy: tensor([[0.3924, 0.5167, 0.2933]], device='cuda:0')\n",
      "Qz: tensor([[0.5284, 0.3648, 0.3548]], device='cuda:0')\n",
      "Qyaw: tensor([[0.3733, 0.3386, 0.3228]], device='cuda:0')\n",
      "[MAX OUTPUT]\n",
      "[Q net] x: 0, y: 1, z: 0, yaw_ind: 0\n",
      "[SUCCESS] estimate actions from network\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[GRASP DISTANCE] 0.0\n",
      "[GRASP REWARD] R: 0.0\n",
      "[OVERALL REWARD] 0.0\n",
      "[ACTION TYPE]: 0\n",
      "[MOVE] move: [ 0.    0.   -0.02  0.  ] move index: [1, 1, 0, 1]\n",
      "next move: tensor([ 0.0000,  0.0000, -0.0200,  0.0000], device='cuda:0') next move index: [1, 1, 0, 1]\n",
      "target_Qx: 0.41694584488868713 Qx: 0.37707990407943726\n",
      "target_Qy: 0.5081066489219666 Qy: 0.5166878700256348\n",
      "target_Qz: 0.5075026750564575 Qz: 0.5283510088920593\n",
      "target_Qyaw: 0.33395060896873474 Qyaw: 0.3386262059211731\n",
      "loss_rank: 0.043752897530794144\n",
      "priority: 0.044282760471105576\n",
      "[SUCCESS] append transition experience\n",
      "==== EXPERT MODE ENABLE BC: False ENABLE RL: False FULL TRAIN: False ====\n",
      "==== low level action taken: 1 N_pickable_item: 5 ====\n",
      "[SUCCESS] get raw data\n",
      "Qx: tensor([[0.5065, 0.4223, 0.3803]], device='cuda:0')\n",
      "Qy: tensor([[0.4168, 0.5127, 0.4167]], device='cuda:0')\n",
      "Qz: tensor([[0.5091, 0.4408, 0.4005]], device='cuda:0')\n",
      "Qyaw: tensor([[0.2688, 0.3421, 0.3287]], device='cuda:0')\n",
      "[MAX OUTPUT]\n",
      "[Q net] x: 0, y: 1, z: 0, yaw_ind: 1\n",
      "[SUCCESS] estimate actions from network\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[GRASP DISTANCE] 0.0\n",
      "[GRASP REWARD] R: 0.0\n",
      "[OVERALL REWARD] 0.0\n",
      "[ACTION TYPE]: 0\n",
      "[MOVE] move: [ 0.    0.   -0.02  0.  ] move index: [1, 1, 0, 1]\n",
      "next move: tensor([ 0.0000,  0.0000, -0.0200,  0.0000], device='cuda:0') next move index: [1, 1, 0, 1]\n",
      "target_Qx: 0.5056729912757874 Qx: 0.42231395840644836\n",
      "target_Qy: 0.5893837213516235 Qy: 0.5127057433128357\n",
      "target_Qz: 0.5399182438850403 Qz: 0.5091279745101929\n",
      "target_Qyaw: 0.3564837574958801 Qyaw: 0.34209907054901123\n",
      "loss_rank: 0.032933447510004044\n",
      "priority: 0.03642924875020981\n",
      "[SUCCESS] append transition experience\n",
      "==== EXPERT MODE ENABLE BC: False ENABLE RL: False FULL TRAIN: False ====\n",
      "==== low level action taken: 1 N_pickable_item: 5 ====\n",
      "[SUCCESS] get raw data\n",
      "Qx: tensor([[0.5258, 0.5174, 0.4925]], device='cuda:0')\n",
      "Qy: tensor([[0.4989, 0.6105, 0.4920]], device='cuda:0')\n",
      "Qz: tensor([[0.5621, 0.5156, 0.4250]], device='cuda:0')\n",
      "Qyaw: tensor([[0.4038, 0.3658, 0.4039]], device='cuda:0')\n",
      "[MAX OUTPUT]\n",
      "[Q net] x: 0, y: 1, z: 0, yaw_ind: 2\n",
      "[SUCCESS] estimate actions from network\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[GRASP DISTANCE] 0.0\n",
      "[GRASP REWARD] R: 0.0\n",
      "[OVERALL REWARD] 0.0\n",
      "[ACTION TYPE]: 0\n",
      "[MOVE] move: [ 0.    0.   -0.02  0.  ] move index: [1, 1, 0, 1]\n",
      "next move: tensor([ 0.0000,  0.0000, -0.0200,  0.0000], device='cuda:0') next move index: [1, 1, 0, 1]\n",
      "target_Qx: 0.5386688113212585 Qx: 0.5173885822296143\n",
      "target_Qy: 0.7081810235977173 Qy: 0.6104540824890137\n",
      "target_Qz: 0.6094147562980652 Qz: 0.5620760321617126\n",
      "target_Qyaw: 0.47116735577583313 Qyaw: 0.3658466041088104\n",
      "loss_rank: 0.04276121035218239\n",
      "priority: 0.04859541356563568\n",
      "[SUCCESS] append transition experience\n",
      "==== EXPERT MODE ENABLE BC: False ENABLE RL: False FULL TRAIN: False ====\n",
      "==== low level action taken: 1 N_pickable_item: 5 ====\n",
      "[SUCCESS] get raw data\n",
      "Qx: tensor([[0.6394, 0.5153, 0.5778]], device='cuda:0')\n",
      "Qy: tensor([[0.5559, 0.6982, 0.6074]], device='cuda:0')\n",
      "Qz: tensor([[0.6222, 0.5334, 0.4332]], device='cuda:0')\n",
      "Qyaw: tensor([[0.5185, 0.4552, 0.4117]], device='cuda:0')\n",
      "[MAX OUTPUT]\n",
      "[Q net] x: 0, y: 1, z: 0, yaw_ind: 0\n",
      "[SUCCESS] estimate actions from network\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[GRASP DISTANCE] 0.0\n",
      "[GRASP REWARD] R: 0.0\n",
      "[OVERALL REWARD] 0.0\n",
      "[ACTION TYPE]: 0\n",
      "[MOVE] move: [ 0.    0.   -0.02  0.  ] move index: [1, 1, 0, 1]\n",
      "next move: tensor([ 0.0000, -0.0150, -0.0200,  0.0000], device='cuda:0') next move index: [1, 0, 0, 1]\n",
      "target_Qx: 0.5720874071121216 Qx: 0.5152755975723267\n",
      "target_Qy: 0.5733103156089783 Qy: 0.6982067823410034\n",
      "target_Qz: 0.6550620198249817 Qz: 0.6221821308135986\n",
      "target_Qyaw: 0.474638432264328 Qyaw: 0.45519310235977173\n",
      "loss_rank: 0.05222901329398155\n",
      "priority: 0.057300493121147156\n",
      "[SUCCESS] append transition experience\n",
      "==== EXPERT MODE ENABLE BC: False ENABLE RL: False FULL TRAIN: False ====\n",
      "==== low level action taken: 1 N_pickable_item: 5 ====\n",
      "[SUCCESS] get raw data\n",
      "Qx: tensor([[0.6109, 0.5632, 0.5677]], device='cuda:0')\n",
      "Qy: tensor([[0.5699, 0.7008, 0.6154]], device='cuda:0')\n",
      "Qz: tensor([[0.6582, 0.4929, 0.4581]], device='cuda:0')\n",
      "Qyaw: tensor([[0.5929, 0.4778, 0.4463]], device='cuda:0')\n",
      "[MAX OUTPUT]\n",
      "[Q net] x: 0, y: 1, z: 0, yaw_ind: 0\n",
      "[SUCCESS] estimate actions from network\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[GRASP DISTANCE] 0.0\n",
      "[GRASP REWARD] R: 0.0\n",
      "[OVERALL REWARD] 0.0\n",
      "[ACTION TYPE]: 0\n",
      "[MOVE] move: [ 0.    -0.015 -0.02   0.   ] move index: [1, 0, 0, 1]\n",
      "next move: tensor([ 0.0000,  0.0000, -0.0200,  0.0000], device='cuda:0') next move index: [1, 1, 0, 1]\n",
      "target_Qx: 0.7761473059654236 Qx: 0.5631592869758606\n",
      "target_Qy: 0.7591782212257385 Qy: 0.5698927044868469\n",
      "target_Qz: 0.8331104516983032 Qz: 0.6581663489341736\n",
      "target_Qyaw: 0.8738757371902466 Qyaw: 0.4778250455856323\n",
      "loss_rank: 0.07601425051689148\n",
      "priority: 0.14317786693572998\n",
      "[SUCCESS] append transition experience\n",
      "==== EXPERT MODE ENABLE BC: False ENABLE RL: False FULL TRAIN: False ====\n",
      "==== low level action taken: 1 N_pickable_item: 5 ====\n",
      "[SUCCESS] get raw data\n",
      "Qx: tensor([[0.5917, 0.7393, 0.5623]], device='cuda:0')\n",
      "Qy: tensor([[0.6180, 0.7527, 0.6806]], device='cuda:0')\n",
      "Qz: tensor([[0.8107, 0.8130, 0.6693]], device='cuda:0')\n",
      "Qyaw: tensor([[0.5999, 0.8417, 0.6062]], device='cuda:0')\n",
      "[MAX OUTPUT]\n",
      "[Q net] x: 1, y: 1, z: 1, yaw_ind: 1\n",
      "[SUCCESS] estimate actions from network\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[GRASP DISTANCE] 0.0\n",
      "[GRASP REWARD] R: 0.0\n",
      "[OVERALL REWARD] 0.0\n",
      "[ACTION TYPE]: 0\n",
      "[MOVE] move: [ 0.    0.   -0.02  0.  ] move index: [1, 1, 0, 1]\n",
      "next move: tensor([ 0.0000,  0.0000, -0.0200,  0.0000], device='cuda:0') next move index: [1, 1, 0, 1]\n",
      "target_Qx: 0.8600220680236816 Qx: 0.7393237352371216\n",
      "target_Qy: 0.8625652194023132 Qy: 0.7526590824127197\n",
      "target_Qz: 0.9337846636772156 Qz: 0.810732901096344\n",
      "target_Qyaw: 0.7949355244636536 Qyaw: 0.8417386412620544\n",
      "loss_rank: 0.010852999985218048\n",
      "priority: 0.021847927942872047\n",
      "[SUCCESS] append transition experience\n",
      "==== EXPERT MODE ENABLE BC: False ENABLE RL: False FULL TRAIN: False ====\n",
      "==== low level action taken: 1 N_pickable_item: 5 ====\n",
      "[SUCCESS] get raw data\n",
      "Qx: tensor([[0.6185, 0.8728, 0.5839]], device='cuda:0')\n",
      "Qy: tensor([[0.6785, 0.8758, 0.6633]], device='cuda:0')\n",
      "Qz: tensor([[0.9505, 0.7630, 0.7042]], device='cuda:0')\n",
      "Qyaw: tensor([[0.5029, 0.8056, 0.6862]], device='cuda:0')\n",
      "[MAX OUTPUT]\n",
      "[Q net] x: 1, y: 1, z: 0, yaw_ind: 1\n",
      "[SUCCESS] estimate actions from network\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[GRASP DISTANCE] 0.0\n",
      "[GRASP REWARD] R: 0.0\n",
      "[OVERALL REWARD] 0.0\n",
      "[ACTION TYPE]: 0\n",
      "[MOVE] move: [ 0.    0.   -0.02  0.  ] move index: [1, 1, 0, 1]\n",
      "next move: tensor([ 0.0000,  0.0000, -0.0200,  0.0000], device='cuda:0') next move index: [1, 1, 0, 1]\n",
      "target_Qx: 0.900319516658783 Qx: 0.8728304505348206\n",
      "target_Qy: 0.9698382019996643 Qy: 0.8758211135864258\n",
      "target_Qz: 0.9659942984580994 Qz: 0.9504510760307312\n",
      "target_Qyaw: 0.9302352666854858 Qyaw: 0.8056468963623047\n",
      "loss_rank: 0.0\n",
      "priority: 0.006339679006487131\n",
      "[SUCCESS] append transition experience\n",
      "==== EXPERT MODE ENABLE BC: False ENABLE RL: False FULL TRAIN: False ====\n",
      "==== low level action taken: 1 N_pickable_item: 5 ====\n",
      "[SUCCESS] get raw data\n",
      "Qx: tensor([[0.6886, 0.9097, 0.7486]], device='cuda:0')\n",
      "Qy: tensor([[0.7765, 0.9831, 0.6687]], device='cuda:0')\n",
      "Qz: tensor([[0.9741, 0.7883, 0.7328]], device='cuda:0')\n",
      "Qyaw: tensor([[0.6563, 0.9348, 0.6418]], device='cuda:0')\n",
      "[RANDOM OUTPUT]\n",
      "[Q net] x: 0, y: 1, z: 0, yaw_ind: 1\n",
      "[SUCCESS] estimate actions from network\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[GRASP DISTANCE] 0.0\n",
      "[GRASP REWARD] R: 0.0\n",
      "[OVERALL REWARD] 0.0\n",
      "[ACTION TYPE]: 0\n",
      "[MOVE] move: [ 0.    0.   -0.02  0.  ] move index: [1, 1, 0, 1]\n",
      "next move: tensor([ 0.0000,  0.0000, -0.0200,  0.0000], device='cuda:0') next move index: [1, 1, 0, 1]\n",
      "target_Qx: 0.9628917574882507 Qx: 0.9096882343292236\n",
      "target_Qy: 0.9563794732093811 Qy: 0.9830554127693176\n",
      "target_Qz: 0.9516990780830383 Qz: 0.9740641713142395\n",
      "target_Qyaw: 0.9634679555892944 Qyaw: 0.9347833395004272\n",
      "loss_rank: 0.0\n",
      "priority: 0.0012163063511252403\n",
      "[SUCCESS] append transition experience\n",
      "==== EXPERT MODE ENABLE BC: False ENABLE RL: False FULL TRAIN: False ====\n",
      "==== low level action taken: 1 N_pickable_item: 5 ====\n",
      "[SUCCESS] get raw data\n",
      "Qx: tensor([[0.8004, 0.9634, 0.8110]], device='cuda:0')\n",
      "Qy: tensor([[0.7741, 0.9609, 0.8885]], device='cuda:0')\n",
      "Qz: tensor([[0.9716, 0.8758, 0.8628]], device='cuda:0')\n",
      "Qyaw: tensor([[0.8123, 0.9606, 0.9262]], device='cuda:0')\n",
      "[MAX OUTPUT]\n",
      "[Q net] x: 1, y: 1, z: 0, yaw_ind: 1\n",
      "[SUCCESS] estimate actions from network\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[GRASP DISTANCE] 0.0\n",
      "[GRASP REWARD] R: 0.0\n",
      "[OVERALL REWARD] 0.0\n",
      "[ACTION TYPE]: 0\n",
      "[MOVE] move: [ 0.    0.   -0.02  0.  ] move index: [1, 1, 0, 1]\n",
      "next move: tensor([ 0.0000,  0.0000, -0.0200,  0.0000], device='cuda:0') next move index: [1, 1, 0, 1]\n",
      "target_Qx: 0.9617418050765991 Qx: 0.9633923172950745\n",
      "target_Qy: 0.9881147146224976 Qy: 0.9608585834503174\n",
      "target_Qz: 0.9642735123634338 Qz: 0.9715748429298401\n",
      "target_Qyaw: 0.965989351272583 Qyaw: 0.9605870246887207\n",
      "loss_rank: 0.008118852972984314\n",
      "priority: 0.008325882256031036\n",
      "[SUCCESS] append transition experience\n",
      "==== EXPERT MODE ENABLE BC: False ENABLE RL: False FULL TRAIN: False ====\n",
      "==== low level action taken: 1 N_pickable_item: 5 ====\n",
      "[SUCCESS] get raw data\n",
      "Qx: tensor([[0.8517, 0.9632, 0.8559]], device='cuda:0')\n",
      "Qy: tensor([[0.8998, 0.9896, 0.9507]], device='cuda:0')\n",
      "Qz: tensor([[0.9818, 0.8541, 0.8677]], device='cuda:0')\n",
      "Qyaw: tensor([[0.8136, 0.9599, 0.9505]], device='cuda:0')\n",
      "[MAX OUTPUT]\n",
      "[Q net] x: 1, y: 1, z: 0, yaw_ind: 1\n",
      "[SUCCESS] estimate actions from network\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[GRASP] grasp something\n",
      "[GRIPPER STATUS] NON_CLOSE_NON_OPEN\n",
      "[GRASP DISTANCE] 0.0\n",
      "[SUCESS] picked an item!\n",
      "[GRASP REWARD] R: 1.0\n",
      "[OVERALL REWARD] 1.0\n",
      "[ACTION TYPE]: 0\n",
      "[MOVE] move: [ 0.    0.   -0.02  0.  ] move index: [1, 1, 0, 1]\n",
      "next move: tensor([ 0.0000,  0.0000, -0.0200,  0.0000], device='cuda:0') next move index: [1, 1, 0, 1]\n",
      "target_Qx: 1.0 Qx: 0.9631783962249756\n",
      "target_Qy: 1.0 Qy: 0.9896419048309326\n",
      "target_Qz: 1.0 Qz: 0.981769859790802\n",
      "target_Qyaw: 1.0 Qyaw: 0.9599466323852539\n",
      "loss_rank: 0.013483518734574318\n",
      "priority: 0.014333451166749\n",
      "[SUCCESS] append transition experience\n",
      "[SUCCESS] grasp an item successfully\n",
      "=== end of action ===\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[SUCCESS] return home\n",
      "[SUCCESS] return home\n",
      "[SUCCESS] GRASP OR PUSH ACTION\n",
      "[GRASP SUCCESS RATE] 0.2%/-inf% [500]\n",
      "[SUCCESS] save agent data\n",
      "==== episode: 0 ====\n",
      "[MIN_DISTANCE] 0.003947562994148749\n",
      "[compute_push_path]: 0.05000295694148177\n",
      "N_step_x: 1.0, N_step_y: 7.0, N_step_z: 10.0, N_step_yaw: 10.0, N_step: 10.0\n",
      "[MIN_DISTANCE] 0.014204124519690001\n",
      "[MIN_DISTANCE] 0.014204124519690001\n",
      "[MIN_DISTANCE] 0.022890206223393304\n",
      "N_step_low_level: 12\n",
      "==== EXPERT MODE ENABLE BC: False ENABLE RL: False FULL TRAIN: False ====\n",
      "==== low level action taken: 2 N_pickable_item: 4 ====\n",
      "[SUCCESS] get raw data\n",
      "Qx: tensor([[0.4433, 0.4756, 0.4370]], device='cuda:0')\n",
      "Qy: tensor([[0.4183, 0.3280, 0.4982]], device='cuda:0')\n",
      "Qz: tensor([[0.4435, 0.2275, 0.2486]], device='cuda:0')\n",
      "Qyaw: tensor([[0.4372, 0.2843, 0.3111]], device='cuda:0')\n",
      "[MAX OUTPUT]\n",
      "[Q net] x: 1, y: 2, z: 0, yaw_ind: 0\n",
      "[SUCCESS] estimate actions from network\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "gripper tip height: 0.24913047762981494\n",
      "[PUSH REWARD] R: 0.0\n",
      "[OVERALL REWARD] 0.0\n",
      "[ACTION TYPE]: 1\n",
      "[MOVE] move: [-0.015      0.015     -0.02      -0.2617994] move index: [0, 2, 0, 0]\n",
      "next move: tensor([ 0.0000,  0.0150, -0.0200, -0.2618], device='cuda:0') next move index: [1, 2, 0, 0]\n",
      "target_Qx: 0.5242593288421631 Qx: 0.4432958960533142\n",
      "target_Qy: 0.5681911706924438 Qy: 0.49824461340904236\n",
      "target_Qz: 0.5528565049171448 Qz: 0.44349515438079834\n",
      "target_Qyaw: 0.538162350654602 Qyaw: 0.43717697262763977\n",
      "loss_rank: 0.020512981340289116\n",
      "priority: 0.028914369642734528\n",
      "[SUCCESS] append transition experience\n",
      "==== EXPERT MODE ENABLE BC: False ENABLE RL: False FULL TRAIN: False ====\n",
      "==== low level action taken: 2 N_pickable_item: 4 ====\n",
      "[SUCCESS] get raw data\n",
      "Qx: tensor([[0.5041, 0.5035, 0.5046]], device='cuda:0')\n",
      "Qy: tensor([[0.5344, 0.4807, 0.5468]], device='cuda:0')\n",
      "Qz: tensor([[0.5122, 0.3737, 0.3995]], device='cuda:0')\n",
      "Qyaw: tensor([[0.5326, 0.4115, 0.3914]], device='cuda:0')\n",
      "[MAX OUTPUT]\n",
      "[Q net] x: 2, y: 2, z: 0, yaw_ind: 0\n",
      "[SUCCESS] estimate actions from network\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "gripper tip height: 0.22909247518602213\n",
      "[PUSH REWARD] R: 0.0\n",
      "[OVERALL REWARD] 0.0\n",
      "[ACTION TYPE]: 1\n",
      "[MOVE] move: [ 0.         0.015     -0.02      -0.2617994] move index: [1, 2, 0, 0]\n",
      "next move: tensor([ 0.0000,  0.0000, -0.0200, -0.2618], device='cuda:0') next move index: [1, 1, 0, 0]\n",
      "target_Qx: 0.6019530296325684 Qx: 0.5034639239311218\n",
      "target_Qy: 0.7041690349578857 Qy: 0.5467831492424011\n",
      "target_Qz: 0.5783251523971558 Qz: 0.5121640563011169\n",
      "target_Qyaw: 0.5758311748504639 Qyaw: 0.5326194167137146\n",
      "loss_rank: 0.026953525841236115\n",
      "priority: 0.03713226690888405\n",
      "[SUCCESS] append transition experience\n",
      "==== EXPERT MODE ENABLE BC: False ENABLE RL: False FULL TRAIN: False ====\n",
      "==== low level action taken: 2 N_pickable_item: 4 ====\n",
      "[SUCCESS] get raw data\n",
      "Qx: tensor([[0.5073, 0.5958, 0.4710]], device='cuda:0')\n",
      "Qy: tensor([[0.4635, 0.6901, 0.5281]], device='cuda:0')\n",
      "Qz: tensor([[0.5622, 0.5178, 0.4651]], device='cuda:0')\n",
      "Qyaw: tensor([[0.5540, 0.5488, 0.5287]], device='cuda:0')\n",
      "[MAX OUTPUT]\n",
      "[Q net] x: 1, y: 1, z: 0, yaw_ind: 0\n",
      "[SUCCESS] estimate actions from network\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "gripper tip height: 0.20851806651770208\n",
      "[PUSH REWARD] R: 0.0\n",
      "[OVERALL REWARD] 0.0\n",
      "[ACTION TYPE]: 1\n",
      "[MOVE] move: [ 0.         0.        -0.02      -0.2617994] move index: [1, 1, 0, 0]\n",
      "next move: tensor([ 0.0000,  0.0150, -0.0200, -0.2618], device='cuda:0') next move index: [1, 2, 0, 0]\n",
      "target_Qx: 0.7112609148025513 Qx: 0.595794677734375\n",
      "target_Qy: 0.6889700889587402 Qy: 0.6900817155838013\n",
      "target_Qz: 0.6914497017860413 Qz: 0.5621588826179504\n",
      "target_Qyaw: 0.6490084528923035 Qyaw: 0.5540116429328918\n",
      "loss_rank: 0.0199600700289011\n",
      "priority: 0.029728619381785393\n",
      "[SUCCESS] append transition experience\n",
      "==== EXPERT MODE ENABLE BC: False ENABLE RL: False FULL TRAIN: False ====\n",
      "==== low level action taken: 2 N_pickable_item: 4 ====\n",
      "[SUCCESS] get raw data\n",
      "Qx: tensor([[0.6371, 0.7145, 0.5998]], device='cuda:0')\n",
      "Qy: tensor([[0.5963, 0.7314, 0.7005]], device='cuda:0')\n",
      "Qz: tensor([[0.6905, 0.5071, 0.5360]], device='cuda:0')\n",
      "Qyaw: tensor([[0.6679, 0.6928, 0.6308]], device='cuda:0')\n",
      "[MAX OUTPUT]\n",
      "[Q net] x: 1, y: 1, z: 0, yaw_ind: 1\n",
      "[SUCCESS] estimate actions from network\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "gripper tip height: 0.1889595200272013\n",
      "[PUSH REWARD] R: 0.0\n",
      "[OVERALL REWARD] 0.0\n",
      "[ACTION TYPE]: 1\n",
      "[MOVE] move: [ 0.         0.015     -0.02      -0.2617994] move index: [1, 2, 0, 0]\n",
      "next move: tensor([ 0.0000,  0.0000, -0.0200, -0.2618], device='cuda:0') next move index: [1, 1, 0, 0]\n",
      "target_Qx: 0.7493402361869812 Qx: 0.7144988775253296\n",
      "target_Qy: 0.781307578086853 Qy: 0.7004944682121277\n",
      "target_Qz: 0.7733890414237976 Qz: 0.6904584169387817\n",
      "target_Qyaw: 0.7733749747276306 Qyaw: 0.6679455041885376\n",
      "loss_rank: 0.028429914265871048\n",
      "priority: 0.03486429899930954\n",
      "[SUCCESS] append transition experience\n",
      "==== EXPERT MODE ENABLE BC: False ENABLE RL: False FULL TRAIN: False ====\n",
      "==== low level action taken: 2 N_pickable_item: 4 ====\n",
      "[SUCCESS] get raw data\n",
      "Qx: tensor([[0.6849, 0.7268, 0.6446]], device='cuda:0')\n",
      "Qy: tensor([[0.6121, 0.7537, 0.7187]], device='cuda:0')\n",
      "Qz: tensor([[0.7586, 0.6524, 0.6391]], device='cuda:0')\n",
      "Qyaw: tensor([[0.7668, 0.7187, 0.6338]], device='cuda:0')\n",
      "[MAX OUTPUT]\n",
      "[Q net] x: 1, y: 1, z: 0, yaw_ind: 0\n",
      "[SUCCESS] estimate actions from network\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "gripper tip height: 0.16844082138387123\n",
      "[PUSH REWARD] R: 0.0\n",
      "[OVERALL REWARD] 0.0\n",
      "[ACTION TYPE]: 1\n",
      "[MOVE] move: [ 0.         0.        -0.02      -0.2617994] move index: [1, 1, 0, 0]\n",
      "next move: tensor([ 0.0000,  0.0150, -0.0200, -0.2618], device='cuda:0') next move index: [1, 2, 0, 0]\n",
      "target_Qx: 0.8283202648162842 Qx: 0.7268053293228149\n",
      "target_Qy: 0.7880306839942932 Qy: 0.7536555528640747\n",
      "target_Qz: 0.8077676892280579 Qz: 0.7586157321929932\n",
      "target_Qyaw: 0.8486102223396301 Qyaw: 0.766792356967926\n",
      "loss_rank: 0.016068710014224052\n",
      "priority: 0.021217962726950645\n",
      "[SUCCESS] append transition experience\n",
      "==== EXPERT MODE ENABLE BC: False ENABLE RL: False FULL TRAIN: False ====\n",
      "==== low level action taken: 2 N_pickable_item: 4 ====\n",
      "[SUCCESS] get raw data\n",
      "Qx: tensor([[0.6665, 0.8147, 0.6129]], device='cuda:0')\n",
      "Qy: tensor([[0.6425, 0.7444, 0.8045]], device='cuda:0')\n",
      "Qz: tensor([[0.7905, 0.6484, 0.5905]], device='cuda:0')\n",
      "Qyaw: tensor([[0.8370, 0.7144, 0.6301]], device='cuda:0')\n",
      "[MAX OUTPUT]\n",
      "[Q net] x: 1, y: 2, z: 0, yaw_ind: 0\n",
      "[SUCCESS] estimate actions from network\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "gripper tip height: 0.14784045656205075\n",
      "[PUSH REWARD] R: 0.0\n",
      "[OVERALL REWARD] 0.0\n",
      "[ACTION TYPE]: 1\n",
      "[MOVE] move: [ 0.         0.015     -0.02      -0.2617994] move index: [1, 2, 0, 0]\n",
      "next move: tensor([ 0.0000,  0.0150, -0.0200, -0.2618], device='cuda:0') next move index: [1, 2, 0, 0]\n",
      "target_Qx: 0.8150290250778198 Qx: 0.8146671056747437\n",
      "target_Qy: 0.7952853441238403 Qy: 0.8044903874397278\n",
      "target_Qz: 0.8387336134910583 Qz: 0.7904567718505859\n",
      "target_Qyaw: 0.8834591507911682 Qyaw: 0.8369703888893127\n",
      "loss_rank: 0.0033282670192420483\n",
      "priority: 0.00447244755923748\n",
      "[SUCCESS] append transition experience\n",
      "==== EXPERT MODE ENABLE BC: False ENABLE RL: False FULL TRAIN: False ====\n",
      "==== low level action taken: 2 N_pickable_item: 4 ====\n",
      "[SUCCESS] get raw data\n",
      "Qx: tensor([[0.8197, 0.8118, 0.7744]], device='cuda:0')\n",
      "Qy: tensor([[0.7628, 0.8296, 0.8011]], device='cuda:0')\n",
      "Qz: tensor([[0.8333, 0.7606, 0.7622]], device='cuda:0')\n",
      "Qyaw: tensor([[0.8765, 0.8252, 0.7466]], device='cuda:0')\n",
      "[MAX OUTPUT]\n",
      "[Q net] x: 0, y: 1, z: 0, yaw_ind: 0\n",
      "[SUCCESS] estimate actions from network\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "gripper tip height: 0.1264958235976854\n",
      "[PUSH REWARD] R: 0.0\n",
      "[OVERALL REWARD] 0.0\n",
      "[ACTION TYPE]: 1\n",
      "[MOVE] move: [ 0.         0.015     -0.02      -0.2617994] move index: [1, 2, 0, 0]\n",
      "next move: tensor([ 0.0000,  0.0000, -0.0200, -0.2618], device='cuda:0') next move index: [1, 1, 0, 0]\n",
      "target_Qx: 0.8599517345428467 Qx: 0.8118171691894531\n",
      "target_Qy: 0.918077826499939 Qy: 0.8011250495910645\n",
      "target_Qz: 0.8632279634475708 Qz: 0.8332937359809875\n",
      "target_Qyaw: 0.9359233975410461 Qyaw: 0.8764727711677551\n",
      "loss_rank: 0.03879734128713608\n",
      "priority: 0.04390367120504379\n",
      "[SUCCESS] append transition experience\n",
      "==== EXPERT MODE ENABLE BC: False ENABLE RL: False FULL TRAIN: False ====\n",
      "==== low level action taken: 2 N_pickable_item: 4 ====\n",
      "[SUCCESS] get raw data\n",
      "Qx: tensor([[0.7437, 0.8612, 0.7512]], device='cuda:0')\n",
      "Qy: tensor([[0.8039, 0.9228, 0.8371]], device='cuda:0')\n",
      "Qz: tensor([[0.8595, 0.7299, 0.7509]], device='cuda:0')\n",
      "Qyaw: tensor([[0.9213, 0.8524, 0.8401]], device='cuda:0')\n",
      "[MAX OUTPUT]\n",
      "[Q net] x: 1, y: 1, z: 0, yaw_ind: 0\n",
      "[SUCCESS] estimate actions from network\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "gripper tip height: 0.10602068207089099\n",
      "[PUSH REWARD] R: 0.0\n",
      "[OVERALL REWARD] 0.0\n",
      "[ACTION TYPE]: 1\n",
      "[MOVE] move: [ 0.         0.        -0.02      -0.2617994] move index: [1, 1, 0, 0]\n",
      "next move: tensor([ 0.0000,  0.0150, -0.0200, -0.2618], device='cuda:0') next move index: [1, 2, 0, 0]\n",
      "target_Qx: 0.9295137524604797 Qx: 0.8612140417098999\n",
      "target_Qy: 0.8680453896522522 Qy: 0.9228194952011108\n",
      "target_Qz: 0.9152225852012634 Qz: 0.8595157861709595\n",
      "target_Qyaw: 0.9259534478187561 Qyaw: 0.9213419556617737\n",
      "loss_rank: 0.005344847217202187\n",
      "priority: 0.008042238652706146\n",
      "[SUCCESS] append transition experience\n",
      "==== EXPERT MODE ENABLE BC: False ENABLE RL: False FULL TRAIN: False ====\n",
      "==== low level action taken: 2 N_pickable_item: 4 ====\n",
      "[SUCCESS] get raw data\n",
      "Qx: tensor([[0.8686, 0.9424, 0.7720]], device='cuda:0')\n",
      "Qy: tensor([[0.7626, 0.8452, 0.8885]], device='cuda:0')\n",
      "Qz: tensor([[0.9101, 0.8719, 0.8028]], device='cuda:0')\n",
      "Qyaw: tensor([[0.9298, 0.8399, 0.8066]], device='cuda:0')\n",
      "[RANDOM OUTPUT]\n",
      "[Q net] x: 0, y: 2, z: 0, yaw_ind: 1\n",
      "[SUCCESS] estimate actions from network\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[GRIPPER STATUS] FULL ClOSE\n",
      "gripper tip height: 0.08622094496382476\n",
      "[PUSH REWARD] R: 0.0\n",
      "[OVERALL REWARD] 0.0\n",
      "[ACTION TYPE]: 1\n",
      "[MOVE] move: [ 0.         0.015     -0.02      -0.2617994] move index: [1, 2, 0, 0]\n",
      "next move: tensor([ 0.0000,  0.0000, -0.0200,  0.0000], device='cuda:0') next move index: [1, 1, 0, 1]\n",
      "target_Qx: 0.9583896994590759 Qx: 0.9423551559448242\n",
      "target_Qy: 0.9212499856948853 Qy: 0.8884956240653992\n",
      "target_Qz: 0.9472790360450745 Qz: 0.9101020097732544\n",
      "target_Qyaw: 0.974120557308197 Qyaw: 0.9298128485679626\n",
      "loss_rank: 0.012903419323265553\n",
      "priority: 0.014072233811020851\n",
      "[SUCCESS] append transition experience\n",
      "==== EXPERT MODE ENABLE BC: False ENABLE RL: False FULL TRAIN: False ====\n",
      "==== low level action taken: 2 N_pickable_item: 4 ====\n",
      "[SUCCESS] get raw data\n",
      "Qx: tensor([[0.7115, 0.9365, 0.7480]], device='cuda:0')\n",
      "Qy: tensor([[0.8049, 0.9100, 0.7109]], device='cuda:0')\n",
      "Qz: tensor([[0.9257, 0.7683, 0.7249]], device='cuda:0')\n",
      "Qyaw: tensor([[0.7744, 0.9718, 0.7682]], device='cuda:0')\n",
      "[MAX OUTPUT]\n",
      "[Q net] x: 1, y: 1, z: 0, yaw_ind: 1\n",
      "[SUCCESS] estimate actions from network\n",
      "[GRIPPER STATUS] FULL CLOSE\n",
      "[GRIPPER STATUS] FULL CLOSE\n",
      "gripper tip height: 0.06537208949044393\n",
      "[PUSH REWARD] R: 0.0\n",
      "[OVERALL REWARD] 0.0\n",
      "[ACTION TYPE]: 1\n",
      "[MOVE] move: [ 0.    0.   -0.02  0.  ] move index: [1, 1, 0, 1]\n",
      "next move: tensor([ 0.0000,  0.0000, -0.0200,  0.0000], device='cuda:0') next move index: [1, 1, 0, 1]\n",
      "target_Qx: 0.9865583181381226 Qx: 0.9364558458328247\n",
      "target_Qy: 0.983057975769043 Qy: 0.9099949598312378\n",
      "target_Qz: 0.9668799638748169 Qz: 0.9257326722145081\n",
      "target_Qyaw: 0.9749892354011536 Qyaw: 0.9718179106712341\n",
      "loss_rank: 0.0\n",
      "priority: 0.0023879047948867083\n",
      "[SUCCESS] append transition experience\n",
      "==== EXPERT MODE ENABLE BC: False ENABLE RL: False FULL TRAIN: False ====\n",
      "==== low level action taken: 2 N_pickable_item: 4 ====\n",
      "[SUCCESS] get raw data\n",
      "Qx: tensor([[0.7261, 0.9617, 0.8881]], device='cuda:0')\n",
      "Qy: tensor([[0.8396, 0.9823, 0.7033]], device='cuda:0')\n",
      "Qz: tensor([[0.9436, 0.8349, 0.7611]], device='cuda:0')\n",
      "Qyaw: tensor([[0.7292, 0.9653, 0.8277]], device='cuda:0')\n",
      "[MAX OUTPUT]\n",
      "[Q net] x: 1, y: 1, z: 0, yaw_ind: 1\n",
      "[SUCCESS] estimate actions from network\n",
      "[GRIPPER STATUS] FULL CLOSE\n",
      "[GRIPPER STATUS] FULL CLOSE\n",
      "gripper tip height: 0.0440522459344182\n",
      "[PUSH REWARD] R: 1.0\n",
      "[OVERALL REWARD] 1.0\n",
      "[ACTION TYPE]: 1\n",
      "[MOVE] move: [ 0.    0.   -0.02  0.  ] move index: [1, 1, 0, 1]\n",
      "next move: tensor([ 0.0000,  0.0000, -0.0200,  0.0000], device='cuda:0') next move index: [1, 1, 0, 1]\n",
      "target_Qx: 1.0 Qx: 0.9616994857788086\n",
      "target_Qy: 1.0 Qy: 0.9823435544967651\n",
      "target_Qz: 1.0 Qz: 0.9436212778091431\n",
      "target_Qyaw: 1.0 Qyaw: 0.9652848839759827\n",
      "loss_rank: 0.002199250739067793\n",
      "priority: 0.003739845473319292\n",
      "[SUCCESS] append transition experience\n",
      "[SUCCESS] perform push action\n",
      "=== end of action ===\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[FAIL] return home\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[FAIL] return home\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[SUCCESS] return home\n",
      "[SUCCESS] return home\n",
      "[SUCCESS] GRASP OR PUSH ACTION\n",
      "[PUSH SUCCESS RATE] 0.4%/-inf% [500]\n",
      "[SUCCESS] save agent data\n",
      "==== episode: 0 ====\n",
      "[MIN_DISTANCE] 0.022852160119597977\n",
      "[compute_push_path]: 0.05000295694148177\n",
      "N_step_x: 1.0, N_step_y: 1.0, N_step_z: 10.0, N_step_yaw: 7.0, N_step: 10.0\n",
      "[MIN_DISTANCE] 0.005642229735829758\n",
      "[MIN_DISTANCE] 0.005642229735829758\n",
      "[MIN_DISTANCE] 0.005428307871732672\n",
      "N_step_low_level: 12\n",
      "==== EXPERT MODE ENABLE BC: False ENABLE RL: False FULL TRAIN: False ====\n",
      "==== low level action taken: 3 N_pickable_item: 4 ====\n",
      "[SUCCESS] get raw data\n",
      "Qx: tensor([[0.3101, 0.2862, 0.3185]], device='cuda:0')\n",
      "Qy: tensor([[0.3421, 0.2276, 0.4017]], device='cuda:0')\n",
      "Qz: tensor([[0.3483, 0.2415, 0.2137]], device='cuda:0')\n",
      "Qyaw: tensor([[0.3216, 0.1339, 0.2592]], device='cuda:0')\n",
      "[MAX OUTPUT]\n",
      "[Q net] x: 2, y: 2, z: 0, yaw_ind: 0\n",
      "[SUCCESS] estimate actions from network\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "gripper tip height: 0.24834923079921628\n",
      "[PUSH REWARD] R: 0.0\n",
      "[OVERALL REWARD] 0.0\n",
      "[ACTION TYPE]: 1\n",
      "[MOVE] move: [-0.015      0.015     -0.02      -0.2617994] move index: [0, 2, 0, 0]\n",
      "next move: tensor([ 0.0000,  0.0000, -0.0200, -0.2618], device='cuda:0') next move index: [1, 1, 0, 0]\n",
      "target_Qx: 0.6562976837158203 Qx: 0.31011876463890076\n",
      "target_Qy: 0.5487814545631409 Qy: 0.4017314910888672\n",
      "target_Qz: 0.6643323302268982 Qz: 0.34834691882133484\n",
      "target_Qyaw: 0.5688069462776184 Qyaw: 0.3215528130531311\n",
      "loss_rank: 0.021878212690353394\n",
      "priority: 0.09748944640159607\n",
      "[SUCCESS] append transition experience\n",
      "==== EXPERT MODE ENABLE BC: False ENABLE RL: False FULL TRAIN: False ====\n",
      "==== low level action taken: 3 N_pickable_item: 4 ====\n",
      "[SUCCESS] get raw data\n",
      "Qx: tensor([[0.5558, 0.6735, 0.6158]], device='cuda:0')\n",
      "Qy: tensor([[0.6672, 0.5626, 0.6383]], device='cuda:0')\n",
      "Qz: tensor([[0.6737, 0.5472, 0.5212]], device='cuda:0')\n",
      "Qyaw: tensor([[0.5787, 0.5727, 0.4856]], device='cuda:0')\n",
      "[MAX OUTPUT]\n",
      "[Q net] x: 1, y: 0, z: 0, yaw_ind: 0\n",
      "[SUCCESS] estimate actions from network\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "gripper tip height: 0.22874815120112765\n",
      "[PUSH REWARD] R: 0.0\n",
      "[OVERALL REWARD] 0.0\n",
      "[ACTION TYPE]: 1\n",
      "[MOVE] move: [ 0.         0.        -0.02      -0.2617994] move index: [1, 1, 0, 0]\n",
      "next move: tensor([ 0.0000,  0.0000, -0.0200,  0.0000], device='cuda:0') next move index: [1, 1, 0, 1]\n",
      "target_Qx: 0.677305281162262 Qx: 0.67353355884552\n",
      "target_Qy: 0.6711905002593994 Qy: 0.5625571608543396\n",
      "target_Qz: 0.7386975884437561 Qz: 0.673664927482605\n",
      "target_Qyaw: 0.6174259781837463 Qyaw: 0.5786615610122681\n",
      "loss_rank: 0.04362902045249939\n",
      "priority: 0.048015858978033066\n",
      "[SUCCESS] append transition experience\n",
      "==== EXPERT MODE ENABLE BC: False ENABLE RL: False FULL TRAIN: False ====\n",
      "==== low level action taken: 3 N_pickable_item: 4 ====\n",
      "[SUCCESS] get raw data\n",
      "Qx: tensor([[0.6246, 0.6753, 0.5372]], device='cuda:0')\n",
      "Qy: tensor([[0.6298, 0.6701, 0.7774]], device='cuda:0')\n",
      "Qz: tensor([[0.7230, 0.5349, 0.5292]], device='cuda:0')\n",
      "Qyaw: tensor([[0.6019, 0.6276, 0.5829]], device='cuda:0')\n",
      "[MAX OUTPUT]\n",
      "[Q net] x: 1, y: 2, z: 0, yaw_ind: 1\n",
      "[SUCCESS] estimate actions from network\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "gripper tip height: 0.20815010288166064\n",
      "[PUSH REWARD] R: 0.0\n",
      "[OVERALL REWARD] 0.0\n",
      "[ACTION TYPE]: 1\n",
      "[MOVE] move: [ 0.    0.   -0.02  0.  ] move index: [1, 1, 0, 1]\n",
      "next move: tensor([ 0.0000,  0.0000, -0.0200, -0.2618], device='cuda:0') next move index: [1, 1, 0, 0]\n",
      "target_Qx: 0.7338556051254272 Qx: 0.6753271222114563\n",
      "target_Qy: 0.6878288984298706 Qy: 0.6700922250747681\n",
      "target_Qz: 0.7273708581924438 Qz: 0.7230241298675537\n",
      "target_Qyaw: 0.6255249977111816 Qyaw: 0.6275659799575806\n",
      "loss_rank: 0.03716534748673439\n",
      "priority: 0.03810615465044975\n",
      "[SUCCESS] append transition experience\n",
      "==== EXPERT MODE ENABLE BC: False ENABLE RL: False FULL TRAIN: False ====\n",
      "==== low level action taken: 3 N_pickable_item: 4 ====\n",
      "[SUCCESS] get raw data\n",
      "Qx: tensor([[0.6533, 0.7237, 0.6682]], device='cuda:0')\n",
      "Qy: tensor([[0.6716, 0.6876, 0.8109]], device='cuda:0')\n",
      "Qz: tensor([[0.7323, 0.5515, 0.5812]], device='cuda:0')\n",
      "Qyaw: tensor([[0.6146, 0.6765, 0.6184]], device='cuda:0')\n",
      "[MAX OUTPUT]\n",
      "[Q net] x: 1, y: 2, z: 0, yaw_ind: 1\n",
      "[SUCCESS] estimate actions from network\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "gripper tip height: 0.1874756837675746\n",
      "[PUSH REWARD] R: 0.0\n",
      "[OVERALL REWARD] 0.0\n",
      "[ACTION TYPE]: 1\n",
      "[MOVE] move: [ 0.         0.        -0.02      -0.2617994] move index: [1, 1, 0, 0]\n",
      "next move: tensor([ 0.0000,  0.0000, -0.0200,  0.0000], device='cuda:0') next move index: [1, 1, 0, 1]\n",
      "target_Qx: 0.6616418361663818 Qx: 0.723682165145874\n",
      "target_Qy: 0.5366020202636719 Qy: 0.6875729560852051\n",
      "target_Qz: 0.6622495055198669 Qz: 0.7323349714279175\n",
      "target_Qyaw: 0.6036242842674255 Qyaw: 0.6146203279495239\n",
      "loss_rank: 0.05391674488782883\n",
      "priority: 0.06183527410030365\n",
      "[SUCCESS] append transition experience\n",
      "==== EXPERT MODE ENABLE BC: False ENABLE RL: False FULL TRAIN: False ====\n",
      "==== low level action taken: 3 N_pickable_item: 4 ====\n",
      "[SUCCESS] get raw data\n",
      "Qx: tensor([[0.4966, 0.6681, 0.4311]], device='cuda:0')\n",
      "Qy: tensor([[0.5116, 0.5506, 0.6026]], device='cuda:0')\n",
      "Qz: tensor([[0.6995, 0.5821, 0.5346]], device='cuda:0')\n",
      "Qyaw: tensor([[0.5533, 0.6100, 0.5376]], device='cuda:0')\n",
      "[MAX OUTPUT]\n",
      "[Q net] x: 1, y: 2, z: 0, yaw_ind: 1\n",
      "[SUCCESS] estimate actions from network\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "gripper tip height: 0.16619674157053402\n",
      "[PUSH REWARD] R: 0.0\n",
      "[OVERALL REWARD] 0.0\n",
      "[ACTION TYPE]: 1\n",
      "[MOVE] move: [ 0.    0.   -0.02  0.  ] move index: [1, 1, 0, 1]\n",
      "next move: tensor([ 0.0000,  0.0000, -0.0200, -0.2618], device='cuda:0') next move index: [1, 1, 0, 0]\n",
      "target_Qx: 0.7575955390930176 Qx: 0.6680576205253601\n",
      "target_Qy: 0.6494799852371216 Qy: 0.5505645275115967\n",
      "target_Qz: 0.8159721493721008 Qz: 0.6994962692260742\n",
      "target_Qyaw: 0.7383036017417908 Qyaw: 0.6099807620048523\n",
      "loss_rank: 0.023676596581935883\n",
      "priority: 0.035635270178318024\n",
      "[SUCCESS] append transition experience\n",
      "==== EXPERT MODE ENABLE BC: False ENABLE RL: False FULL TRAIN: False ====\n",
      "==== low level action taken: 3 N_pickable_item: 4 ====\n",
      "[SUCCESS] get raw data\n",
      "Qx: tensor([[0.5919, 0.7455, 0.6114]], device='cuda:0')\n",
      "Qy: tensor([[0.6294, 0.6329, 0.7462]], device='cuda:0')\n",
      "Qz: tensor([[0.8084, 0.6798, 0.6135]], device='cuda:0')\n",
      "Qyaw: tensor([[0.7318, 0.7197, 0.6334]], device='cuda:0')\n",
      "[MAX OUTPUT]\n",
      "[Q net] x: 1, y: 2, z: 0, yaw_ind: 0\n",
      "[SUCCESS] estimate actions from network\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "gripper tip height: 0.14577932110961322\n",
      "[PUSH REWARD] R: 0.0\n",
      "[OVERALL REWARD] 0.0\n",
      "[ACTION TYPE]: 1\n",
      "[MOVE] move: [ 0.         0.        -0.02      -0.2617994] move index: [1, 1, 0, 0]\n",
      "next move: tensor([ 0.0000,  0.0000, -0.0200, -0.2618], device='cuda:0') next move index: [1, 1, 0, 0]\n",
      "target_Qx: 0.5326853394508362 Qx: 0.7455042600631714\n",
      "target_Qy: 0.43033841252326965 Qy: 0.6328827142715454\n",
      "target_Qz: 0.5655239224433899 Qz: 0.8084496855735779\n",
      "target_Qyaw: 0.4576100707054138 Qyaw: 0.7317720651626587\n",
      "loss_rank: 0.03327832743525505\n",
      "priority: 0.08840177953243256\n",
      "[SUCCESS] append transition experience\n",
      "==== EXPERT MODE ENABLE BC: False ENABLE RL: False FULL TRAIN: False ====\n",
      "==== low level action taken: 3 N_pickable_item: 4 ====\n",
      "[SUCCESS] get raw data\n",
      "Qx: tensor([[0.5180, 0.5111, 0.4080]], device='cuda:0')\n",
      "Qy: tensor([[0.3437, 0.3923, 0.4314]], device='cuda:0')\n",
      "Qz: tensor([[0.5404, 0.3541, 0.3162]], device='cuda:0')\n",
      "Qyaw: tensor([[0.4331, 0.5027, 0.4095]], device='cuda:0')\n",
      "[MAX OUTPUT]\n",
      "[Q net] x: 0, y: 2, z: 0, yaw_ind: 1\n",
      "[SUCCESS] estimate actions from network\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "gripper tip height: 0.12544916449055749\n",
      "[PUSH REWARD] R: 0.0\n",
      "[OVERALL REWARD] 0.0\n",
      "[ACTION TYPE]: 1\n",
      "[MOVE] move: [ 0.         0.        -0.02      -0.2617994] move index: [1, 1, 0, 0]\n",
      "next move: tensor([ 0.0000,  0.0000, -0.0200,  0.0000], device='cuda:0') next move index: [1, 1, 0, 1]\n",
      "target_Qx: 0.7160215377807617 Qx: 0.5111218690872192\n",
      "target_Qy: 0.6303718090057373 Qy: 0.3922857344150543\n",
      "target_Qz: 0.7886680960655212 Qz: 0.540419340133667\n",
      "target_Qyaw: 0.6391659379005432 Qyaw: 0.4331315755844116\n",
      "loss_rank: 0.04528158903121948\n",
      "priority: 0.09596820175647736\n",
      "[SUCCESS] append transition experience\n",
      "==== EXPERT MODE ENABLE BC: False ENABLE RL: False FULL TRAIN: False ====\n",
      "==== low level action taken: 3 N_pickable_item: 4 ====\n",
      "[SUCCESS] get raw data\n",
      "Qx: tensor([[0.5524, 0.7379, 0.6488]], device='cuda:0')\n",
      "Qy: tensor([[0.6139, 0.6358, 0.6685]], device='cuda:0')\n",
      "Qz: tensor([[0.7973, 0.5517, 0.5326]], device='cuda:0')\n",
      "Qyaw: tensor([[0.6043, 0.6536, 0.5215]], device='cuda:0')\n",
      "[MAX OUTPUT]\n",
      "[Q net] x: 1, y: 2, z: 0, yaw_ind: 1\n",
      "[SUCCESS] estimate actions from network\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "gripper tip height: 0.10561190608359344\n",
      "[PUSH REWARD] R: 0.0\n",
      "[OVERALL REWARD] 0.0\n",
      "[ACTION TYPE]: 1\n",
      "[MOVE] move: [ 0.    0.   -0.02  0.  ] move index: [1, 1, 0, 1]\n",
      "next move: tensor([ 0.0000,  0.0000, -0.0200, -0.2618], device='cuda:0') next move index: [1, 1, 0, 0]\n",
      "target_Qx: 0.8677548170089722 Qx: 0.7378522753715515\n",
      "target_Qy: 0.8297042846679688 Qy: 0.6358362436294556\n",
      "target_Qz: 0.8683651089668274 Qz: 0.7972778081893921\n",
      "target_Qyaw: 0.8085487484931946 Qyaw: 0.6535519957542419\n",
      "loss_rank: 0.022703884169459343\n",
      "priority: 0.04358810558915138\n",
      "[SUCCESS] append transition experience\n",
      "==== EXPERT MODE ENABLE BC: False ENABLE RL: False FULL TRAIN: False ====\n",
      "==== low level action taken: 3 N_pickable_item: 4 ====\n",
      "[SUCCESS] get raw data\n",
      "Qx: tensor([[0.5970, 0.8523, 0.6752]], device='cuda:0')\n",
      "Qy: tensor([[0.5768, 0.7858, 0.7098]], device='cuda:0')\n",
      "Qz: tensor([[0.8328, 0.6148, 0.6048]], device='cuda:0')\n",
      "Qyaw: tensor([[0.7468, 0.6967, 0.5439]], device='cuda:0')\n",
      "[MAX OUTPUT]\n",
      "[Q net] x: 1, y: 1, z: 0, yaw_ind: 0\n",
      "[SUCCESS] estimate actions from network\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[GRIPPER STATUS] FULL ClOSE\n",
      "gripper tip height: 0.08452758755601586\n",
      "[PUSH REWARD] R: 0.0\n",
      "[OVERALL REWARD] 0.0\n",
      "[ACTION TYPE]: 1\n",
      "[MOVE] move: [ 0.         0.        -0.02      -0.2617994] move index: [1, 1, 0, 0]\n",
      "next move: tensor([ 0.0000,  0.0000, -0.0200,  0.0000], device='cuda:0') next move index: [1, 1, 0, 1]\n",
      "target_Qx: 0.8763636946678162 Qx: 0.8522706627845764\n",
      "target_Qy: 0.8789127469062805 Qy: 0.7857556343078613\n",
      "target_Qz: 0.8931096196174622 Qz: 0.8327741026878357\n",
      "target_Qyaw: 0.8923415541648865 Qyaw: 0.7468234300613403\n",
      "loss_rank: 0.006163722835481167\n",
      "priority: 0.014682377688586712\n",
      "[SUCCESS] append transition experience\n",
      "==== EXPERT MODE ENABLE BC: False ENABLE RL: False FULL TRAIN: False ====\n",
      "==== low level action taken: 3 N_pickable_item: 4 ====\n",
      "[SUCCESS] get raw data\n",
      "Qx: tensor([[0.5774, 0.9096, 0.7518]], device='cuda:0')\n",
      "Qy: tensor([[0.6766, 0.9102, 0.8289]], device='cuda:0')\n",
      "Qz: tensor([[0.9260, 0.6697, 0.7492]], device='cuda:0')\n",
      "Qyaw: tensor([[0.7985, 0.9196, 0.7069]], device='cuda:0')\n",
      "[RANDOM OUTPUT]\n",
      "[Q net] x: 1, y: 0, z: 2, yaw_ind: 2\n",
      "[SUCCESS] estimate actions from network\n",
      "[GRIPPER STATUS] FULL CLOSE\n",
      "[GRIPPER STATUS] FULL CLOSE\n",
      "gripper tip height: 0.06427843194166155\n",
      "[PUSH REWARD] R: 0.0\n",
      "[OVERALL REWARD] 0.0\n",
      "[ACTION TYPE]: 1\n",
      "[MOVE] move: [ 0.    0.   -0.02  0.  ] move index: [1, 1, 0, 1]\n",
      "next move: tensor([ 0.0000,  0.0000, -0.0200,  0.0000], device='cuda:0') next move index: [1, 1, 0, 1]\n",
      "target_Qx: 0.9947580099105835 Qx: 0.9096096754074097\n",
      "target_Qy: 0.9667133688926697 Qy: 0.9101998209953308\n",
      "target_Qz: 0.9248035550117493 Qz: 0.9259886741638184\n",
      "target_Qyaw: 0.9836744666099548 Qyaw: 0.9196432828903198\n",
      "loss_rank: 0.0015551429241895676\n",
      "priority: 0.005191497039049864\n",
      "[SUCCESS] append transition experience\n",
      "==== EXPERT MODE ENABLE BC: False ENABLE RL: False FULL TRAIN: False ====\n",
      "==== low level action taken: 3 N_pickable_item: 4 ====\n",
      "[SUCCESS] get raw data\n",
      "Qx: tensor([[0.7331, 0.9783, 0.8891]], device='cuda:0')\n",
      "Qy: tensor([[0.7222, 0.9286, 0.8791]], device='cuda:0')\n",
      "Qz: tensor([[0.8904, 0.7264, 0.7612]], device='cuda:0')\n",
      "Qyaw: tensor([[0.7802, 0.9505, 0.8268]], device='cuda:0')\n",
      "[MAX OUTPUT]\n",
      "[Q net] x: 1, y: 1, z: 0, yaw_ind: 1\n",
      "[SUCCESS] estimate actions from network\n",
      "[GRIPPER STATUS] FULL CLOSE\n",
      "[GRIPPER STATUS] FULL CLOSE\n",
      "gripper tip height: 0.04394829969317321\n",
      "[PUSH REWARD] R: 1.0\n",
      "[OVERALL REWARD] 1.0\n",
      "[ACTION TYPE]: 1\n",
      "[MOVE] move: [ 0.    0.   -0.02  0.  ] move index: [1, 1, 0, 1]\n",
      "next move: tensor([ 0.0000,  0.0000, -0.0200,  0.0000], device='cuda:0') next move index: [1, 1, 0, 1]\n",
      "target_Qx: 1.0 Qx: 0.9782571792602539\n",
      "target_Qy: 1.0 Qy: 0.9285741448402405\n",
      "target_Qz: 1.0 Qz: 0.8903667330741882\n",
      "target_Qyaw: 1.0 Qyaw: 0.9505172371864319\n",
      "loss_rank: 0.005114735569804907\n",
      "priority: 0.010125335305929184\n",
      "[SUCCESS] append transition experience\n",
      "[SUCCESS] perform push action\n",
      "=== end of action ===\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[SUCCESS] return home\n",
      "[SUCCESS] return home\n",
      "[SUCCESS] GRASP OR PUSH ACTION\n",
      "[PUSH SUCCESS RATE] 0.6%/-inf% [500]\n",
      "[SUCCESS] save agent data\n",
      "==== episode: 0 ====\n",
      "[MIN_DISTANCE] 0.04230141672445545\n",
      "N_step_x: 6.0, N_step_y: 1.0, N_step_z: 10.0, N_step_yaw: 2.0, N_step: 10.0\n",
      "[MIN_DISTANCE] 0.0056685631283961875\n",
      "[compute_push_path]: 0.05000295694148177\n",
      "N_step_x: 7.0, N_step_y: 6.0, N_step_z: 10.0, N_step_yaw: 6.0, N_step: 10.0\n",
      "[MIN_DISTANCE] 0.0056685631283961875\n",
      "[MIN_DISTANCE] 0.005433761050559849\n",
      "N_step_low_level: 13\n",
      "==== EXPERT MODE ENABLE BC: False ENABLE RL: False FULL TRAIN: False ====\n",
      "==== low level action taken: 4 N_pickable_item: 4 ====\n",
      "[SUCCESS] get raw data\n",
      "Qx: tensor([[0.3977, 0.4084, 0.3881]], device='cuda:0')\n",
      "Qy: tensor([[0.3963, 0.4414, 0.4340]], device='cuda:0')\n",
      "Qz: tensor([[0.5392, 0.2187, 0.1654]], device='cuda:0')\n",
      "Qyaw: tensor([[0.2720, 0.2906, 0.2777]], device='cuda:0')\n",
      "[MAX OUTPUT]\n",
      "[Q net] x: 1, y: 1, z: 0, yaw_ind: 1\n",
      "[SUCCESS] estimate actions from network\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[GRASP DISTANCE] 0.0\n",
      "[GRASP REWARD] R: 0.0\n",
      "[OVERALL REWARD] 0.0\n",
      "[ACTION TYPE]: 0\n",
      "[MOVE] move: [ 0.015      0.015     -0.02      -0.1308997] move index: [2, 2, 0, 0]\n",
      "next move: tensor([ 0.0000,  0.0000, -0.0200,  0.0000], device='cuda:0') next move index: [1, 1, 0, 1]\n",
      "target_Qx: 0.49525147676467896 Qx: 0.388062059879303\n",
      "target_Qy: 0.44448456168174744 Qy: 0.43398144841194153\n",
      "target_Qz: 0.43751293420791626 Qz: 0.5392491221427917\n",
      "target_Qyaw: 0.27689453959465027 Qyaw: 0.2720375061035156\n",
      "loss_rank: 0.051996197551488876\n",
      "priority: 0.057489629834890366\n",
      "[SUCCESS] append transition experience\n",
      "==== EXPERT MODE ENABLE BC: False ENABLE RL: False FULL TRAIN: False ====\n",
      "==== low level action taken: 4 N_pickable_item: 4 ====\n",
      "[SUCCESS] get raw data\n",
      "Qx: tensor([[0.4019, 0.4905, 0.4155]], device='cuda:0')\n",
      "Qy: tensor([[0.3889, 0.4177, 0.4057]], device='cuda:0')\n",
      "Qz: tensor([[0.4450, 0.4160, 0.2089]], device='cuda:0')\n",
      "Qyaw: tensor([[0.1307, 0.2780, 0.2026]], device='cuda:0')\n",
      "[RANDOM OUTPUT]\n",
      "[Q net] x: 2, y: 2, z: 0, yaw_ind: 0\n",
      "[SUCCESS] estimate actions from network\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[GRASP DISTANCE] 0.0\n",
      "[GRASP REWARD] R: 0.0\n",
      "[OVERALL REWARD] 0.0\n",
      "[ACTION TYPE]: 0\n",
      "[MOVE] move: [ 0.    0.   -0.02  0.  ] move index: [1, 1, 0, 1]\n",
      "next move: tensor([ 0.0150,  0.0000, -0.0200,  0.0000], device='cuda:0') next move index: [2, 1, 0, 1]\n",
      "target_Qx: 0.5183612108230591 Qx: 0.49049586057662964\n",
      "target_Qy: 0.5086527466773987 Qy: 0.41768065094947815\n",
      "target_Qz: 0.5335284471511841 Qz: 0.44501546025276184\n",
      "target_Qyaw: 0.42890802025794983 Qyaw: 0.2779783606529236\n",
      "loss_rank: 0.024269353598356247\n",
      "priority: 0.03418603166937828\n",
      "[SUCCESS] append transition experience\n",
      "==== EXPERT MODE ENABLE BC: False ENABLE RL: False FULL TRAIN: False ====\n",
      "==== low level action taken: 4 N_pickable_item: 4 ====\n",
      "[SUCCESS] get raw data\n",
      "Qx: tensor([[0.3512, 0.5109, 0.5348]], device='cuda:0')\n",
      "Qy: tensor([[0.3357, 0.5159, 0.3788]], device='cuda:0')\n",
      "Qz: tensor([[0.5501, 0.5462, 0.3866]], device='cuda:0')\n",
      "Qyaw: tensor([[0.3546, 0.4425, 0.2914]], device='cuda:0')\n",
      "[MAX OUTPUT]\n",
      "[Q net] x: 2, y: 1, z: 0, yaw_ind: 1\n",
      "[SUCCESS] estimate actions from network\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[GRASP DISTANCE] 0.0\n",
      "[GRASP REWARD] R: 0.0\n",
      "[OVERALL REWARD] 0.0\n",
      "[ACTION TYPE]: 0\n",
      "[MOVE] move: [ 0.015  0.    -0.02   0.   ] move index: [2, 1, 0, 1]\n",
      "next move: tensor([ 0.0000,  0.0000, -0.0200,  0.0000], device='cuda:0') next move index: [1, 1, 0, 1]\n",
      "target_Qx: 0.5953266620635986 Qx: 0.5347967743873596\n",
      "target_Qy: 0.5677094459533691 Qy: 0.5158740282058716\n",
      "target_Qz: 0.6442721486091614 Qz: 0.5501462817192078\n",
      "target_Qyaw: 0.5231795310974121 Qyaw: 0.4425140619277954\n",
      "loss_rank: 0.015349715948104858\n",
      "priority: 0.020779060199856758\n",
      "[SUCCESS] append transition experience\n",
      "==== EXPERT MODE ENABLE BC: False ENABLE RL: False FULL TRAIN: False ====\n",
      "==== low level action taken: 4 N_pickable_item: 4 ====\n",
      "[SUCCESS] get raw data\n",
      "Qx: tensor([[0.4656, 0.5933, 0.5208]], device='cuda:0')\n",
      "Qy: tensor([[0.3762, 0.5148, 0.4551]], device='cuda:0')\n",
      "Qz: tensor([[0.6278, 0.5145, 0.3926]], device='cuda:0')\n",
      "Qyaw: tensor([[0.3994, 0.4989, 0.3344]], device='cuda:0')\n",
      "[MAX OUTPUT]\n",
      "[Q net] x: 1, y: 1, z: 0, yaw_ind: 1\n",
      "[SUCCESS] estimate actions from network\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[GRASP DISTANCE] 0.0\n",
      "[GRASP REWARD] R: 0.0\n",
      "[OVERALL REWARD] 0.0\n",
      "[ACTION TYPE]: 0\n",
      "[MOVE] move: [ 0.    0.   -0.02  0.  ] move index: [1, 1, 0, 1]\n",
      "next move: tensor([ 0.0150,  0.0000, -0.0200,  0.0000], device='cuda:0') next move index: [2, 1, 0, 1]\n",
      "target_Qx: 0.5449463725090027 Qx: 0.5932753682136536\n",
      "target_Qy: 0.5251518487930298 Qy: 0.5147891044616699\n",
      "target_Qz: 0.6417540907859802 Qz: 0.6278432011604309\n",
      "target_Qyaw: 0.42706403136253357 Qyaw: 0.49893105030059814\n",
      "loss_rank: 0.005692287813872099\n",
      "priority: 0.007642652839422226\n",
      "[SUCCESS] append transition experience\n",
      "==== EXPERT MODE ENABLE BC: False ENABLE RL: False FULL TRAIN: False ====\n",
      "==== low level action taken: 4 N_pickable_item: 4 ====\n",
      "[SUCCESS] get raw data\n",
      "Qx: tensor([[0.4672, 0.5190, 0.5278]], device='cuda:0')\n",
      "Qy: tensor([[0.3826, 0.4951, 0.3948]], device='cuda:0')\n",
      "Qz: tensor([[0.6208, 0.5544, 0.4488]], device='cuda:0')\n",
      "Qyaw: tensor([[0.3994, 0.4244, 0.2601]], device='cuda:0')\n",
      "[RANDOM OUTPUT]\n",
      "[Q net] x: 0, y: 0, z: 0, yaw_ind: 2\n",
      "[SUCCESS] estimate actions from network\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[GRASP DISTANCE] 0.0\n",
      "[GRASP REWARD] R: 0.0\n",
      "[OVERALL REWARD] 0.0\n",
      "[ACTION TYPE]: 0\n",
      "[MOVE] move: [ 0.015  0.    -0.02   0.   ] move index: [2, 1, 0, 1]\n",
      "next move: tensor([ 0.0000,  0.0000, -0.0200,  0.0000], device='cuda:0') next move index: [1, 1, 0, 1]\n",
      "target_Qx: 0.6332559585571289 Qx: 0.5278000831604004\n",
      "target_Qy: 0.43895989656448364 Qy: 0.49511954188346863\n",
      "target_Qz: 0.6228878498077393 Qz: 0.6208239197731018\n",
      "target_Qyaw: 0.47175028920173645 Qyaw: 0.4243866801261902\n",
      "loss_rank: 0.019937342032790184\n",
      "priority: 0.024067945778369904\n",
      "[SUCCESS] append transition experience\n",
      "==== EXPERT MODE ENABLE BC: False ENABLE RL: False FULL TRAIN: False ====\n",
      "==== low level action taken: 4 N_pickable_item: 4 ====\n",
      "[SUCCESS] get raw data\n",
      "Qx: tensor([[0.3798, 0.6258, 0.4992]], device='cuda:0')\n",
      "Qy: tensor([[0.5172, 0.4181, 0.5262]], device='cuda:0')\n",
      "Qz: tensor([[0.5929, 0.3850, 0.4298]], device='cuda:0')\n",
      "Qyaw: tensor([[0.3401, 0.4404, 0.2475]], device='cuda:0')\n",
      "[MAX OUTPUT]\n",
      "[Q net] x: 1, y: 2, z: 0, yaw_ind: 1\n",
      "[SUCCESS] estimate actions from network\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[GRASP DISTANCE] 0.0\n",
      "[GRASP REWARD] R: 0.0\n",
      "[OVERALL REWARD] 0.0\n",
      "[ACTION TYPE]: 0\n",
      "[MOVE] move: [ 0.    0.   -0.02  0.  ] move index: [1, 1, 0, 1]\n",
      "next move: tensor([ 0.0150,  0.0000, -0.0200,  0.0000], device='cuda:0') next move index: [2, 1, 0, 1]\n",
      "target_Qx: 0.586719810962677 Qx: 0.6257949471473694\n",
      "target_Qy: 0.6340147852897644 Qy: 0.4181085228919983\n",
      "target_Qz: 0.683647096157074 Qz: 0.592850923538208\n",
      "target_Qyaw: 0.5278365015983582 Qyaw: 0.44043290615081787\n",
      "loss_rank: 0.03392990678548813\n",
      "priority: 0.049936335533857346\n",
      "[SUCCESS] append transition experience\n",
      "==== EXPERT MODE ENABLE BC: False ENABLE RL: False FULL TRAIN: False ====\n",
      "==== low level action taken: 4 N_pickable_item: 4 ====\n",
      "[SUCCESS] get raw data\n",
      "Qx: tensor([[0.4078, 0.6006, 0.5804]], device='cuda:0')\n",
      "Qy: tensor([[0.5838, 0.6396, 0.5605]], device='cuda:0')\n",
      "Qz: tensor([[0.6696, 0.4837, 0.4893]], device='cuda:0')\n",
      "Qyaw: tensor([[0.5477, 0.5529, 0.3348]], device='cuda:0')\n",
      "[MAX OUTPUT]\n",
      "[Q net] x: 1, y: 1, z: 0, yaw_ind: 1\n",
      "[SUCCESS] estimate actions from network\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[GRASP DISTANCE] 0.0\n",
      "[GRASP REWARD] R: 0.0\n",
      "[OVERALL REWARD] 0.0\n",
      "[ACTION TYPE]: 0\n",
      "[MOVE] move: [ 0.015  0.    -0.02   0.   ] move index: [2, 1, 0, 1]\n",
      "next move: tensor([ 0.0000,  0.0000, -0.0200,  0.0000], device='cuda:0') next move index: [1, 1, 0, 1]\n",
      "target_Qx: 0.7563363313674927 Qx: 0.5804226994514465\n",
      "target_Qy: 0.833501398563385 Qy: 0.639571487903595\n",
      "target_Qz: 0.8200076222419739 Qz: 0.6696363091468811\n",
      "target_Qyaw: 0.7884201407432556 Qyaw: 0.5529255867004395\n",
      "loss_rank: 0.02334047667682171\n",
      "priority: 0.059996381402015686\n",
      "[SUCCESS] append transition experience\n",
      "==== EXPERT MODE ENABLE BC: False ENABLE RL: False FULL TRAIN: False ====\n",
      "==== low level action taken: 4 N_pickable_item: 4 ====\n",
      "[SUCCESS] get raw data\n",
      "Qx: tensor([[0.6177, 0.7519, 0.7511]], device='cuda:0')\n",
      "Qy: tensor([[0.8281, 0.8259, 0.7311]], device='cuda:0')\n",
      "Qz: tensor([[0.8197, 0.6317, 0.5820]], device='cuda:0')\n",
      "Qyaw: tensor([[0.6157, 0.7867, 0.5771]], device='cuda:0')\n",
      "[RANDOM OUTPUT]\n",
      "[Q net] x: 1, y: 0, z: 0, yaw_ind: 2\n",
      "[SUCCESS] estimate actions from network\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[GRASP DISTANCE] 0.0\n",
      "[GRASP REWARD] R: 0.0\n",
      "[OVERALL REWARD] 0.0\n",
      "[ACTION TYPE]: 0\n",
      "[MOVE] move: [ 0.    0.   -0.02  0.  ] move index: [1, 1, 0, 1]\n",
      "next move: tensor([ 0.0150,  0.0000, -0.0200,  0.0000], device='cuda:0') next move index: [2, 1, 0, 1]\n",
      "target_Qx: 0.7481089234352112 Qx: 0.751887857913971\n",
      "target_Qy: 0.7829263806343079 Qy: 0.8258808255195618\n",
      "target_Qz: 0.7384055256843567 Qz: 0.8197219967842102\n",
      "target_Qyaw: 0.6395062804222107 Qyaw: 0.7866674065589905\n",
      "loss_rank: 0.017217624932527542\n",
      "priority: 0.02474965713918209\n",
      "[SUCCESS] append transition experience\n",
      "==== EXPERT MODE ENABLE BC: False ENABLE RL: False FULL TRAIN: False ====\n",
      "==== low level action taken: 4 N_pickable_item: 4 ====\n",
      "[SUCCESS] get raw data\n",
      "Qx: tensor([[0.5653, 0.5445, 0.7297]], device='cuda:0')\n",
      "Qy: tensor([[0.6026, 0.7814, 0.2963]], device='cuda:0')\n",
      "Qz: tensor([[0.7041, 0.5637, 0.4742]], device='cuda:0')\n",
      "Qyaw: tensor([[0.5579, 0.5921, 0.3975]], device='cuda:0')\n",
      "[MAX OUTPUT]\n",
      "[Q net] x: 2, y: 1, z: 0, yaw_ind: 1\n",
      "[SUCCESS] estimate actions from network\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[GRASP DISTANCE] 0.0\n",
      "[GRASP REWARD] R: 0.0\n",
      "[OVERALL REWARD] 0.0\n",
      "[ACTION TYPE]: 0\n",
      "[MOVE] move: [ 0.015  0.    -0.02   0.   ] move index: [2, 1, 0, 1]\n",
      "next move: tensor([ 0.0000,  0.0000, -0.0200,  0.0000], device='cuda:0') next move index: [1, 1, 0, 1]\n",
      "target_Qx: 0.9886509776115417 Qx: 0.7296935319900513\n",
      "target_Qy: 0.9926234483718872 Qy: 0.7813639044761658\n",
      "target_Qz: 0.9807214140892029 Qz: 0.7041200995445251\n",
      "target_Qyaw: 0.9413023591041565 Qyaw: 0.5920602083206177\n",
      "loss_rank: 0.005488562863320112\n",
      "priority: 0.08303053677082062\n",
      "[SUCCESS] append transition experience\n",
      "==== EXPERT MODE ENABLE BC: False ENABLE RL: False FULL TRAIN: False ====\n",
      "==== low level action taken: 4 N_pickable_item: 4 ====\n",
      "[SUCCESS] get raw data\n",
      "Qx: tensor([[0.8758, 1.0139, 0.9580]], device='cuda:0')\n",
      "Qy: tensor([[0.8991, 0.9995, 0.9354]], device='cuda:0')\n",
      "Qz: tensor([[0.9893, 0.8555, 0.8459]], device='cuda:0')\n",
      "Qyaw: tensor([[0.7943, 0.9545, 0.7664]], device='cuda:0')\n",
      "[MAX OUTPUT]\n",
      "[Q net] x: 1, y: 1, z: 0, yaw_ind: 1\n",
      "[SUCCESS] estimate actions from network\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[GRASP DISTANCE] 0.0\n",
      "[GRASP REWARD] R: 0.0\n",
      "[OVERALL REWARD] 0.0\n",
      "[ACTION TYPE]: 0\n",
      "[MOVE] move: [ 0.    0.   -0.02  0.  ] move index: [1, 1, 0, 1]\n",
      "next move: tensor([ 0.0000,  0.0000, -0.0200,  0.0000], device='cuda:0') next move index: [1, 1, 0, 1]\n",
      "target_Qx: 0.9500677585601807 Qx: 1.0139412879943848\n",
      "target_Qy: 0.9596144556999207 Qy: 0.9995158910751343\n",
      "target_Qz: 0.9627670645713806 Qz: 0.9892979264259338\n",
      "target_Qyaw: 0.9459140300750732 Qyaw: 0.95453941822052\n",
      "loss_rank: 0.006660731043666601\n",
      "priority: 0.0082732904702425\n",
      "[SUCCESS] append transition experience\n",
      "==== EXPERT MODE ENABLE BC: False ENABLE RL: False FULL TRAIN: False ====\n",
      "==== low level action taken: 4 N_pickable_item: 4 ====\n",
      "[SUCCESS] get raw data\n",
      "Qx: tensor([[0.7054, 0.9345, 0.7733]], device='cuda:0')\n",
      "Qy: tensor([[0.7827, 0.9613, 0.6687]], device='cuda:0')\n",
      "Qz: tensor([[0.9596, 0.8248, 0.7759]], device='cuda:0')\n",
      "Qyaw: tensor([[0.7167, 0.9511, 0.8317]], device='cuda:0')\n",
      "[MAX OUTPUT]\n",
      "[Q net] x: 1, y: 1, z: 0, yaw_ind: 1\n",
      "[SUCCESS] estimate actions from network\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[GRASP] grasp something\n",
      "[GRIPPER STATUS] NON_CLOSE_NON_OPEN\n",
      "[GRASP DISTANCE] 0.0\n",
      "[SUCESS] picked an item!\n",
      "[GRASP REWARD] R: 1.0\n",
      "[OVERALL REWARD] 1.0\n",
      "[ACTION TYPE]: 0\n",
      "[MOVE] move: [ 0.    0.   -0.02  0.  ] move index: [1, 1, 0, 1]\n",
      "next move: tensor([ 0.0000,  0.0000, -0.0200,  0.0000], device='cuda:0') next move index: [1, 1, 0, 1]\n",
      "target_Qx: 1.0 Qx: 0.9345163106918335\n",
      "target_Qy: 1.0 Qy: 0.961274266242981\n",
      "target_Qz: 1.0 Qz: 0.9595994353294373\n",
      "target_Qyaw: 1.0 Qyaw: 0.9511101245880127\n",
      "loss_rank: 0.0\n",
      "priority: 0.002452555578202009\n",
      "[SUCCESS] append transition experience\n",
      "[SUCCESS] grasp an item successfully\n",
      "=== end of action ===\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[SUCCESS] return home\n",
      "[SUCCESS] return home\n",
      "[SUCCESS] GRASP OR PUSH ACTION\n",
      "[GRASP SUCCESS RATE] 0.4%/-inf% [500]\n",
      "[SUCCESS] save agent data\n",
      "==== episode: 0 ====\n",
      "[MIN_DISTANCE] 0.005701646503285338\n",
      "[compute_push_path]: 0.05000295694148177\n",
      "N_step_x: 7.0, N_step_y: 6.0, N_step_z: 10.0, N_step_yaw: 6.0, N_step: 10.0\n",
      "[MIN_DISTANCE] 0.005701646503285338\n",
      "[MIN_DISTANCE] 0.005450334999596687\n",
      "N_step_low_level: 12\n",
      "==== EXPERT MODE ENABLE BC: False ENABLE RL: False FULL TRAIN: False ====\n",
      "==== low level action taken: 5 N_pickable_item: 3 ====\n",
      "[SUCCESS] get raw data\n",
      "Qx: tensor([[0.4295, 0.4920, 0.5381]], device='cuda:0')\n",
      "Qy: tensor([[0.5315, 0.4399, 0.6001]], device='cuda:0')\n",
      "Qz: tensor([[0.5974, 0.5258, 0.4759]], device='cuda:0')\n",
      "Qyaw: tensor([[0.5582, 0.3431, 0.4080]], device='cuda:0')\n",
      "[MAX OUTPUT]\n",
      "[Q net] x: 2, y: 2, z: 0, yaw_ind: 0\n",
      "[SUCCESS] estimate actions from network\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "gripper tip height: 0.24941296022614318\n",
      "[PUSH REWARD] R: 0.0\n",
      "[OVERALL REWARD] 0.0\n",
      "[ACTION TYPE]: 1\n",
      "[MOVE] move: [ 0.015      0.015     -0.02       0.2617994] move index: [2, 2, 0, 2]\n",
      "next move: tensor([ 0.0150,  0.0000, -0.0200,  0.0000], device='cuda:0') next move index: [2, 1, 0, 1]\n",
      "target_Qx: 0.5139850974082947 Qx: 0.5380582809448242\n",
      "target_Qy: 0.6088701486587524 Qy: 0.6000857949256897\n",
      "target_Qz: 0.595649778842926 Qz: 0.5974322557449341\n",
      "target_Qyaw: 0.39894020557403564 Qyaw: 0.40799474716186523\n",
      "loss_rank: 0.03325754404067993\n",
      "priority: 0.03344300389289856\n",
      "[SUCCESS] append transition experience\n",
      "==== EXPERT MODE ENABLE BC: False ENABLE RL: False FULL TRAIN: False ====\n",
      "==== low level action taken: 5 N_pickable_item: 3 ====\n",
      "[SUCCESS] get raw data\n",
      "Qx: tensor([[0.4583, 0.4864, 0.5061]], device='cuda:0')\n",
      "Qy: tensor([[0.5524, 0.6053, 0.4656]], device='cuda:0')\n",
      "Qz: tensor([[0.5961, 0.3268, 0.4534]], device='cuda:0')\n",
      "Qyaw: tensor([[0.4698, 0.4037, 0.3001]], device='cuda:0')\n",
      "[MAX OUTPUT]\n",
      "[Q net] x: 2, y: 1, z: 0, yaw_ind: 0\n",
      "[SUCCESS] estimate actions from network\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "gripper tip height: 0.22811337947610125\n",
      "[PUSH REWARD] R: 0.0\n",
      "[OVERALL REWARD] 0.0\n",
      "[ACTION TYPE]: 1\n",
      "[MOVE] move: [ 0.015  0.    -0.02   0.   ] move index: [2, 1, 0, 1]\n",
      "next move: tensor([ 0.0000,  0.0150, -0.0200,  0.2618], device='cuda:0') next move index: [1, 2, 0, 2]\n",
      "target_Qx: 0.4526892602443695 Qx: 0.5060868263244629\n",
      "target_Qy: 0.5170977115631104 Qy: 0.6053359508514404\n",
      "target_Qz: 0.5467105507850647 Qz: 0.5961355566978455\n",
      "target_Qyaw: 0.21219894289970398 Qyaw: 0.40369415283203125\n",
      "loss_rank: 0.028806552290916443\n",
      "priority: 0.04124418646097183\n",
      "[SUCCESS] append transition experience\n",
      "==== EXPERT MODE ENABLE BC: False ENABLE RL: False FULL TRAIN: False ====\n",
      "==== low level action taken: 5 N_pickable_item: 3 ====\n",
      "[SUCCESS] get raw data\n",
      "Qx: tensor([[0.3707, 0.4781, 0.4740]], device='cuda:0')\n",
      "Qy: tensor([[0.4926, 0.6312, 0.5247]], device='cuda:0')\n",
      "Qz: tensor([[0.5824, 0.3088, 0.3405]], device='cuda:0')\n",
      "Qyaw: tensor([[0.3880, 0.3536, 0.2476]], device='cuda:0')\n",
      "[MAX OUTPUT]\n",
      "[Q net] x: 1, y: 1, z: 0, yaw_ind: 0\n",
      "[SUCCESS] estimate actions from network\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "gripper tip height: 0.20778862326576727\n",
      "[PUSH REWARD] R: 0.0\n",
      "[OVERALL REWARD] 0.0\n",
      "[ACTION TYPE]: 1\n",
      "[MOVE] move: [ 0.         0.015     -0.02       0.2617994] move index: [1, 2, 0, 2]\n",
      "next move: tensor([ 0.0150,  0.0000, -0.0200,  0.0000], device='cuda:0') next move index: [2, 1, 0, 1]\n",
      "target_Qx: 0.361940860748291 Qx: 0.4780921936035156\n",
      "target_Qy: 0.491858571767807 Qy: 0.5247011184692383\n",
      "target_Qz: 0.3892521560192108 Qz: 0.5823724269866943\n",
      "target_Qyaw: 0.39735639095306396 Qyaw: 0.24757738411426544\n",
      "loss_rank: 0.06805870682001114\n",
      "priority: 0.08663344383239746\n",
      "[SUCCESS] append transition experience\n",
      "==== EXPERT MODE ENABLE BC: False ENABLE RL: False FULL TRAIN: False ====\n",
      "==== low level action taken: 5 N_pickable_item: 3 ====\n",
      "[SUCCESS] get raw data\n",
      "Qx: tensor([[0.3078, 0.4909, 0.4101]], device='cuda:0')\n",
      "Qy: tensor([[0.4491, 0.5097, 0.4308]], device='cuda:0')\n",
      "Qz: tensor([[0.4252, 0.3798, 0.3475]], device='cuda:0')\n",
      "Qyaw: tensor([[0.3768, 0.4110, 0.4367]], device='cuda:0')\n",
      "[MAX OUTPUT]\n",
      "[Q net] x: 1, y: 1, z: 0, yaw_ind: 2\n",
      "[SUCCESS] estimate actions from network\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "gripper tip height: 0.18737841432742153\n",
      "[PUSH REWARD] R: 0.0\n",
      "[OVERALL REWARD] 0.0\n",
      "[ACTION TYPE]: 1\n",
      "[MOVE] move: [ 0.015  0.    -0.02   0.   ] move index: [2, 1, 0, 1]\n",
      "next move: tensor([ 0.0000,  0.0150, -0.0200,  0.2618], device='cuda:0') next move index: [1, 2, 0, 2]\n",
      "target_Qx: 0.42852383852005005 Qx: 0.41014790534973145\n",
      "target_Qy: 0.5551101565361023 Qy: 0.5096915364265442\n",
      "target_Qz: 0.5066751837730408 Qz: 0.42520102858543396\n",
      "target_Qyaw: 0.22011640667915344 Qyaw: 0.4110451936721802\n",
      "loss_rank: 0.042455509305000305\n",
      "priority: 0.05382860079407692\n",
      "[SUCCESS] append transition experience\n",
      "==== EXPERT MODE ENABLE BC: False ENABLE RL: False FULL TRAIN: False ====\n",
      "==== low level action taken: 5 N_pickable_item: 3 ====\n",
      "[SUCCESS] get raw data\n",
      "Qx: tensor([[0.3256, 0.4285, 0.4312]], device='cuda:0')\n",
      "Qy: tensor([[0.4583, 0.5268, 0.5692]], device='cuda:0')\n",
      "Qz: tensor([[0.5215, 0.3467, 0.3659]], device='cuda:0')\n",
      "Qyaw: tensor([[0.3446, 0.3951, 0.2527]], device='cuda:0')\n",
      "[MAX OUTPUT]\n",
      "[Q net] x: 2, y: 2, z: 0, yaw_ind: 1\n",
      "[SUCCESS] estimate actions from network\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "gripper tip height: 0.1667710816570936\n",
      "[PUSH REWARD] R: 0.0\n",
      "[OVERALL REWARD] 0.0\n",
      "[ACTION TYPE]: 1\n",
      "[MOVE] move: [ 0.         0.015     -0.02       0.2617994] move index: [1, 2, 0, 2]\n",
      "next move: tensor([ 0.0150,  0.0000, -0.0200,  0.0000], device='cuda:0') next move index: [2, 1, 0, 1]\n",
      "target_Qx: 0.34741273522377014 Qx: 0.4284926652908325\n",
      "target_Qy: 0.5095438957214355 Qy: 0.5692145228385925\n",
      "target_Qz: 0.5730977058410645 Qz: 0.5214625597000122\n",
      "target_Qyaw: 0.4758368730545044 Qyaw: 0.2526630163192749\n",
      "loss_rank: 0.0495569184422493\n",
      "priority: 0.06520874053239822\n",
      "[SUCCESS] append transition experience\n",
      "==== EXPERT MODE ENABLE BC: False ENABLE RL: False FULL TRAIN: False ====\n",
      "==== low level action taken: 5 N_pickable_item: 3 ====\n",
      "[SUCCESS] get raw data\n",
      "Qx: tensor([[0.3173, 0.4390, 0.3666]], device='cuda:0')\n",
      "Qy: tensor([[0.3939, 0.5076, 0.4071]], device='cuda:0')\n",
      "Qz: tensor([[0.5856, 0.4009, 0.4498]], device='cuda:0')\n",
      "Qyaw: tensor([[0.3590, 0.4800, 0.3809]], device='cuda:0')\n",
      "[RANDOM OUTPUT]\n",
      "[Q net] x: 0, y: 2, z: 0, yaw_ind: 0\n",
      "[SUCCESS] estimate actions from network\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "gripper tip height: 0.14641561721974228\n",
      "[PUSH REWARD] R: 0.0\n",
      "[OVERALL REWARD] 0.0\n",
      "[ACTION TYPE]: 1\n",
      "[MOVE] move: [ 0.015  0.    -0.02   0.   ] move index: [2, 1, 0, 1]\n",
      "next move: tensor([ 0.0150,  0.0150, -0.0200,  0.2618], device='cuda:0') next move index: [2, 2, 0, 2]\n",
      "target_Qx: 0.43931686878204346 Qx: 0.36664479970932007\n",
      "target_Qy: 0.6084707379341125 Qy: 0.507580578327179\n",
      "target_Qz: 0.6906130909919739 Qz: 0.5855732560157776\n",
      "target_Qyaw: 0.5850998759269714 Qyaw: 0.4800204038619995\n",
      "loss_rank: 0.018663249909877777\n",
      "priority: 0.028047028928995132\n",
      "[SUCCESS] append transition experience\n",
      "==== EXPERT MODE ENABLE BC: False ENABLE RL: False FULL TRAIN: False ====\n",
      "==== low level action taken: 5 N_pickable_item: 3 ====\n",
      "[SUCCESS] get raw data\n",
      "Qx: tensor([[0.3588, 0.5441, 0.4495]], device='cuda:0')\n",
      "Qy: tensor([[0.5335, 0.6472, 0.6027]], device='cuda:0')\n",
      "Qz: tensor([[0.7234, 0.4900, 0.3931]], device='cuda:0')\n",
      "Qyaw: tensor([[0.5561, 0.6028, 0.5982]], device='cuda:0')\n",
      "[RANDOM OUTPUT]\n",
      "[Q net] x: 0, y: 1, z: 0, yaw_ind: 1\n",
      "[SUCCESS] estimate actions from network\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "gripper tip height: 0.12605478079909616\n",
      "[PUSH REWARD] R: 0.0\n",
      "[OVERALL REWARD] 0.0\n",
      "[ACTION TYPE]: 1\n",
      "[MOVE] move: [ 0.015      0.015     -0.02       0.2617994] move index: [2, 2, 0, 2]\n",
      "next move: tensor([ 0.0000,  0.0000, -0.0200,  0.0000], device='cuda:0') next move index: [1, 1, 0, 1]\n",
      "target_Qx: 0.560470461845398 Qx: 0.44953277707099915\n",
      "target_Qy: 0.7670650482177734 Qy: 0.6027419567108154\n",
      "target_Qz: 0.7372352480888367 Qz: 0.7233921885490417\n",
      "target_Qyaw: 0.6668530106544495 Qyaw: 0.5981574654579163\n",
      "loss_rank: 0.04514113813638687\n",
      "priority: 0.05619612708687782\n",
      "[SUCCESS] append transition experience\n",
      "==== EXPERT MODE ENABLE BC: False ENABLE RL: False FULL TRAIN: False ====\n",
      "==== low level action taken: 5 N_pickable_item: 3 ====\n",
      "[SUCCESS] get raw data\n",
      "Qx: tensor([[0.4363, 0.4778, 0.5694]], device='cuda:0')\n",
      "Qy: tensor([[0.5992, 0.7043, 0.5932]], device='cuda:0')\n",
      "Qz: tensor([[0.6778, 0.4960, 0.4905]], device='cuda:0')\n",
      "Qyaw: tensor([[0.4424, 0.6445, 0.6567]], device='cuda:0')\n",
      "[MAX OUTPUT]\n",
      "[Q net] x: 2, y: 1, z: 0, yaw_ind: 2\n",
      "[SUCCESS] estimate actions from network\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "gripper tip height: 0.10575234524737714\n",
      "[PUSH REWARD] R: 0.0\n",
      "[OVERALL REWARD] 0.0\n",
      "[ACTION TYPE]: 1\n",
      "[MOVE] move: [ 0.    0.   -0.02  0.  ] move index: [1, 1, 0, 1]\n",
      "next move: tensor([ 0.0150,  0.0150, -0.0200,  0.2618], device='cuda:0') next move index: [2, 2, 0, 2]\n",
      "target_Qx: 0.7806188464164734 Qx: 0.4778136610984802\n",
      "target_Qy: 0.8385036587715149 Qy: 0.7043001055717468\n",
      "target_Qz: 0.8656564950942993 Qz: 0.6778432130813599\n",
      "target_Qyaw: 0.8266603350639343 Qyaw: 0.644537627696991\n",
      "loss_rank: 0.030186373740434647\n",
      "priority: 0.07472239434719086\n",
      "[SUCCESS] append transition experience\n",
      "==== EXPERT MODE ENABLE BC: False ENABLE RL: False FULL TRAIN: False ====\n",
      "==== low level action taken: 5 N_pickable_item: 3 ====\n",
      "[SUCCESS] get raw data\n",
      "Qx: tensor([[0.5990, 0.7007, 0.8169]], device='cuda:0')\n",
      "Qy: tensor([[0.6126, 0.7868, 0.8206]], device='cuda:0')\n",
      "Qz: tensor([[0.8589, 0.6547, 0.5884]], device='cuda:0')\n",
      "Qyaw: tensor([[0.6685, 0.9144, 0.8067]], device='cuda:0')\n",
      "[MAX OUTPUT]\n",
      "[Q net] x: 2, y: 2, z: 0, yaw_ind: 1\n",
      "[SUCCESS] estimate actions from network\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[GRIPPER STATUS] FULL ClOSE\n",
      "gripper tip height: 0.08528884915553447\n",
      "[PUSH REWARD] R: 0.0\n",
      "[OVERALL REWARD] 0.0\n",
      "[ACTION TYPE]: 1\n",
      "[MOVE] move: [ 0.015      0.015     -0.02       0.2617994] move index: [2, 2, 0, 2]\n",
      "next move: tensor([ 0.0000,  0.0000, -0.0200,  0.0000], device='cuda:0') next move index: [1, 1, 0, 1]\n",
      "target_Qx: 0.7940239310264587 Qx: 0.8168624639511108\n",
      "target_Qy: 0.8167139291763306 Qy: 0.8206402659416199\n",
      "target_Qz: 0.9070164561271667 Qz: 0.8588907718658447\n",
      "target_Qyaw: 0.849733293056488 Qyaw: 0.806712806224823\n",
      "loss_rank: 0.022815756499767303\n",
      "priority: 0.023991720750927925\n",
      "[SUCCESS] append transition experience\n",
      "==== EXPERT MODE ENABLE BC: False ENABLE RL: False FULL TRAIN: False ====\n",
      "==== low level action taken: 5 N_pickable_item: 3 ====\n",
      "[SUCCESS] get raw data\n",
      "Qx: tensor([[0.6727, 0.7566, 0.6118]], device='cuda:0')\n",
      "Qy: tensor([[0.6854, 0.7465, 0.6794]], device='cuda:0')\n",
      "Qz: tensor([[0.8782, 0.6420, 0.5657]], device='cuda:0')\n",
      "Qyaw: tensor([[0.5954, 0.7942, 0.6103]], device='cuda:0')\n",
      "[MAX OUTPUT]\n",
      "[Q net] x: 1, y: 1, z: 0, yaw_ind: 1\n",
      "[SUCCESS] estimate actions from network\n",
      "[GRIPPER STATUS] FULL CLOSE\n",
      "[GRIPPER STATUS] FULL CLOSE\n",
      "gripper tip height: 0.06538184206265585\n",
      "[PUSH REWARD] R: 0.0\n",
      "[OVERALL REWARD] 0.0\n",
      "[ACTION TYPE]: 1\n",
      "[MOVE] move: [ 0.    0.   -0.02  0.  ] move index: [1, 1, 0, 1]\n",
      "next move: tensor([ 0.0000,  0.0000, -0.0200,  0.0000], device='cuda:0') next move index: [1, 1, 0, 1]\n",
      "target_Qx: 0.8332448601722717 Qx: 0.7565719485282898\n",
      "target_Qy: 0.8485889434814453 Qy: 0.7465366721153259\n",
      "target_Qz: 0.9496270418167114 Qz: 0.8781794309616089\n",
      "target_Qyaw: 0.9034106731414795 Qyaw: 0.794162929058075\n",
      "loss_rank: 0.007321561221033335\n",
      "priority: 0.015654869377613068\n",
      "[SUCCESS] append transition experience\n",
      "==== EXPERT MODE ENABLE BC: False ENABLE RL: False FULL TRAIN: False ====\n",
      "==== low level action taken: 5 N_pickable_item: 3 ====\n",
      "[SUCCESS] get raw data\n",
      "Qx: tensor([[0.8016, 0.8124, 0.6314]], device='cuda:0')\n",
      "Qy: tensor([[0.7459, 0.7793, 0.7703]], device='cuda:0')\n",
      "Qz: tensor([[0.8439, 0.7162, 0.6244]], device='cuda:0')\n",
      "Qyaw: tensor([[0.6408, 0.8240, 0.6447]], device='cuda:0')\n",
      "[RANDOM OUTPUT]\n",
      "[Q net] x: 0, y: 1, z: 1, yaw_ind: 2\n",
      "[SUCCESS] estimate actions from network\n",
      "[GRIPPER STATUS] FULL CLOSE\n",
      "[GRIPPER STATUS] FULL CLOSE\n",
      "gripper tip height: 0.044691611150142085\n",
      "[PUSH REWARD] R: 1.0\n",
      "[OVERALL REWARD] 1.0\n",
      "[ACTION TYPE]: 1\n",
      "[MOVE] move: [ 0.    0.   -0.02  0.  ] move index: [1, 1, 0, 1]\n",
      "next move: tensor([ 0.0000,  0.0000, -0.0200,  0.0000], device='cuda:0') next move index: [1, 1, 0, 1]\n",
      "target_Qx: 1.0 Qx: 0.8123502135276794\n",
      "target_Qy: 1.0 Qy: 0.7792692184448242\n",
      "target_Qz: 1.0 Qz: 0.843874454498291\n",
      "target_Qyaw: 1.0 Qyaw: 0.8239926099777222\n",
      "loss_rank: 0.02058008313179016\n",
      "priority: 0.055402159690856934\n",
      "[SUCCESS] append transition experience\n",
      "[SUCCESS] perform push action\n",
      "=== end of action ===\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[SUCCESS] return home\n",
      "[SUCCESS] return home\n",
      "[SUCCESS] GRASP OR PUSH ACTION\n",
      "[PUSH SUCCESS RATE] 0.8%/-inf% [500]\n",
      "[SUCCESS] save agent data\n",
      "==== episode: 0 ====\n",
      "[MIN_DISTANCE] 0.008921925403112688\n",
      "[compute_push_path]: 0.05000295694148177\n",
      "N_step_x: 3.0, N_step_y: 6.0, N_step_z: 10.0, N_step_yaw: 5.0, N_step: 10.0\n",
      "[MIN_DISTANCE] 0.030437257319701896\n",
      "[MIN_DISTANCE] 0.008921925403112688\n",
      "N_step_low_level: 12\n",
      "==== EXPERT MODE ENABLE BC: False ENABLE RL: False FULL TRAIN: False ====\n",
      "==== low level action taken: 6 N_pickable_item: 3 ====\n",
      "[SUCCESS] get raw data\n",
      "Qx: tensor([[0.5444, 0.3993, 0.5297]], device='cuda:0')\n",
      "Qy: tensor([[0.5865, 0.3801, 0.5436]], device='cuda:0')\n",
      "Qz: tensor([[0.5834, 0.3838, 0.4239]], device='cuda:0')\n",
      "Qyaw: tensor([[0.5669, 0.4380, 0.4112]], device='cuda:0')\n",
      "[MAX OUTPUT]\n",
      "[Q net] x: 0, y: 0, z: 0, yaw_ind: 0\n",
      "[SUCCESS] estimate actions from network\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "gripper tip height: 0.2499650454967864\n",
      "[PUSH REWARD] R: 0.0\n",
      "[OVERALL REWARD] 0.0\n",
      "[ACTION TYPE]: 1\n",
      "[MOVE] move: [ 0.015      0.015     -0.02       0.2617994] move index: [2, 2, 0, 2]\n",
      "next move: tensor([ 0.0000,  0.0000, -0.0200,  0.0000], device='cuda:0') next move index: [1, 1, 0, 1]\n",
      "target_Qx: 0.3806058466434479 Qx: 0.5297360420227051\n",
      "target_Qy: 0.3602488338947296 Qy: 0.5436256527900696\n",
      "target_Qz: 0.43060827255249023 Qz: 0.5834310054779053\n",
      "target_Qyaw: 0.26982128620147705 Qyaw: 0.4112333655357361\n",
      "loss_rank: 0.05332918092608452\n",
      "priority: 0.07813394069671631\n",
      "[SUCCESS] append transition experience\n",
      "==== EXPERT MODE ENABLE BC: False ENABLE RL: False FULL TRAIN: False ====\n",
      "==== low level action taken: 6 N_pickable_item: 3 ====\n",
      "[SUCCESS] get raw data\n",
      "Qx: tensor([[0.2695, 0.3960, 0.2329]], device='cuda:0')\n",
      "Qy: tensor([[0.4131, 0.3772, 0.4484]], device='cuda:0')\n",
      "Qz: tensor([[0.4597, 0.2370, 0.2684]], device='cuda:0')\n",
      "Qyaw: tensor([[0.4263, 0.3031, 0.1823]], device='cuda:0')\n",
      "[MAX OUTPUT]\n",
      "[Q net] x: 1, y: 2, z: 0, yaw_ind: 0\n",
      "[SUCCESS] estimate actions from network\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "gripper tip height: 0.22942479031642285\n",
      "[PUSH REWARD] R: 0.0\n",
      "[OVERALL REWARD] 0.0\n",
      "[ACTION TYPE]: 1\n",
      "[MOVE] move: [ 0.    0.   -0.02  0.  ] move index: [1, 1, 0, 1]\n",
      "next move: tensor([ 0.0000,  0.0150, -0.0200,  0.2618], device='cuda:0') next move index: [1, 2, 0, 2]\n",
      "target_Qx: 0.3561631143093109 Qx: 0.3960456848144531\n",
      "target_Qy: 0.35017040371894836 Qy: 0.377205491065979\n",
      "target_Qz: 0.37159600853919983 Qz: 0.45965760946273804\n",
      "target_Qyaw: 0.09121347963809967 Qyaw: 0.30307909846305847\n",
      "loss_rank: 0.04419895261526108\n",
      "priority: 0.0579398050904274\n",
      "[SUCCESS] append transition experience\n",
      "==== EXPERT MODE ENABLE BC: False ENABLE RL: False FULL TRAIN: False ====\n",
      "==== low level action taken: 6 N_pickable_item: 3 ====\n",
      "[SUCCESS] get raw data\n",
      "Qx: tensor([[0.2452, 0.3616, 0.2662]], device='cuda:0')\n",
      "Qy: tensor([[0.4278, 0.4211, 0.3966]], device='cuda:0')\n",
      "Qz: tensor([[0.4149, 0.2645, 0.2088]], device='cuda:0')\n",
      "Qyaw: tensor([[0.3924, 0.2771, 0.1134]], device='cuda:0')\n",
      "[RANDOM OUTPUT]\n",
      "[Q net] x: 1, y: 0, z: 1, yaw_ind: 2\n",
      "[SUCCESS] estimate actions from network\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "gripper tip height: 0.20821537279108904\n",
      "[PUSH REWARD] R: 0.0\n",
      "[OVERALL REWARD] 0.0\n",
      "[ACTION TYPE]: 1\n",
      "[MOVE] move: [ 0.         0.015     -0.02       0.2617994] move index: [1, 2, 0, 2]\n",
      "next move: tensor([ 0.0000,  0.0000, -0.0200,  0.0000], device='cuda:0') next move index: [1, 1, 0, 1]\n",
      "target_Qx: 0.19487731158733368 Qx: 0.3615778684616089\n",
      "target_Qy: 0.39714011549949646 Qy: 0.3966429531574249\n",
      "target_Qz: 0.351526141166687 Qz: 0.414894163608551\n",
      "target_Qyaw: 0.24468113481998444 Qyaw: 0.11337117850780487\n",
      "loss_rank: 0.07525661587715149\n",
      "priority: 0.08751840144395828\n",
      "[SUCCESS] append transition experience\n",
      "==== EXPERT MODE ENABLE BC: False ENABLE RL: False FULL TRAIN: False ====\n",
      "==== low level action taken: 6 N_pickable_item: 3 ====\n",
      "[SUCCESS] get raw data\n",
      "Qx: tensor([[0.2171, 0.2022, 0.3294]], device='cuda:0')\n",
      "Qy: tensor([[0.4428, 0.4295, 0.4837]], device='cuda:0')\n",
      "Qz: tensor([[0.4248, 0.2462, 0.1953]], device='cuda:0')\n",
      "Qyaw: tensor([[0.3081, 0.2705, 0.2956]], device='cuda:0')\n",
      "[MAX OUTPUT]\n",
      "[Q net] x: 2, y: 2, z: 0, yaw_ind: 0\n",
      "[SUCCESS] estimate actions from network\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "gripper tip height: 0.1886253950297791\n",
      "[PUSH REWARD] R: 0.0\n",
      "[OVERALL REWARD] 0.0\n",
      "[ACTION TYPE]: 1\n",
      "[MOVE] move: [ 0.    0.   -0.02  0.  ] move index: [1, 1, 0, 1]\n",
      "next move: tensor([ 0.0000,  0.0150, -0.0200,  0.0000], device='cuda:0') next move index: [1, 2, 0, 1]\n",
      "target_Qx: 0.35803237557411194 Qx: 0.2022223174571991\n",
      "target_Qy: 0.546934187412262 Qy: 0.4295041561126709\n",
      "target_Qz: 0.5265335440635681 Qz: 0.42475852370262146\n",
      "target_Qyaw: 0.36631834506988525 Qyaw: 0.27052775025367737\n",
      "loss_rank: 0.07268384844064713\n",
      "priority: 0.08708399534225464\n",
      "[SUCCESS] append transition experience\n",
      "==== EXPERT MODE ENABLE BC: False ENABLE RL: False FULL TRAIN: False ====\n",
      "==== low level action taken: 6 N_pickable_item: 3 ====\n",
      "[SUCCESS] get raw data\n",
      "Qx: tensor([[0.3165, 0.3774, 0.4379]], device='cuda:0')\n",
      "Qy: tensor([[0.4603, 0.4492, 0.5655]], device='cuda:0')\n",
      "Qz: tensor([[0.5691, 0.3394, 0.3752]], device='cuda:0')\n",
      "Qyaw: tensor([[0.4413, 0.3866, 0.4109]], device='cuda:0')\n",
      "[MAX OUTPUT]\n",
      "[Q net] x: 2, y: 2, z: 0, yaw_ind: 0\n",
      "[SUCCESS] estimate actions from network\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "gripper tip height: 0.16793515999953895\n",
      "[PUSH REWARD] R: 0.0\n",
      "[OVERALL REWARD] 0.0\n",
      "[ACTION TYPE]: 1\n",
      "[MOVE] move: [ 0.     0.015 -0.02   0.   ] move index: [1, 2, 0, 1]\n",
      "next move: tensor([ 0.0150,  0.0000, -0.0200,  0.2618], device='cuda:0') next move index: [2, 1, 0, 2]\n",
      "target_Qx: 0.39233237504959106 Qx: 0.37738779187202454\n",
      "target_Qy: 0.4935864508152008 Qy: 0.565503716468811\n",
      "target_Qz: 0.5232328772544861 Qz: 0.569093644618988\n",
      "target_Qyaw: 0.3865952491760254 Qyaw: 0.3865582346916199\n",
      "loss_rank: 0.03989990055561066\n",
      "priority: 0.04177456349134445\n",
      "[SUCCESS] append transition experience\n",
      "==== EXPERT MODE ENABLE BC: False ENABLE RL: False FULL TRAIN: False ====\n",
      "==== low level action taken: 6 N_pickable_item: 3 ====\n",
      "[SUCCESS] get raw data\n",
      "Qx: tensor([[0.3677, 0.4330, 0.4238]], device='cuda:0')\n",
      "Qy: tensor([[0.3667, 0.5280, 0.5403]], device='cuda:0')\n",
      "Qz: tensor([[0.5631, 0.3239, 0.3890]], device='cuda:0')\n",
      "Qyaw: tensor([[0.3221, 0.4036, 0.4270]], device='cuda:0')\n",
      "[MAX OUTPUT]\n",
      "[Q net] x: 1, y: 2, z: 0, yaw_ind: 2\n",
      "[SUCCESS] estimate actions from network\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "gripper tip height: 0.14731404152081184\n",
      "[PUSH REWARD] R: 0.0\n",
      "[OVERALL REWARD] 0.0\n",
      "[ACTION TYPE]: 1\n",
      "[MOVE] move: [ 0.015      0.        -0.02       0.2617994] move index: [2, 1, 0, 2]\n",
      "next move: tensor([ 0.0000,  0.0150, -0.0200,  0.0000], device='cuda:0') next move index: [1, 2, 0, 1]\n",
      "target_Qx: 0.39561936259269714 Qx: 0.42381903529167175\n",
      "target_Qy: 0.512023389339447 Qy: 0.5280401706695557\n",
      "target_Qz: 0.5258603692054749 Qz: 0.563056468963623\n",
      "target_Qyaw: 0.5535088777542114 Qyaw: 0.42702990770339966\n",
      "loss_rank: 0.028495483100414276\n",
      "priority: 0.033103540539741516\n",
      "[SUCCESS] append transition experience\n",
      "==== EXPERT MODE ENABLE BC: False ENABLE RL: False FULL TRAIN: False ====\n",
      "==== low level action taken: 6 N_pickable_item: 3 ====\n",
      "[SUCCESS] get raw data\n",
      "Qx: tensor([[0.2295, 0.3728, 0.3567]], device='cuda:0')\n",
      "Qy: tensor([[0.4332, 0.4968, 0.4838]], device='cuda:0')\n",
      "Qz: tensor([[0.5033, 0.3359, 0.3713]], device='cuda:0')\n",
      "Qyaw: tensor([[0.3441, 0.5019, 0.5001]], device='cuda:0')\n",
      "[MAX OUTPUT]\n",
      "[Q net] x: 1, y: 1, z: 0, yaw_ind: 1\n",
      "[SUCCESS] estimate actions from network\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "gripper tip height: 0.12599298820306826\n",
      "[PUSH REWARD] R: 0.0\n",
      "[OVERALL REWARD] 0.0\n",
      "[ACTION TYPE]: 1\n",
      "[MOVE] move: [ 0.     0.015 -0.02   0.   ] move index: [1, 2, 0, 1]\n",
      "next move: tensor([ 0.0000,  0.0000, -0.0200,  0.2618], device='cuda:0') next move index: [1, 1, 0, 2]\n",
      "target_Qx: 0.5589320659637451 Qx: 0.37277793884277344\n",
      "target_Qy: 0.608267068862915 Qy: 0.4838264286518097\n",
      "target_Qz: 0.5724926590919495 Qz: 0.5033392906188965\n",
      "target_Qyaw: 0.5226629376411438 Qyaw: 0.5018518567085266\n",
      "loss_rank: 0.02871132642030716\n",
      "priority: 0.04254985600709915\n",
      "[SUCCESS] append transition experience\n",
      "==== EXPERT MODE ENABLE BC: False ENABLE RL: False FULL TRAIN: False ====\n",
      "==== low level action taken: 6 N_pickable_item: 3 ====\n",
      "[SUCCESS] get raw data\n",
      "Qx: tensor([[0.3446, 0.5451, 0.4473]], device='cuda:0')\n",
      "Qy: tensor([[0.4669, 0.6152, 0.5932]], device='cuda:0')\n",
      "Qz: tensor([[0.5896, 0.4137, 0.5053]], device='cuda:0')\n",
      "Qyaw: tensor([[0.5056, 0.5733, 0.5412]], device='cuda:0')\n",
      "[MAX OUTPUT]\n",
      "[Q net] x: 1, y: 1, z: 0, yaw_ind: 1\n",
      "[SUCCESS] estimate actions from network\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "gripper tip height: 0.10646273704596454\n",
      "[PUSH REWARD] R: 0.0\n",
      "[OVERALL REWARD] 0.0\n",
      "[ACTION TYPE]: 1\n",
      "[MOVE] move: [ 0.         0.        -0.02       0.2617994] move index: [1, 1, 0, 2]\n",
      "next move: tensor([ 0.0000,  0.0150, -0.0200,  0.0000], device='cuda:0') next move index: [1, 2, 0, 1]\n",
      "target_Qx: 0.6937413215637207 Qx: 0.5450811386108398\n",
      "target_Qy: 0.6991152763366699 Qy: 0.6152097582817078\n",
      "target_Qz: 0.7674214243888855 Qz: 0.5895888209342957\n",
      "target_Qyaw: 0.7606310248374939 Qyaw: 0.5412002205848694\n",
      "loss_rank: 0.02436554804444313\n",
      "priority: 0.051594123244285583\n",
      "[SUCCESS] append transition experience\n",
      "==== EXPERT MODE ENABLE BC: False ENABLE RL: False FULL TRAIN: False ====\n",
      "==== low level action taken: 6 N_pickable_item: 3 ====\n",
      "[SUCCESS] get raw data\n",
      "Qx: tensor([[0.4564, 0.6670, 0.5203]], device='cuda:0')\n",
      "Qy: tensor([[0.6078, 0.6668, 0.7159]], device='cuda:0')\n",
      "Qz: tensor([[0.7645, 0.5400, 0.5275]], device='cuda:0')\n",
      "Qyaw: tensor([[0.5295, 0.7325, 0.6549]], device='cuda:0')\n",
      "[MAX OUTPUT]\n",
      "[Q net] x: 1, y: 2, z: 0, yaw_ind: 1\n",
      "[SUCCESS] estimate actions from network\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[GRIPPER STATUS] FULL ClOSE\n",
      "gripper tip height: 0.08579129004212566\n",
      "[PUSH REWARD] R: 0.0\n",
      "[OVERALL REWARD] 0.0\n",
      "[ACTION TYPE]: 1\n",
      "[MOVE] move: [ 0.     0.015 -0.02   0.   ] move index: [1, 2, 0, 1]\n",
      "next move: tensor([ 0.0000,  0.0000, -0.0200,  0.0000], device='cuda:0') next move index: [1, 1, 0, 1]\n",
      "target_Qx: 0.6382743716239929 Qx: 0.6669802069664001\n",
      "target_Qy: 0.65788733959198 Qy: 0.7158809304237366\n",
      "target_Qz: 0.6810880303382874 Qz: 0.764546275138855\n",
      "target_Qyaw: 0.7047958970069885 Qyaw: 0.7325066924095154\n",
      "loss_rank: 0.0061103422194719315\n",
      "priority: 0.009090454317629337\n",
      "[SUCCESS] append transition experience\n",
      "==== EXPERT MODE ENABLE BC: False ENABLE RL: False FULL TRAIN: False ====\n",
      "==== low level action taken: 6 N_pickable_item: 3 ====\n",
      "[SUCCESS] get raw data\n",
      "Qx: tensor([[0.3942, 0.5153, 0.3621]], device='cuda:0')\n",
      "Qy: tensor([[0.5068, 0.5553, 0.4919]], device='cuda:0')\n",
      "Qz: tensor([[0.5863, 0.4796, 0.4253]], device='cuda:0')\n",
      "Qyaw: tensor([[0.5134, 0.6137, 0.5192]], device='cuda:0')\n",
      "[MAX OUTPUT]\n",
      "[Q net] x: 1, y: 1, z: 0, yaw_ind: 1\n",
      "[SUCCESS] estimate actions from network\n",
      "[GRIPPER STATUS] FULL CLOSE\n",
      "[GRIPPER STATUS] FULL CLOSE\n",
      "gripper tip height: 0.06444484630186176\n",
      "[PUSH REWARD] R: 0.0\n",
      "[OVERALL REWARD] 0.0\n",
      "[ACTION TYPE]: 1\n",
      "[MOVE] move: [ 0.    0.   -0.02  0.  ] move index: [1, 1, 0, 1]\n",
      "next move: tensor([ 0.0000,  0.0000, -0.0200,  0.0000], device='cuda:0') next move index: [1, 1, 0, 1]\n",
      "target_Qx: 0.780083954334259 Qx: 0.5152574777603149\n",
      "target_Qy: 0.8080852031707764 Qy: 0.5552937388420105\n",
      "target_Qz: 0.8312207460403442 Qz: 0.5863164663314819\n",
      "target_Qyaw: 0.8670873641967773 Qyaw: 0.6137046217918396\n",
      "loss_rank: 0.007806054782122374\n",
      "priority: 0.07236043363809586\n",
      "[SUCCESS] append transition experience\n",
      "==== EXPERT MODE ENABLE BC: False ENABLE RL: False FULL TRAIN: False ====\n",
      "==== low level action taken: 6 N_pickable_item: 3 ====\n",
      "[SUCCESS] get raw data\n",
      "Qx: tensor([[0.5320, 0.6349, 0.4413]], device='cuda:0')\n",
      "Qy: tensor([[0.6310, 0.6486, 0.5888]], device='cuda:0')\n",
      "Qz: tensor([[0.6624, 0.4841, 0.5252]], device='cuda:0')\n",
      "Qyaw: tensor([[0.4215, 0.7267, 0.4864]], device='cuda:0')\n",
      "[MAX OUTPUT]\n",
      "[Q net] x: 1, y: 1, z: 0, yaw_ind: 1\n",
      "[SUCCESS] estimate actions from network\n",
      "[GRIPPER STATUS] FULL CLOSE\n",
      "[GRIPPER STATUS] FULL CLOSE\n",
      "gripper tip height: 0.04408448528791348\n",
      "[PUSH REWARD] R: 1.0\n",
      "[OVERALL REWARD] 1.0\n",
      "[ACTION TYPE]: 1\n",
      "[MOVE] move: [ 0.    0.   -0.02  0.  ] move index: [1, 1, 0, 1]\n",
      "next move: tensor([ 0.0000,  0.0000, -0.0200,  0.0000], device='cuda:0') next move index: [1, 1, 0, 1]\n",
      "target_Qx: 1.0 Qx: 0.6349091529846191\n",
      "target_Qy: 1.0 Qy: 0.6486244201660156\n",
      "target_Qz: 1.0 Qz: 0.6623941659927368\n",
      "target_Qyaw: 1.0 Qyaw: 0.7267345190048218\n",
      "loss_rank: 0.010217549279332161\n",
      "priority: 0.12156951427459717\n",
      "[SUCCESS] append transition experience\n",
      "[SUCCESS] perform push action\n",
      "=== end of action ===\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[SUCCESS] return home\n",
      "[SUCCESS] return home\n",
      "[SUCCESS] GRASP OR PUSH ACTION\n",
      "[PUSH SUCCESS RATE] 1.0%/-inf% [500]\n",
      "[SUCCESS] save agent data\n",
      "==== episode: 0 ====\n",
      "[MIN_DISTANCE] 0.0058896901827733385\n",
      "[compute_push_path]: 0.05000295694148177\n",
      "N_step_x: 1.0, N_step_y: 7.0, N_step_z: 10.0, N_step_yaw: 4.0, N_step: 10.0\n",
      "[MIN_DISTANCE] 0.03046256514614211\n",
      "[MIN_DISTANCE] 0.0058896901827733385\n",
      "N_step_low_level: 12\n",
      "==== EXPERT MODE ENABLE BC: False ENABLE RL: False FULL TRAIN: False ====\n",
      "==== low level action taken: 7 N_pickable_item: 3 ====\n",
      "[SUCCESS] get raw data\n",
      "Qx: tensor([[0.2813, 0.2949, 0.3291]], device='cuda:0')\n",
      "Qy: tensor([[0.3551, 0.3655, 0.4909]], device='cuda:0')\n",
      "Qz: tensor([[0.4057, 0.3086, 0.2531]], device='cuda:0')\n",
      "Qyaw: tensor([[0.4029, 0.2330, 0.2210]], device='cuda:0')\n",
      "[MAX OUTPUT]\n",
      "[Q net] x: 2, y: 2, z: 0, yaw_ind: 0\n",
      "[SUCCESS] estimate actions from network\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "gripper tip height: 0.24883994764039635\n",
      "[PUSH REWARD] R: 0.0\n",
      "[OVERALL REWARD] 0.0\n",
      "[ACTION TYPE]: 1\n",
      "[MOVE] move: [-0.015      0.015     -0.02       0.2617994] move index: [0, 2, 0, 2]\n",
      "next move: tensor([ 0.0000,  0.0150, -0.0200,  0.0000], device='cuda:0') next move index: [1, 2, 0, 1]\n",
      "target_Qx: 0.2605411112308502 Qx: 0.28133392333984375\n",
      "target_Qy: 0.45960670709609985 Qy: 0.4908697009086609\n",
      "target_Qz: 0.32390686869621277 Qz: 0.40572118759155273\n",
      "target_Qyaw: 0.1000671237707138 Qyaw: 0.22097377479076385\n",
      "loss_rank: 0.05484472215175629\n",
      "priority: 0.06052515283226967\n",
      "[SUCCESS] append transition experience\n",
      "==== EXPERT MODE ENABLE BC: False ENABLE RL: False FULL TRAIN: False ====\n",
      "==== low level action taken: 7 N_pickable_item: 3 ====\n",
      "[SUCCESS] get raw data\n",
      "Qx: tensor([[0.2612, 0.2782, 0.3527]], device='cuda:0')\n",
      "Qy: tensor([[0.4084, 0.4332, 0.5056]], device='cuda:0')\n",
      "Qz: tensor([[0.3949, 0.2617, 0.2345]], device='cuda:0')\n",
      "Qyaw: tensor([[0.2684, 0.1723, 0.0043]], device='cuda:0')\n",
      "[MAX OUTPUT]\n",
      "[Q net] x: 2, y: 2, z: 0, yaw_ind: 0\n",
      "[SUCCESS] estimate actions from network\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "gripper tip height: 0.22888251709092733\n",
      "[PUSH REWARD] R: 0.0\n",
      "[OVERALL REWARD] 0.0\n",
      "[ACTION TYPE]: 1\n",
      "[MOVE] move: [ 0.     0.015 -0.02   0.   ] move index: [1, 2, 0, 1]\n",
      "next move: tensor([ 0.0000,  0.0000, -0.0200,  0.0000], device='cuda:0') next move index: [1, 1, 0, 1]\n",
      "target_Qx: 0.3708290457725525 Qx: 0.278202086687088\n",
      "target_Qy: 0.37115678191185 Qy: 0.5056151151657104\n",
      "target_Qz: 0.3764815032482147 Qz: 0.39492714405059814\n",
      "target_Qyaw: 0.1779177337884903 Qyaw: 0.17229710519313812\n",
      "loss_rank: 0.04032915085554123\n",
      "priority: 0.04708680883049965\n",
      "[SUCCESS] append transition experience\n",
      "==== EXPERT MODE ENABLE BC: False ENABLE RL: False FULL TRAIN: False ====\n",
      "==== low level action taken: 7 N_pickable_item: 3 ====\n",
      "[SUCCESS] get raw data\n",
      "Qx: tensor([[0.2800, 0.3656, 0.3135]], device='cuda:0')\n",
      "Qy: tensor([[0.4508, 0.4074, 0.5161]], device='cuda:0')\n",
      "Qz: tensor([[0.4137, 0.2997, 0.2361]], device='cuda:0')\n",
      "Qyaw: tensor([[0.2485, 0.1796, 0.1326]], device='cuda:0')\n",
      "[MAX OUTPUT]\n",
      "[Q net] x: 1, y: 2, z: 0, yaw_ind: 0\n",
      "[SUCCESS] estimate actions from network\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "gripper tip height: 0.20779295469157094\n",
      "[PUSH REWARD] R: 0.0\n",
      "[OVERALL REWARD] 0.0\n",
      "[ACTION TYPE]: 1\n",
      "[MOVE] move: [ 0.    0.   -0.02  0.  ] move index: [1, 1, 0, 1]\n",
      "next move: tensor([ 0.0000,  0.0150, -0.0200,  0.2618], device='cuda:0') next move index: [1, 2, 0, 2]\n",
      "target_Qx: 0.3786253333091736 Qx: 0.36562255024909973\n",
      "target_Qy: 0.528853178024292 Qy: 0.407375305891037\n",
      "target_Qz: 0.4618270993232727 Qz: 0.4137237071990967\n",
      "target_Qyaw: 0.19121922552585602 Qyaw: 0.17955760657787323\n",
      "loss_rank: 0.05304013192653656\n",
      "priority: 0.057384099811315536\n",
      "[SUCCESS] append transition experience\n",
      "==== EXPERT MODE ENABLE BC: False ENABLE RL: False FULL TRAIN: False ====\n",
      "==== low level action taken: 7 N_pickable_item: 3 ====\n",
      "[SUCCESS] get raw data\n",
      "Qx: tensor([[0.2937, 0.3989, 0.3464]], device='cuda:0')\n",
      "Qy: tensor([[0.4814, 0.5362, 0.5531]], device='cuda:0')\n",
      "Qz: tensor([[0.4918, 0.3317, 0.3148]], device='cuda:0')\n",
      "Qyaw: tensor([[0.3402, 0.3046, 0.2045]], device='cuda:0')\n",
      "[MAX OUTPUT]\n",
      "[Q net] x: 1, y: 2, z: 0, yaw_ind: 0\n",
      "[SUCCESS] estimate actions from network\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "gripper tip height: 0.18812820454034337\n",
      "[PUSH REWARD] R: 0.0\n",
      "[OVERALL REWARD] 0.0\n",
      "[ACTION TYPE]: 1\n",
      "[MOVE] move: [ 0.         0.015     -0.02       0.2617994] move index: [1, 2, 0, 2]\n",
      "next move: tensor([ 0.0000,  0.0000, -0.0200,  0.0000], device='cuda:0') next move index: [1, 1, 0, 1]\n",
      "target_Qx: 0.24222275614738464 Qx: 0.3988597095012665\n",
      "target_Qy: 0.25759047269821167 Qy: 0.5530681610107422\n",
      "target_Qz: 0.2898276448249817 Qz: 0.4918403625488281\n",
      "target_Qyaw: 0.2699931263923645 Qyaw: 0.20454950630664825\n",
      "loss_rank: 0.04955212399363518\n",
      "priority: 0.08878567814826965\n",
      "[SUCCESS] append transition experience\n",
      "==== EXPERT MODE ENABLE BC: False ENABLE RL: False FULL TRAIN: False ====\n",
      "==== low level action taken: 7 N_pickable_item: 3 ====\n",
      "[SUCCESS] get raw data\n",
      "Qx: tensor([[0.1997, 0.2597, 0.2906]], device='cuda:0')\n",
      "Qy: tensor([[0.2255, 0.2577, 0.4895]], device='cuda:0')\n",
      "Qz: tensor([[0.3055, 0.2482, 0.2271]], device='cuda:0')\n",
      "Qyaw: tensor([[0.1331, 0.2955, 0.2350]], device='cuda:0')\n",
      "[MAX OUTPUT]\n",
      "[Q net] x: 2, y: 2, z: 0, yaw_ind: 1\n",
      "[SUCCESS] estimate actions from network\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "gripper tip height: 0.16752102875938496\n",
      "[PUSH REWARD] R: 0.0\n",
      "[OVERALL REWARD] 0.0\n",
      "[ACTION TYPE]: 1\n",
      "[MOVE] move: [ 0.    0.   -0.02  0.  ] move index: [1, 1, 0, 1]\n",
      "next move: tensor([ 0.0000,  0.0150, -0.0200,  0.0000], device='cuda:0') next move index: [1, 2, 0, 1]\n",
      "target_Qx: 0.3948182165622711 Qx: 0.25967445969581604\n",
      "target_Qy: 0.4523535370826721 Qy: 0.2576821744441986\n",
      "target_Qz: 0.32636621594429016 Qz: 0.3054925203323364\n",
      "target_Qyaw: 0.2689155340194702 Qyaw: 0.29548323154449463\n",
      "loss_rank: 0.05621011182665825\n",
      "priority: 0.07053568959236145\n",
      "[SUCCESS] append transition experience\n",
      "==== EXPERT MODE ENABLE BC: False ENABLE RL: False FULL TRAIN: False ====\n",
      "==== low level action taken: 7 N_pickable_item: 3 ====\n",
      "[SUCCESS] get raw data\n",
      "Qx: tensor([[0.1984, 0.4094, 0.2522]], device='cuda:0')\n",
      "Qy: tensor([[0.1911, 0.4035, 0.4789]], device='cuda:0')\n",
      "Qz: tensor([[0.3311, 0.2131, 0.1345]], device='cuda:0')\n",
      "Qyaw: tensor([[0.1458, 0.2693, 0.0666]], device='cuda:0')\n",
      "[MAX OUTPUT]\n",
      "[Q net] x: 1, y: 2, z: 0, yaw_ind: 1\n",
      "[SUCCESS] estimate actions from network\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "gripper tip height: 0.14652295347976538\n",
      "[PUSH REWARD] R: 0.0\n",
      "[OVERALL REWARD] 0.0\n",
      "[ACTION TYPE]: 1\n",
      "[MOVE] move: [ 0.     0.015 -0.02   0.   ] move index: [1, 2, 0, 1]\n",
      "next move: tensor([ 0.0000,  0.0150, -0.0200,  0.2618], device='cuda:0') next move index: [1, 2, 0, 2]\n",
      "target_Qx: 0.49184587597846985 Qx: 0.4094443619251251\n",
      "target_Qy: 0.5985640287399292 Qy: 0.47887730598449707\n",
      "target_Qz: 0.5383457541465759 Qz: 0.33113348484039307\n",
      "target_Qyaw: 0.38829225301742554 Qyaw: 0.2693239748477936\n",
      "loss_rank: 0.002053363248705864\n",
      "priority: 0.021604686975479126\n",
      "[SUCCESS] append transition experience\n",
      "==== EXPERT MODE ENABLE BC: False ENABLE RL: False FULL TRAIN: False ====\n",
      "==== low level action taken: 7 N_pickable_item: 3 ====\n",
      "[SUCCESS] get raw data\n",
      "Qx: tensor([[0.4116, 0.4777, 0.3821]], device='cuda:0')\n",
      "Qy: tensor([[0.4206, 0.4736, 0.6124]], device='cuda:0')\n",
      "Qz: tensor([[0.5493, 0.3782, 0.3790]], device='cuda:0')\n",
      "Qyaw: tensor([[0.2644, 0.4050, 0.3967]], device='cuda:0')\n",
      "[MAX OUTPUT]\n",
      "[Q net] x: 1, y: 2, z: 0, yaw_ind: 1\n",
      "[SUCCESS] estimate actions from network\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "gripper tip height: 0.12596374948491112\n",
      "[PUSH REWARD] R: 0.0\n",
      "[OVERALL REWARD] 0.0\n",
      "[ACTION TYPE]: 1\n",
      "[MOVE] move: [ 0.         0.015     -0.02       0.2617994] move index: [1, 2, 0, 2]\n",
      "next move: tensor([ 0.0000,  0.0000, -0.0200,  0.0000], device='cuda:0') next move index: [1, 1, 0, 1]\n",
      "target_Qx: 0.6157175302505493 Qx: 0.4777204394340515\n",
      "target_Qy: 0.68150794506073 Qy: 0.6124493479728699\n",
      "target_Qz: 0.6640072464942932 Qz: 0.5493440628051758\n",
      "target_Qyaw: 0.660504937171936 Qyaw: 0.3967331647872925\n",
      "loss_rank: 0.012211689725518227\n",
      "priority: 0.03884556144475937\n",
      "[SUCCESS] append transition experience\n",
      "==== EXPERT MODE ENABLE BC: False ENABLE RL: False FULL TRAIN: False ====\n",
      "==== low level action taken: 7 N_pickable_item: 3 ====\n",
      "[SUCCESS] get raw data\n",
      "Qx: tensor([[0.3108, 0.6016, 0.3310]], device='cuda:0')\n",
      "Qy: tensor([[0.5084, 0.6564, 0.6067]], device='cuda:0')\n",
      "Qz: tensor([[0.6214, 0.5124, 0.3772]], device='cuda:0')\n",
      "Qyaw: tensor([[0.5473, 0.6353, 0.4326]], device='cuda:0')\n",
      "[MAX OUTPUT]\n",
      "[Q net] x: 1, y: 1, z: 0, yaw_ind: 1\n",
      "[SUCCESS] estimate actions from network\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "gripper tip height: 0.10477097080908404\n",
      "[PUSH REWARD] R: 0.0\n",
      "[OVERALL REWARD] 0.0\n",
      "[ACTION TYPE]: 1\n",
      "[MOVE] move: [ 0.    0.   -0.02  0.  ] move index: [1, 1, 0, 1]\n",
      "next move: tensor([ 0.0000,  0.0150, -0.0200,  0.0000], device='cuda:0') next move index: [1, 2, 0, 1]\n",
      "target_Qx: 0.7304986715316772 Qx: 0.6016175150871277\n",
      "target_Qy: 0.6971484422683716 Qy: 0.6563653945922852\n",
      "target_Qz: 0.8296452164649963 Qz: 0.6214128136634827\n",
      "target_Qyaw: 0.7749866247177124 Qyaw: 0.6352917551994324\n",
      "loss_rank: 0.005199170671403408\n",
      "priority: 0.025486420840024948\n",
      "[SUCCESS] append transition experience\n",
      "==== EXPERT MODE ENABLE BC: False ENABLE RL: False FULL TRAIN: False ====\n",
      "==== low level action taken: 7 N_pickable_item: 3 ====\n",
      "[SUCCESS] get raw data\n",
      "Qx: tensor([[0.5326, 0.7117, 0.4506]], device='cuda:0')\n",
      "Qy: tensor([[0.4462, 0.6149, 0.6974]], device='cuda:0')\n",
      "Qz: tensor([[0.7988, 0.6335, 0.4932]], device='cuda:0')\n",
      "Qyaw: tensor([[0.5731, 0.7424, 0.5766]], device='cuda:0')\n",
      "[MAX OUTPUT]\n",
      "[Q net] x: 1, y: 2, z: 0, yaw_ind: 1\n",
      "[SUCCESS] estimate actions from network\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[GRIPPER STATUS] FULL ClOSE\n",
      "gripper tip height: 0.08417940691077841\n",
      "[PUSH REWARD] R: 0.0\n",
      "[OVERALL REWARD] 0.0\n",
      "[ACTION TYPE]: 1\n",
      "[MOVE] move: [ 0.     0.015 -0.02   0.   ] move index: [1, 2, 0, 1]\n",
      "next move: tensor([ 0.0000,  0.0000, -0.0200,  0.0000], device='cuda:0') next move index: [1, 1, 0, 1]\n",
      "target_Qx: 0.7649631500244141 Qx: 0.7117060422897339\n",
      "target_Qy: 0.8089513778686523 Qy: 0.6973637938499451\n",
      "target_Qz: 0.8392078876495361 Qz: 0.7987975478172302\n",
      "target_Qyaw: 0.8099974989891052 Qyaw: 0.7423830628395081\n",
      "loss_rank: 0.001464489265345037\n",
      "priority: 0.0068376935087144375\n",
      "[SUCCESS] append transition experience\n",
      "==== EXPERT MODE ENABLE BC: False ENABLE RL: False FULL TRAIN: False ====\n",
      "==== low level action taken: 7 N_pickable_item: 3 ====\n",
      "[SUCCESS] get raw data\n",
      "Qx: tensor([[0.7711, 0.7525, 0.5994]], device='cuda:0')\n",
      "Qy: tensor([[0.6889, 0.7946, 0.7267]], device='cuda:0')\n",
      "Qz: tensor([[0.8540, 0.6787, 0.5903]], device='cuda:0')\n",
      "Qyaw: tensor([[0.6857, 0.8080, 0.6409]], device='cuda:0')\n",
      "[MAX OUTPUT]\n",
      "[Q net] x: 0, y: 1, z: 0, yaw_ind: 1\n",
      "[SUCCESS] estimate actions from network\n",
      "[GRIPPER STATUS] FULL CLOSE\n",
      "[GRIPPER STATUS] FULL CLOSE\n",
      "gripper tip height: 0.06421391135071541\n",
      "[PUSH REWARD] R: 0.0\n",
      "[OVERALL REWARD] 0.0\n",
      "[ACTION TYPE]: 1\n",
      "[MOVE] move: [ 0.    0.   -0.02  0.  ] move index: [1, 1, 0, 1]\n",
      "next move: tensor([ 0.0000,  0.0000, -0.0200,  0.0000], device='cuda:0') next move index: [1, 1, 0, 1]\n",
      "target_Qx: 0.9347363114356995 Qx: 0.7524944543838501\n",
      "target_Qy: 0.9488821029663086 Qy: 0.7945939302444458\n",
      "target_Qz: 0.968061625957489 Qz: 0.8540288209915161\n",
      "target_Qyaw: 0.9881853461265564 Qyaw: 0.8079690337181091\n",
      "loss_rank: 0.012554383836686611\n",
      "priority: 0.03817896917462349\n",
      "[SUCCESS] append transition experience\n",
      "==== EXPERT MODE ENABLE BC: False ENABLE RL: False FULL TRAIN: False ====\n",
      "==== low level action taken: 7 N_pickable_item: 3 ====\n",
      "[SUCCESS] get raw data\n",
      "Qx: tensor([[0.9096, 0.9229, 0.8390]], device='cuda:0')\n",
      "Qy: tensor([[0.8255, 0.9326, 0.8810]], device='cuda:0')\n",
      "Qz: tensor([[0.9622, 0.7666, 0.7309]], device='cuda:0')\n",
      "Qyaw: tensor([[0.8393, 0.9721, 0.8130]], device='cuda:0')\n",
      "[MAX OUTPUT]\n",
      "[Q net] x: 1, y: 1, z: 0, yaw_ind: 1\n",
      "[SUCCESS] estimate actions from network\n",
      "[GRIPPER STATUS] FULL CLOSE\n",
      "[GRIPPER STATUS] FULL CLOSE\n",
      "gripper tip height: 0.043439547221349795\n",
      "[PUSH REWARD] R: 1.0\n",
      "[OVERALL REWARD] 1.0\n",
      "[ACTION TYPE]: 1\n",
      "[MOVE] move: [ 0.    0.   -0.02  0.  ] move index: [1, 1, 0, 1]\n",
      "next move: tensor([ 0.0000,  0.0000, -0.0200,  0.0000], device='cuda:0') next move index: [1, 1, 0, 1]\n",
      "target_Qx: 1.0 Qx: 0.9229246973991394\n",
      "target_Qy: 1.0 Qy: 0.9325509667396545\n",
      "target_Qz: 1.0 Qz: 0.9622334241867065\n",
      "target_Qyaw: 1.0 Qyaw: 0.9721457362174988\n",
      "loss_rank: 0.012602642178535461\n",
      "priority: 0.01577567867934704\n",
      "[SUCCESS] append transition experience\n",
      "[SUCCESS] perform push action\n",
      "=== end of action ===\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[SUCCESS] return home\n",
      "[SUCCESS] return home\n",
      "[SUCCESS] GRASP OR PUSH ACTION\n",
      "[PUSH SUCCESS RATE] 1.2%/-inf% [500]\n",
      "[SUCCESS] save agent data\n",
      "==== episode: 0 ====\n",
      "[MIN_DISTANCE] 0.030395810375225985\n",
      "[compute_push_path]: 0.05000295694148177\n",
      "N_step_x: 2.0, N_step_y: 8.0, N_step_z: 10.0, N_step_yaw: 5.0, N_step: 10.0\n",
      "[MIN_DISTANCE] 0.030395810375225985\n",
      "[MIN_DISTANCE] 0.03939406288459876\n",
      "N_step_x: 6.0, N_step_y: 12.0, N_step_z: 10.0, N_step_yaw: 6.0, N_step: 12.0\n",
      "N_step_low_level: 14\n",
      "==== EXPERT MODE ENABLE BC: False ENABLE RL: False FULL TRAIN: False ====\n",
      "==== low level action taken: 8 N_pickable_item: 3 ====\n",
      "[SUCCESS] get raw data\n",
      "Qx: tensor([[0.4463, 0.3535, 0.4950]], device='cuda:0')\n",
      "Qy: tensor([[0.3475, 0.1792, 0.4200]], device='cuda:0')\n",
      "Qz: tensor([[0.5552, 0.1857, 0.1880]], device='cuda:0')\n",
      "Qyaw: tensor([[0.1952, 0.2501, 0.3182]], device='cuda:0')\n",
      "[MAX OUTPUT]\n",
      "[Q net] x: 2, y: 2, z: 0, yaw_ind: 2\n",
      "[SUCCESS] estimate actions from network\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[GRASP DISTANCE] 0.0\n",
      "[GRASP REWARD] R: 0.0\n",
      "[OVERALL REWARD] 0.0\n",
      "[ACTION TYPE]: 0\n",
      "[MOVE] move: [-0.015      0.015     -0.02       0.1308997] move index: [0, 2, 0, 2]\n",
      "next move: tensor([ 0.0000,  0.0150, -0.0200,  0.0000], device='cuda:0') next move index: [1, 2, 0, 1]\n",
      "target_Qx: 0.4526229798793793 Qx: 0.4462796747684479\n",
      "target_Qy: 0.47195470333099365 Qy: 0.4200228452682495\n",
      "target_Qz: 0.5269175171852112 Qz: 0.5552467107772827\n",
      "target_Qyaw: 0.2524014711380005 Qyaw: 0.31817740201950073\n",
      "loss_rank: 0.017949407920241356\n",
      "priority: 0.019915951415896416\n",
      "[SUCCESS] append transition experience\n",
      "==== EXPERT MODE ENABLE BC: False ENABLE RL: False FULL TRAIN: False ====\n",
      "==== low level action taken: 8 N_pickable_item: 3 ====\n",
      "[SUCCESS] get raw data\n",
      "Qx: tensor([[0.3656, 0.4982, 0.4400]], device='cuda:0')\n",
      "Qy: tensor([[0.3228, 0.4345, 0.4603]], device='cuda:0')\n",
      "Qz: tensor([[0.5697, 0.2699, 0.2267]], device='cuda:0')\n",
      "Qyaw: tensor([[0.2438, 0.2603, 0.2151]], device='cuda:0')\n",
      "[MAX OUTPUT]\n",
      "[Q net] x: 1, y: 2, z: 0, yaw_ind: 1\n",
      "[SUCCESS] estimate actions from network\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[GRASP DISTANCE] 0.0\n",
      "[GRASP REWARD] R: 0.0\n",
      "[OVERALL REWARD] 0.0\n",
      "[ACTION TYPE]: 0\n",
      "[MOVE] move: [ 0.     0.015 -0.02   0.   ] move index: [1, 2, 0, 1]\n",
      "next move: tensor([-0.0150,  0.0150, -0.0200,  0.1309], device='cuda:0') next move index: [0, 2, 0, 2]\n",
      "target_Qx: 0.43139979243278503 Qx: 0.498183935880661\n",
      "target_Qy: 0.4537338316440582 Qy: 0.4602711796760559\n",
      "target_Qz: 0.5401575565338135 Qz: 0.5696589946746826\n",
      "target_Qyaw: 0.26373490691185 Qyaw: 0.2602621912956238\n",
      "loss_rank: 0.021194476634263992\n",
      "priority: 0.02254078909754753\n",
      "[SUCCESS] append transition experience\n",
      "==== EXPERT MODE ENABLE BC: False ENABLE RL: False FULL TRAIN: False ====\n",
      "==== low level action taken: 8 N_pickable_item: 3 ====\n",
      "[SUCCESS] get raw data\n",
      "Qx: tensor([[0.4452, 0.5133, 0.4743]], device='cuda:0')\n",
      "Qy: tensor([[0.3579, 0.4169, 0.4443]], device='cuda:0')\n",
      "Qz: tensor([[0.5767, 0.4106, 0.3635]], device='cuda:0')\n",
      "Qyaw: tensor([[0.2879, 0.3676, 0.2897]], device='cuda:0')\n",
      "[RANDOM OUTPUT]\n",
      "[Q net] x: 0, y: 2, z: 2, yaw_ind: 1\n",
      "[SUCCESS] estimate actions from network\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[GRASP DISTANCE] 0.0\n",
      "[GRASP REWARD] R: 0.0\n",
      "[OVERALL REWARD] 0.0\n",
      "[ACTION TYPE]: 0\n",
      "[MOVE] move: [-0.015      0.015     -0.02       0.1308997] move index: [0, 2, 0, 2]\n",
      "next move: tensor([0.0000, 0.0150, 0.0000, 0.0000], device='cuda:0') next move index: [1, 2, 1, 1]\n",
      "target_Qx: 0.46615535020828247 Qx: 0.4451737105846405\n",
      "target_Qy: 0.5003320574760437 Qy: 0.44429340958595276\n",
      "target_Qz: 0.5314696431159973 Qz: 0.5766966342926025\n",
      "target_Qyaw: 0.4914536774158478 Qyaw: 0.28969842195510864\n",
      "loss_rank: 0.054971568286418915\n",
      "priority: 0.06655437499284744\n",
      "[SUCCESS] append transition experience\n",
      "==== EXPERT MODE ENABLE BC: False ENABLE RL: False FULL TRAIN: False ====\n",
      "==== low level action taken: 8 N_pickable_item: 3 ====\n",
      "[SUCCESS] get raw data\n",
      "Qx: tensor([[0.5062, 0.4792, 0.5008]], device='cuda:0')\n",
      "Qy: tensor([[0.4672, 0.4660, 0.4661]], device='cuda:0')\n",
      "Qz: tensor([[0.6592, 0.5147, 0.4597]], device='cuda:0')\n",
      "Qyaw: tensor([[0.3751, 0.5218, 0.3955]], device='cuda:0')\n",
      "[MAX OUTPUT]\n",
      "[Q net] x: 0, y: 0, z: 0, yaw_ind: 1\n",
      "[SUCCESS] estimate actions from network\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[GRASP DISTANCE] 0.0\n",
      "[GRASP REWARD] R: 0.0\n",
      "[OVERALL REWARD] 0.0\n",
      "[ACTION TYPE]: 0\n",
      "[MOVE] move: [0.    0.015 0.    0.   ] move index: [1, 2, 1, 1]\n",
      "next move: tensor([-0.0150,  0.0150, -0.0200,  0.1309], device='cuda:0') next move index: [0, 2, 0, 2]\n",
      "target_Qx: 0.5590075254440308 Qx: 0.47915297746658325\n",
      "target_Qy: 0.5695326328277588 Qy: 0.4661339521408081\n",
      "target_Qz: 0.6520232558250427 Qz: 0.5146613717079163\n",
      "target_Qyaw: 0.4251258969306946 Qyaw: 0.52177494764328\n",
      "loss_rank: 0.06159117445349693\n",
      "priority: 0.07291051745414734\n",
      "[SUCCESS] append transition experience\n",
      "==== EXPERT MODE ENABLE BC: False ENABLE RL: False FULL TRAIN: False ====\n",
      "==== low level action taken: 8 N_pickable_item: 3 ====\n",
      "[SUCCESS] get raw data\n",
      "Qx: tensor([[0.5636, 0.6042, 0.4799]], device='cuda:0')\n",
      "Qy: tensor([[0.5008, 0.4947, 0.5441]], device='cuda:0')\n",
      "Qz: tensor([[0.6735, 0.4593, 0.4959]], device='cuda:0')\n",
      "Qyaw: tensor([[0.3691, 0.4388, 0.4377]], device='cuda:0')\n",
      "[MAX OUTPUT]\n",
      "[Q net] x: 1, y: 2, z: 0, yaw_ind: 1\n",
      "[SUCCESS] estimate actions from network\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[GRASP DISTANCE] 0.0\n",
      "[GRASP REWARD] R: 0.0\n",
      "[OVERALL REWARD] 0.0\n",
      "[ACTION TYPE]: 0\n",
      "[MOVE] move: [-0.015      0.015     -0.02       0.1308997] move index: [0, 2, 0, 2]\n",
      "next move: tensor([ 0.0000,  0.0150, -0.0200,  0.0000], device='cuda:0') next move index: [1, 2, 0, 1]\n",
      "target_Qx: 0.5566790699958801 Qx: 0.5636494159698486\n",
      "target_Qy: 0.5812205076217651 Qy: 0.5440753698348999\n",
      "target_Qz: 0.6590346097946167 Qz: 0.6734872460365295\n",
      "target_Qyaw: 0.5047574043273926 Qyaw: 0.4377365708351135\n",
      "loss_rank: 0.03305353596806526\n",
      "priority: 0.03458578884601593\n",
      "[SUCCESS] append transition experience\n",
      "==== EXPERT MODE ENABLE BC: False ENABLE RL: False FULL TRAIN: False ====\n",
      "==== low level action taken: 8 N_pickable_item: 3 ====\n",
      "[SUCCESS] get raw data\n",
      "Qx: tensor([[0.5382, 0.4878, 0.5297]], device='cuda:0')\n",
      "Qy: tensor([[0.3572, 0.4756, 0.5588]], device='cuda:0')\n",
      "Qz: tensor([[0.6731, 0.4729, 0.4061]], device='cuda:0')\n",
      "Qyaw: tensor([[0.3634, 0.5126, 0.4151]], device='cuda:0')\n",
      "[MAX OUTPUT]\n",
      "[Q net] x: 0, y: 2, z: 0, yaw_ind: 1\n",
      "[SUCCESS] estimate actions from network\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[GRASP DISTANCE] 0.0\n",
      "[GRASP REWARD] R: 0.0\n",
      "[OVERALL REWARD] 0.0\n",
      "[ACTION TYPE]: 0\n",
      "[MOVE] move: [ 0.     0.015 -0.02   0.   ] move index: [1, 2, 0, 1]\n",
      "next move: tensor([ 0.0000,  0.0150, -0.0200,  0.0000], device='cuda:0') next move index: [1, 2, 0, 1]\n",
      "target_Qx: 0.5509658455848694 Qx: 0.48777058720588684\n",
      "target_Qy: 0.6376786231994629 Qy: 0.5587604641914368\n",
      "target_Qz: 0.6970540285110474 Qz: 0.6730673313140869\n",
      "target_Qyaw: 0.5617099404335022 Qyaw: 0.5125787854194641\n",
      "loss_rank: 0.02597714774310589\n",
      "priority: 0.029279883950948715\n",
      "[SUCCESS] append transition experience\n",
      "==== EXPERT MODE ENABLE BC: False ENABLE RL: False FULL TRAIN: False ====\n",
      "==== low level action taken: 8 N_pickable_item: 3 ====\n",
      "[SUCCESS] get raw data\n",
      "Qx: tensor([[0.5629, 0.5647, 0.5885]], device='cuda:0')\n",
      "Qy: tensor([[0.3684, 0.5695, 0.6477]], device='cuda:0')\n",
      "Qz: tensor([[0.7085, 0.5113, 0.5069]], device='cuda:0')\n",
      "Qyaw: tensor([[0.4190, 0.5714, 0.5261]], device='cuda:0')\n",
      "[MAX OUTPUT]\n",
      "[Q net] x: 2, y: 2, z: 0, yaw_ind: 1\n",
      "[SUCCESS] estimate actions from network\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[GRASP DISTANCE] 0.0\n",
      "[GRASP REWARD] R: 0.0\n",
      "[OVERALL REWARD] 0.0\n",
      "[ACTION TYPE]: 0\n",
      "[MOVE] move: [ 0.     0.015 -0.02   0.   ] move index: [1, 2, 0, 1]\n",
      "next move: tensor([-0.0150,  0.0150,  0.0000,  0.1309], device='cuda:0') next move index: [0, 2, 1, 2]\n",
      "target_Qx: 0.6561037302017212 Qx: 0.5647163391113281\n",
      "target_Qy: 0.6421438455581665 Qy: 0.6477203369140625\n",
      "target_Qz: 0.5337181091308594 Qz: 0.7085383534431458\n",
      "target_Qyaw: 0.5078845024108887 Qyaw: 0.57136070728302\n",
      "loss_rank: 0.024865074083209038\n",
      "priority: 0.035608597099781036\n",
      "[SUCCESS] append transition experience\n",
      "==== EXPERT MODE ENABLE BC: False ENABLE RL: False FULL TRAIN: False ====\n",
      "==== low level action taken: 8 N_pickable_item: 3 ====\n",
      "[SUCCESS] get raw data\n",
      "Qx: tensor([[0.6487, 0.5029, 0.6537]], device='cuda:0')\n",
      "Qy: tensor([[0.4663, 0.3910, 0.6396]], device='cuda:0')\n",
      "Qz: tensor([[0.6214, 0.5454, 0.5151]], device='cuda:0')\n",
      "Qyaw: tensor([[0.4619, 0.5340, 0.4521]], device='cuda:0')\n",
      "[RANDOM OUTPUT]\n",
      "[Q net] x: 0, y: 0, z: 1, yaw_ind: 1\n",
      "[SUCCESS] estimate actions from network\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[GRASP DISTANCE] 0.0\n",
      "[GRASP REWARD] R: 0.0\n",
      "[OVERALL REWARD] 0.0\n",
      "[ACTION TYPE]: 0\n",
      "[MOVE] move: [-0.015      0.015      0.         0.1308997] move index: [0, 2, 1, 2]\n",
      "next move: tensor([ 0.0000,  0.0150, -0.0200,  0.0000], device='cuda:0') next move index: [1, 2, 0, 1]\n",
      "target_Qx: 0.6989118456840515 Qx: 0.6487381458282471\n",
      "target_Qy: 0.7320763468742371 Qy: 0.6396249532699585\n",
      "target_Qz: 0.7836828231811523 Qz: 0.5454124808311462\n",
      "target_Qyaw: 0.7331794500350952 Qyaw: 0.4520900249481201\n",
      "loss_rank: 0.053534358739852905\n",
      "priority: 0.0902465283870697\n",
      "[SUCCESS] append transition experience\n",
      "==== EXPERT MODE ENABLE BC: False ENABLE RL: False FULL TRAIN: False ====\n",
      "==== low level action taken: 8 N_pickable_item: 3 ====\n",
      "[SUCCESS] get raw data\n",
      "Qx: tensor([[0.7528, 0.6641, 0.5817]], device='cuda:0')\n",
      "Qy: tensor([[0.6495, 0.5750, 0.7287]], device='cuda:0')\n",
      "Qz: tensor([[0.8196, 0.3979, 0.4330]], device='cuda:0')\n",
      "Qyaw: tensor([[0.4711, 0.7111, 0.4567]], device='cuda:0')\n",
      "[MAX OUTPUT]\n",
      "[Q net] x: 0, y: 2, z: 0, yaw_ind: 1\n",
      "[SUCCESS] estimate actions from network\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[GRASP DISTANCE] 0.0\n",
      "[GRASP REWARD] R: 0.0\n",
      "[OVERALL REWARD] 0.0\n",
      "[ACTION TYPE]: 0\n",
      "[MOVE] move: [ 0.     0.015 -0.02   0.   ] move index: [1, 2, 0, 1]\n",
      "next move: tensor([-0.0150,  0.0150, -0.0200,  0.1309], device='cuda:0') next move index: [0, 2, 0, 2]\n",
      "target_Qx: 0.8527974486351013 Qx: 0.6641349196434021\n",
      "target_Qy: 0.7966622114181519 Qy: 0.72868812084198\n",
      "target_Qz: 0.8644383549690247 Qz: 0.819633960723877\n",
      "target_Qyaw: 0.6197984218597412 Qyaw: 0.7110978364944458\n",
      "loss_rank: 0.018921051174402237\n",
      "priority: 0.03156031295657158\n",
      "[SUCCESS] append transition experience\n",
      "==== EXPERT MODE ENABLE BC: False ENABLE RL: False FULL TRAIN: False ====\n",
      "==== low level action taken: 8 N_pickable_item: 3 ====\n",
      "[SUCCESS] get raw data\n",
      "Qx: tensor([[0.8488, 0.7159, 0.6897]], device='cuda:0')\n",
      "Qy: tensor([[0.5989, 0.7305, 0.7918]], device='cuda:0')\n",
      "Qz: tensor([[0.8590, 0.6903, 0.5640]], device='cuda:0')\n",
      "Qyaw: tensor([[0.5201, 0.8632, 0.5983]], device='cuda:0')\n",
      "[RANDOM OUTPUT]\n",
      "[Q net] x: 1, y: 2, z: 2, yaw_ind: 2\n",
      "[SUCCESS] estimate actions from network\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[GRASP DISTANCE] 0.0\n",
      "[GRASP REWARD] R: 0.0\n",
      "[OVERALL REWARD] 0.0\n",
      "[ACTION TYPE]: 0\n",
      "[MOVE] move: [-0.015      0.015     -0.02       0.1308997] move index: [0, 2, 0, 2]\n",
      "next move: tensor([ 0.0000,  0.0150, -0.0200,  0.0000], device='cuda:0') next move index: [1, 2, 0, 1]\n",
      "target_Qx: 0.8642759919166565 Qx: 0.848808765411377\n",
      "target_Qy: 0.8346262574195862 Qy: 0.791776716709137\n",
      "target_Qz: 0.8519753813743591 Qz: 0.859011709690094\n",
      "target_Qyaw: 0.8613056540489197 Qyaw: 0.5983203649520874\n",
      "loss_rank: 0.0354490727186203\n",
      "priority: 0.05327059328556061\n",
      "[SUCCESS] append transition experience\n",
      "==== EXPERT MODE ENABLE BC: False ENABLE RL: False FULL TRAIN: False ====\n",
      "==== low level action taken: 8 N_pickable_item: 3 ====\n",
      "[SUCCESS] get raw data\n",
      "Qx: tensor([[0.7184, 0.8134, 0.4637]], device='cuda:0')\n",
      "Qy: tensor([[0.6604, 0.7811, 0.7726]], device='cuda:0')\n",
      "Qz: tensor([[0.8286, 0.7199, 0.6787]], device='cuda:0')\n",
      "Qyaw: tensor([[0.5039, 0.8248, 0.6585]], device='cuda:0')\n",
      "[MAX OUTPUT]\n",
      "[Q net] x: 1, y: 1, z: 0, yaw_ind: 1\n",
      "[SUCCESS] estimate actions from network\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[GRASP DISTANCE] 0.0\n",
      "[GRASP REWARD] R: 0.0\n",
      "[OVERALL REWARD] 0.0\n",
      "[ACTION TYPE]: 0\n",
      "[MOVE] move: [ 0.     0.015 -0.02   0.   ] move index: [1, 2, 0, 1]\n",
      "next move: tensor([ 0.0000,  0.0000, -0.0200,  0.0000], device='cuda:0') next move index: [1, 1, 0, 1]\n",
      "target_Qx: 0.9249054193496704 Qx: 0.8133748173713684\n",
      "target_Qy: 0.9230168461799622 Qy: 0.7725719809532166\n",
      "target_Qz: 0.8898118734359741 Qz: 0.8285656571388245\n",
      "target_Qyaw: 0.8642579913139343 Qyaw: 0.8247935771942139\n",
      "loss_rank: 0.009464602917432785\n",
      "priority: 0.019559919834136963\n",
      "[SUCCESS] append transition experience\n",
      "==== EXPERT MODE ENABLE BC: False ENABLE RL: False FULL TRAIN: False ====\n",
      "==== low level action taken: 8 N_pickable_item: 3 ====\n",
      "[SUCCESS] get raw data\n",
      "Qx: tensor([[0.8615, 0.9186, 0.7339]], device='cuda:0')\n",
      "Qy: tensor([[0.7460, 0.9155, 0.6067]], device='cuda:0')\n",
      "Qz: tensor([[0.8926, 0.7862, 0.7794]], device='cuda:0')\n",
      "Qyaw: tensor([[0.6982, 0.8683, 0.8700]], device='cuda:0')\n",
      "[MAX OUTPUT]\n",
      "[Q net] x: 1, y: 1, z: 0, yaw_ind: 2\n",
      "[SUCCESS] estimate actions from network\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[GRASP DISTANCE] 0.0\n",
      "[GRASP REWARD] R: 0.0\n",
      "[OVERALL REWARD] 0.0\n",
      "[ACTION TYPE]: 0\n",
      "[MOVE] move: [ 0.    0.   -0.02  0.  ] move index: [1, 1, 0, 1]\n",
      "next move: tensor([ 0.0000,  0.0000, -0.0200,  0.0000], device='cuda:0') next move index: [1, 1, 0, 1]\n",
      "target_Qx: 0.9605564475059509 Qx: 0.9186207056045532\n",
      "target_Qy: 0.9613792300224304 Qy: 0.9155192375183105\n",
      "target_Qz: 0.9354081749916077 Qz: 0.8925843834877014\n",
      "target_Qyaw: 0.9727557897567749 Qyaw: 0.8683329820632935\n",
      "loss_rank: 0.012043562717735767\n",
      "priority: 0.01619349792599678\n",
      "[SUCCESS] append transition experience\n",
      "==== EXPERT MODE ENABLE BC: False ENABLE RL: False FULL TRAIN: False ====\n",
      "==== low level action taken: 8 N_pickable_item: 3 ====\n",
      "[SUCCESS] get raw data\n",
      "Qx: tensor([[0.9382, 0.9600, 0.7426]], device='cuda:0')\n",
      "Qy: tensor([[0.8857, 0.9616, 0.7077]], device='cuda:0')\n",
      "Qz: tensor([[0.9401, 0.8415, 0.8252]], device='cuda:0')\n",
      "Qyaw: tensor([[0.8023, 0.9924, 0.9341]], device='cuda:0')\n",
      "[MAX OUTPUT]\n",
      "[Q net] x: 1, y: 1, z: 0, yaw_ind: 1\n",
      "[SUCCESS] estimate actions from network\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[GRASP] grasp something\n",
      "[GRIPPER STATUS] NON_CLOSE_NON_OPEN\n",
      "[GRASP DISTANCE] 0.0\n",
      "[SUCESS] picked an item!\n",
      "[GRASP REWARD] R: 1.0\n",
      "[OVERALL REWARD] 1.0\n",
      "[ACTION TYPE]: 0\n",
      "[MOVE] move: [ 0.    0.   -0.02  0.  ] move index: [1, 1, 0, 1]\n",
      "next move: tensor([ 0.0000,  0.0000, -0.0200,  0.0000], device='cuda:0') next move index: [1, 1, 0, 1]\n",
      "target_Qx: 1.0 Qx: 0.9599553942680359\n",
      "target_Qy: 1.0 Qy: 0.9616289138793945\n",
      "target_Qz: 1.0 Qz: 0.9401323795318604\n",
      "target_Qyaw: 1.0 Qyaw: 0.9923546314239502\n",
      "loss_rank: 0.012113166972994804\n",
      "priority: 0.013792790472507477\n",
      "[SUCCESS] append transition experience\n",
      "[SUCCESS] grasp an item successfully\n",
      "=== end of action ===\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[SUCCESS] return home\n",
      "[SUCCESS] return home\n",
      "[SUCCESS] GRASP OR PUSH ACTION\n",
      "[GRASP SUCCESS RATE] 0.6%/-inf% [500]\n",
      "[SUCCESS] save agent data\n",
      "==== episode: 0 ====\n",
      "[MIN_DISTANCE] 0.030463691521437658\n",
      "[compute_push_path]: 0.05000295694148177\n",
      "N_step_x: 2.0, N_step_y: 8.0, N_step_z: 10.0, N_step_yaw: 5.0, N_step: 10.0\n",
      "[MIN_DISTANCE] 0.030463691521437658\n",
      "N_step_low_level: 12\n",
      "==== EXPERT MODE ENABLE BC: False ENABLE RL: False FULL TRAIN: False ====\n",
      "==== low level action taken: 9 N_pickable_item: 2 ====\n",
      "[SUCCESS] get raw data\n",
      "Qx: tensor([[0.3006, 0.3772, 0.4328]], device='cuda:0')\n",
      "Qy: tensor([[0.4312, 0.2991, 0.4663]], device='cuda:0')\n",
      "Qz: tensor([[0.4424, 0.2945, 0.2831]], device='cuda:0')\n",
      "Qyaw: tensor([[0.4313, 0.3471, 0.2519]], device='cuda:0')\n",
      "[MAX OUTPUT]\n",
      "[Q net] x: 2, y: 2, z: 0, yaw_ind: 0\n",
      "[SUCCESS] estimate actions from network\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "gripper tip height: 0.2501886180981726\n",
      "[PUSH REWARD] R: 0.0\n",
      "[OVERALL REWARD] 0.0\n",
      "[ACTION TYPE]: 1\n",
      "[MOVE] move: [ 0.015      0.015     -0.02      -0.2617994] move index: [2, 2, 0, 0]\n",
      "next move: tensor([ 0.0000,  0.0150, -0.0200,  0.0000], device='cuda:0') next move index: [1, 2, 0, 1]\n",
      "target_Qx: 0.36221233010292053 Qx: 0.43279919028282166\n",
      "target_Qy: 0.3755961060523987 Qy: 0.46625828742980957\n",
      "target_Qz: 0.4039124846458435 Qz: 0.4424152076244354\n",
      "target_Qyaw: 0.34045976400375366 Qyaw: 0.4312959313392639\n",
      "loss_rank: 0.010429409332573414\n",
      "priority: 0.016163360327482224\n",
      "[SUCCESS] append transition experience\n",
      "==== EXPERT MODE ENABLE BC: False ENABLE RL: False FULL TRAIN: False ====\n",
      "==== low level action taken: 9 N_pickable_item: 2 ====\n",
      "[SUCCESS] get raw data\n",
      "Qx: tensor([[0.3848, 0.3832, 0.2973]], device='cuda:0')\n",
      "Qy: tensor([[0.3147, 0.3584, 0.3874]], device='cuda:0')\n",
      "Qz: tensor([[0.4200, 0.3187, 0.3311]], device='cuda:0')\n",
      "Qyaw: tensor([[0.4095, 0.3665, 0.2857]], device='cuda:0')\n",
      "[MAX OUTPUT]\n",
      "[Q net] x: 0, y: 2, z: 0, yaw_ind: 0\n",
      "[SUCCESS] estimate actions from network\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "gripper tip height: 0.22856627416484865\n",
      "[PUSH REWARD] R: 0.0\n",
      "[OVERALL REWARD] 0.0\n",
      "[ACTION TYPE]: 1\n",
      "[MOVE] move: [ 0.     0.015 -0.02   0.   ] move index: [1, 2, 0, 1]\n",
      "next move: tensor([ 0.0000,  0.0150, -0.0200, -0.2618], device='cuda:0') next move index: [1, 2, 0, 0]\n",
      "target_Qx: 0.4019259512424469 Qx: 0.3831503093242645\n",
      "target_Qy: 0.4946753978729248 Qy: 0.387423574924469\n",
      "target_Qz: 0.4823821187019348 Qz: 0.41999000310897827\n",
      "target_Qyaw: 0.4840657711029053 Qyaw: 0.3665241003036499\n",
      "loss_rank: 0.032274726778268814\n",
      "priority: 0.03966580331325531\n",
      "[SUCCESS] append transition experience\n",
      "==== EXPERT MODE ENABLE BC: False ENABLE RL: False FULL TRAIN: False ====\n",
      "==== low level action taken: 9 N_pickable_item: 2 ====\n",
      "[SUCCESS] get raw data\n",
      "Qx: tensor([[0.4367, 0.4210, 0.4422]], device='cuda:0')\n",
      "Qy: tensor([[0.4592, 0.3786, 0.5267]], device='cuda:0')\n",
      "Qz: tensor([[0.4982, 0.4059, 0.4230]], device='cuda:0')\n",
      "Qyaw: tensor([[0.5123, 0.4222, 0.3794]], device='cuda:0')\n",
      "[MAX OUTPUT]\n",
      "[Q net] x: 2, y: 2, z: 0, yaw_ind: 0\n",
      "[SUCCESS] estimate actions from network\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "gripper tip height: 0.2089702295288223\n",
      "[PUSH REWARD] R: 0.0\n",
      "[OVERALL REWARD] 0.0\n",
      "[ACTION TYPE]: 1\n",
      "[MOVE] move: [ 0.         0.015     -0.02      -0.2617994] move index: [1, 2, 0, 0]\n",
      "next move: tensor([ 0.0000,  0.0000, -0.0200,  0.0000], device='cuda:0') next move index: [1, 1, 0, 1]\n",
      "target_Qx: 0.46808451414108276 Qx: 0.4210034906864166\n",
      "target_Qy: 0.48040053248405457 Qy: 0.5266725420951843\n",
      "target_Qz: 0.4925956130027771 Qz: 0.49817168712615967\n",
      "target_Qyaw: 0.5163599848747253 Qyaw: 0.5123112201690674\n",
      "loss_rank: 0.0259923804551363\n",
      "priority: 0.027093682438135147\n",
      "[SUCCESS] append transition experience\n",
      "==== EXPERT MODE ENABLE BC: False ENABLE RL: False FULL TRAIN: False ====\n",
      "==== low level action taken: 9 N_pickable_item: 2 ====\n",
      "[SUCCESS] get raw data\n",
      "Qx: tensor([[0.4097, 0.4613, 0.3764]], device='cuda:0')\n",
      "Qy: tensor([[0.4250, 0.4829, 0.5082]], device='cuda:0')\n",
      "Qz: tensor([[0.5077, 0.4073, 0.4454]], device='cuda:0')\n",
      "Qyaw: tensor([[0.5296, 0.5464, 0.4687]], device='cuda:0')\n",
      "[RANDOM OUTPUT]\n",
      "[Q net] x: 0, y: 1, z: 0, yaw_ind: 2\n",
      "[SUCCESS] estimate actions from network\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "gripper tip height: 0.1878901242413859\n",
      "[PUSH REWARD] R: 0.0\n",
      "[OVERALL REWARD] 0.0\n",
      "[ACTION TYPE]: 1\n",
      "[MOVE] move: [ 0.    0.   -0.02  0.  ] move index: [1, 1, 0, 1]\n",
      "next move: tensor([ 0.0000,  0.0150, -0.0200,  0.0000], device='cuda:0') next move index: [1, 2, 0, 1]\n",
      "target_Qx: 0.6314236521720886 Qx: 0.46133875846862793\n",
      "target_Qy: 0.6536616683006287 Qy: 0.48290157318115234\n",
      "target_Qz: 0.6312670707702637 Qz: 0.5076912641525269\n",
      "target_Qyaw: 0.6345271468162537 Qyaw: 0.5463978052139282\n",
      "loss_rank: 0.03117804229259491\n",
      "priority: 0.051459453999996185\n",
      "[SUCCESS] append transition experience\n",
      "==== EXPERT MODE ENABLE BC: False ENABLE RL: False FULL TRAIN: False ====\n",
      "==== low level action taken: 9 N_pickable_item: 2 ====\n",
      "[SUCCESS] get raw data\n",
      "Qx: tensor([[0.4979, 0.5907, 0.4977]], device='cuda:0')\n",
      "Qy: tensor([[0.5252, 0.5808, 0.6223]], device='cuda:0')\n",
      "Qz: tensor([[0.6027, 0.5108, 0.4662]], device='cuda:0')\n",
      "Qyaw: tensor([[0.5507, 0.5971, 0.5216]], device='cuda:0')\n",
      "[MAX OUTPUT]\n",
      "[Q net] x: 1, y: 2, z: 0, yaw_ind: 1\n",
      "[SUCCESS] estimate actions from network\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "gripper tip height: 0.16755094802417314\n",
      "[PUSH REWARD] R: 0.0\n",
      "[OVERALL REWARD] 0.0\n",
      "[ACTION TYPE]: 1\n",
      "[MOVE] move: [ 0.     0.015 -0.02   0.   ] move index: [1, 2, 0, 1]\n",
      "next move: tensor([ 0.0000,  0.0150, -0.0200, -0.2618], device='cuda:0') next move index: [1, 2, 0, 0]\n",
      "target_Qx: 0.7197337746620178 Qx: 0.5907057523727417\n",
      "target_Qy: 0.7514540553092957 Qy: 0.6223392486572266\n",
      "target_Qz: 0.776546061038971 Qz: 0.6027246117591858\n",
      "target_Qyaw: 0.7558906674385071 Qyaw: 0.5971260070800781\n",
      "loss_rank: 0.01347278244793415\n",
      "priority: 0.035657525062561035\n",
      "[SUCCESS] append transition experience\n",
      "==== EXPERT MODE ENABLE BC: False ENABLE RL: False FULL TRAIN: False ====\n",
      "==== low level action taken: 9 N_pickable_item: 2 ====\n",
      "[SUCCESS] get raw data\n",
      "Qx: tensor([[0.6169, 0.7011, 0.6916]], device='cuda:0')\n",
      "Qy: tensor([[0.6392, 0.7907, 0.7332]], device='cuda:0')\n",
      "Qz: tensor([[0.7597, 0.6031, 0.6627]], device='cuda:0')\n",
      "Qyaw: tensor([[0.7568, 0.7087, 0.6145]], device='cuda:0')\n",
      "[MAX OUTPUT]\n",
      "[Q net] x: 1, y: 1, z: 0, yaw_ind: 0\n",
      "[SUCCESS] estimate actions from network\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "gripper tip height: 0.14778496448907896\n",
      "[PUSH REWARD] R: 0.0\n",
      "[OVERALL REWARD] 0.0\n",
      "[ACTION TYPE]: 1\n",
      "[MOVE] move: [ 0.         0.015     -0.02      -0.2617994] move index: [1, 2, 0, 0]\n",
      "next move: tensor([ 0.0000,  0.0000, -0.0200,  0.0000], device='cuda:0') next move index: [1, 1, 0, 1]\n",
      "target_Qx: 0.7704311609268188 Qx: 0.7010651230812073\n",
      "target_Qy: 0.7117760181427002 Qy: 0.7332265973091125\n",
      "target_Qz: 0.7797370553016663 Qz: 0.759713351726532\n",
      "target_Qyaw: 0.7077116370201111 Qyaw: 0.7568460702896118\n",
      "loss_rank: 0.02704778127372265\n",
      "priority: 0.029069509357213974\n",
      "[SUCCESS] append transition experience\n",
      "==== EXPERT MODE ENABLE BC: False ENABLE RL: False FULL TRAIN: False ====\n",
      "==== low level action taken: 9 N_pickable_item: 2 ====\n",
      "[SUCCESS] get raw data\n",
      "Qx: tensor([[0.6202, 0.7727, 0.7258]], device='cuda:0')\n",
      "Qy: tensor([[0.6288, 0.6976, 0.7669]], device='cuda:0')\n",
      "Qz: tensor([[0.7716, 0.6361, 0.5917]], device='cuda:0')\n",
      "Qyaw: tensor([[0.7242, 0.7013, 0.5795]], device='cuda:0')\n",
      "[MAX OUTPUT]\n",
      "[Q net] x: 1, y: 2, z: 0, yaw_ind: 0\n",
      "[SUCCESS] estimate actions from network\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "gripper tip height: 0.12703586848215237\n",
      "[PUSH REWARD] R: 0.0\n",
      "[OVERALL REWARD] 0.0\n",
      "[ACTION TYPE]: 1\n",
      "[MOVE] move: [ 0.    0.   -0.02  0.  ] move index: [1, 1, 0, 1]\n",
      "next move: tensor([ 0.0000,  0.0150, -0.0200, -0.2618], device='cuda:0') next move index: [1, 2, 0, 0]\n",
      "target_Qx: 0.8593990802764893 Qx: 0.772737443447113\n",
      "target_Qy: 0.8582478761672974 Qy: 0.6976170539855957\n",
      "target_Qz: 0.846065104007721 Qz: 0.7715834379196167\n",
      "target_Qyaw: 0.8285278081893921 Qyaw: 0.7012670040130615\n",
      "loss_rank: 0.031359393149614334\n",
      "priority: 0.045123226940631866\n",
      "[SUCCESS] append transition experience\n",
      "==== EXPERT MODE ENABLE BC: False ENABLE RL: False FULL TRAIN: False ====\n",
      "==== low level action taken: 9 N_pickable_item: 2 ====\n",
      "[SUCCESS] get raw data\n",
      "Qx: tensor([[0.6657, 0.8568, 0.7552]], device='cuda:0')\n",
      "Qy: tensor([[0.7112, 0.7609, 0.8545]], device='cuda:0')\n",
      "Qz: tensor([[0.8347, 0.7190, 0.6470]], device='cuda:0')\n",
      "Qyaw: tensor([[0.8057, 0.7940, 0.6562]], device='cuda:0')\n",
      "[MAX OUTPUT]\n",
      "[Q net] x: 1, y: 2, z: 0, yaw_ind: 0\n",
      "[SUCCESS] estimate actions from network\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "gripper tip height: 0.1056104262951762\n",
      "[PUSH REWARD] R: 0.0\n",
      "[OVERALL REWARD] 0.0\n",
      "[ACTION TYPE]: 1\n",
      "[MOVE] move: [ 0.         0.015     -0.02      -0.2617994] move index: [1, 2, 0, 0]\n",
      "next move: tensor([ 0.0000,  0.0150, -0.0200,  0.0000], device='cuda:0') next move index: [1, 2, 0, 1]\n",
      "target_Qx: 0.9467907547950745 Qx: 0.8567588925361633\n",
      "target_Qy: 0.9041589498519897 Qy: 0.8545026183128357\n",
      "target_Qz: 0.9372692704200745 Qz: 0.834688663482666\n",
      "target_Qyaw: 0.939400315284729 Qyaw: 0.805665135383606\n",
      "loss_rank: 0.007897338829934597\n",
      "priority: 0.017642181366682053\n",
      "[SUCCESS] append transition experience\n",
      "==== EXPERT MODE ENABLE BC: False ENABLE RL: False FULL TRAIN: False ====\n",
      "==== low level action taken: 9 N_pickable_item: 2 ====\n",
      "[SUCCESS] get raw data\n",
      "Qx: tensor([[0.7489, 0.9417, 0.8613]], device='cuda:0')\n",
      "Qy: tensor([[0.7349, 0.8486, 0.8927]], device='cuda:0')\n",
      "Qz: tensor([[0.9192, 0.8238, 0.8479]], device='cuda:0')\n",
      "Qyaw: tensor([[0.8651, 0.9216, 0.7805]], device='cuda:0')\n",
      "[MAX OUTPUT]\n",
      "[Q net] x: 1, y: 2, z: 0, yaw_ind: 1\n",
      "[SUCCESS] estimate actions from network\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[GRIPPER STATUS] FULL ClOSE\n",
      "gripper tip height: 0.08526160414758932\n",
      "[PUSH REWARD] R: 0.0\n",
      "[OVERALL REWARD] 0.0\n",
      "[ACTION TYPE]: 1\n",
      "[MOVE] move: [ 0.     0.015 -0.02   0.   ] move index: [1, 2, 0, 1]\n",
      "next move: tensor([ 0.0000,  0.0000, -0.0200,  0.0000], device='cuda:0') next move index: [1, 1, 0, 1]\n",
      "target_Qx: 0.9989962577819824 Qx: 0.94167160987854\n",
      "target_Qy: 0.9292478561401367 Qy: 0.8927268385887146\n",
      "target_Qz: 0.9470687508583069 Qz: 0.9191767573356628\n",
      "target_Qyaw: 0.8399407267570496 Qyaw: 0.9216005206108093\n",
      "loss_rank: 0.012703181244432926\n",
      "priority: 0.01571972668170929\n",
      "[SUCCESS] append transition experience\n",
      "==== EXPERT MODE ENABLE BC: False ENABLE RL: False FULL TRAIN: False ====\n",
      "==== low level action taken: 9 N_pickable_item: 2 ====\n",
      "[SUCCESS] get raw data\n",
      "Qx: tensor([[0.7335, 0.9999, 0.7678]], device='cuda:0')\n",
      "Qy: tensor([[0.7170, 0.9355, 0.7177]], device='cuda:0')\n",
      "Qz: tensor([[0.9281, 0.7459, 0.7456]], device='cuda:0')\n",
      "Qyaw: tensor([[0.7574, 0.8405, 0.7045]], device='cuda:0')\n",
      "[MAX OUTPUT]\n",
      "[Q net] x: 1, y: 1, z: 0, yaw_ind: 1\n",
      "[SUCCESS] estimate actions from network\n",
      "[GRIPPER STATUS] FULL CLOSE\n",
      "[GRIPPER STATUS] FULL CLOSE\n",
      "gripper tip height: 0.06544962581351171\n",
      "[PUSH REWARD] R: 0.0\n",
      "[OVERALL REWARD] 0.0\n",
      "[ACTION TYPE]: 1\n",
      "[MOVE] move: [ 0.    0.   -0.02  0.  ] move index: [1, 1, 0, 1]\n",
      "next move: tensor([ 0.0000,  0.0000, -0.0200,  0.0000], device='cuda:0') next move index: [1, 1, 0, 1]\n",
      "target_Qx: 0.991363525390625 Qx: 0.9999371767044067\n",
      "target_Qy: 0.9888252019882202 Qy: 0.9355180859565735\n",
      "target_Qz: 0.986764132976532 Qz: 0.9281499981880188\n",
      "target_Qyaw: 0.9617764353752136 Qyaw: 0.8405351042747498\n",
      "loss_rank: 0.001408187672495842\n",
      "priority: 0.006670746020972729\n",
      "[SUCCESS] append transition experience\n",
      "==== EXPERT MODE ENABLE BC: False ENABLE RL: False FULL TRAIN: False ====\n",
      "==== low level action taken: 9 N_pickable_item: 2 ====\n",
      "[SUCCESS] get raw data\n",
      "Qx: tensor([[0.7999, 0.9757, 0.9153]], device='cuda:0')\n",
      "Qy: tensor([[0.8158, 0.9707, 0.7653]], device='cuda:0')\n",
      "Qz: tensor([[0.9598, 0.7627, 0.8834]], device='cuda:0')\n",
      "Qyaw: tensor([[0.8005, 0.9369, 0.8631]], device='cuda:0')\n",
      "[MAX OUTPUT]\n",
      "[Q net] x: 1, y: 1, z: 0, yaw_ind: 1\n",
      "[SUCCESS] estimate actions from network\n",
      "[GRIPPER STATUS] FULL CLOSE\n",
      "[GRIPPER STATUS] FULL CLOSE\n",
      "gripper tip height: 0.0447555554505461\n",
      "[PUSH REWARD] R: 1.0\n",
      "[OVERALL REWARD] 1.0\n",
      "[ACTION TYPE]: 1\n",
      "[MOVE] move: [ 0.    0.   -0.02  0.  ] move index: [1, 1, 0, 1]\n",
      "next move: tensor([ 0.0000,  0.0000, -0.0200,  0.0000], device='cuda:0') next move index: [1, 1, 0, 1]\n",
      "target_Qx: 1.0 Qx: 0.975682258605957\n",
      "target_Qy: 1.0 Qy: 0.9706627726554871\n",
      "target_Qz: 1.0 Qz: 0.9598009586334229\n",
      "target_Qyaw: 1.0 Qyaw: 0.9368730783462524\n",
      "loss_rank: 0.007453744299709797\n",
      "priority: 0.009216993115842342\n",
      "[SUCCESS] append transition experience\n",
      "[SUCCESS] perform push action\n",
      "=== end of action ===\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[SUCCESS] return home\n",
      "[SUCCESS] return home\n",
      "[SUCCESS] GRASP OR PUSH ACTION\n",
      "[PUSH SUCCESS RATE] 1.4000000000000001%/-inf% [500]\n",
      "[SUCCESS] save agent data\n",
      "==== episode: 0 ====\n",
      "[MIN_DISTANCE] 0.05250767738995247\n",
      "N_step_x: 1.0, N_step_y: 11.0, N_step_z: 10.0, N_step_yaw: 2.0, N_step: 11.0\n",
      "[MIN_DISTANCE] 0.05250767738995247\n",
      "N_step_low_level: 13\n",
      "==== EXPERT MODE ENABLE BC: False ENABLE RL: False FULL TRAIN: False ====\n",
      "==== low level action taken: 10 N_pickable_item: 2 ====\n",
      "[SUCCESS] get raw data\n",
      "Qx: tensor([[0.4097, 0.3832, 0.4811]], device='cuda:0')\n",
      "Qy: tensor([[0.4116, 0.3659, 0.4107]], device='cuda:0')\n",
      "Qz: tensor([[0.5388, 0.2191, 0.0827]], device='cuda:0')\n",
      "Qyaw: tensor([[0.2449, 0.2327, 0.2740]], device='cuda:0')\n",
      "[MAX OUTPUT]\n",
      "[Q net] x: 2, y: 0, z: 0, yaw_ind: 2\n",
      "[SUCCESS] estimate actions from network\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[GRASP DISTANCE] 0.0\n",
      "[GRASP REWARD] R: 0.0\n",
      "[OVERALL REWARD] 0.0\n",
      "[ACTION TYPE]: 0\n",
      "[MOVE] move: [ 0.015      0.015     -0.02      -0.1308997] move index: [2, 2, 0, 0]\n",
      "next move: tensor([ 0.0000,  0.0150, -0.0200,  0.0000], device='cuda:0') next move index: [1, 2, 0, 1]\n",
      "target_Qx: 0.420656681060791 Qx: 0.48106649518013\n",
      "target_Qy: 0.4388262629508972 Qy: 0.41065341234207153\n",
      "target_Qz: 0.45816779136657715 Qz: 0.5388385653495789\n",
      "target_Qyaw: 0.2210453897714615 Qyaw: 0.24492356181144714\n",
      "loss_rank: 0.033650144934654236\n",
      "priority: 0.03653039410710335\n",
      "[SUCCESS] append transition experience\n",
      "==== EXPERT MODE ENABLE BC: False ENABLE RL: False FULL TRAIN: False ====\n",
      "==== low level action taken: 10 N_pickable_item: 2 ====\n",
      "[SUCCESS] get raw data\n",
      "Qx: tensor([[0.3298, 0.4342, 0.4324]], device='cuda:0')\n",
      "Qy: tensor([[0.3330, 0.2260, 0.4322]], device='cuda:0')\n",
      "Qz: tensor([[0.4499, 0.2918, 0.2797]], device='cuda:0')\n",
      "Qyaw: tensor([[0.1719, 0.2005, 0.2487]], device='cuda:0')\n",
      "[MAX OUTPUT]\n",
      "[Q net] x: 1, y: 2, z: 0, yaw_ind: 2\n",
      "[SUCCESS] estimate actions from network\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[GRASP DISTANCE] 0.0\n",
      "[GRASP REWARD] R: 0.0\n",
      "[OVERALL REWARD] 0.0\n",
      "[ACTION TYPE]: 0\n",
      "[MOVE] move: [ 0.     0.015 -0.02   0.   ] move index: [1, 2, 0, 1]\n",
      "next move: tensor([ 0.0000,  0.0150, -0.0200,  0.0000], device='cuda:0') next move index: [1, 2, 0, 1]\n",
      "target_Qx: 0.46439141035079956 Qx: 0.4342184066772461\n",
      "target_Qy: 0.48886173963546753 Qy: 0.43219131231307983\n",
      "target_Qz: 0.481708824634552 Qz: 0.44992607831954956\n",
      "target_Qyaw: 0.23671935498714447 Qyaw: 0.20052313804626465\n",
      "loss_rank: 0.02654077485203743\n",
      "priority: 0.028151338919997215\n",
      "[SUCCESS] append transition experience\n",
      "==== EXPERT MODE ENABLE BC: False ENABLE RL: False FULL TRAIN: False ====\n",
      "==== low level action taken: 10 N_pickable_item: 2 ====\n",
      "[SUCCESS] get raw data\n",
      "Qx: tensor([[0.3619, 0.4590, 0.5092]], device='cuda:0')\n",
      "Qy: tensor([[0.3055, 0.2524, 0.4857]], device='cuda:0')\n",
      "Qz: tensor([[0.4764, 0.3016, 0.3345]], device='cuda:0')\n",
      "Qyaw: tensor([[0.1584, 0.2407, 0.2072]], device='cuda:0')\n",
      "[MAX OUTPUT]\n",
      "[Q net] x: 2, y: 2, z: 0, yaw_ind: 1\n",
      "[SUCCESS] estimate actions from network\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[GRASP DISTANCE] 0.0\n",
      "[GRASP REWARD] R: 0.0\n",
      "[OVERALL REWARD] 0.0\n",
      "[ACTION TYPE]: 0\n",
      "[MOVE] move: [ 0.     0.015 -0.02   0.   ] move index: [1, 2, 0, 1]\n",
      "next move: tensor([ 0.0000,  0.0150, -0.0200,  0.0000], device='cuda:0') next move index: [1, 2, 0, 1]\n",
      "target_Qx: 0.5907943844795227 Qx: 0.4590088427066803\n",
      "target_Qy: 0.5336710214614868 Qy: 0.48565104603767395\n",
      "target_Qz: 0.5383334159851074 Qz: 0.4764145016670227\n",
      "target_Qyaw: 0.39588284492492676 Qyaw: 0.24074506759643555\n",
      "loss_rank: 0.01977183297276497\n",
      "priority: 0.03166558966040611\n",
      "[SUCCESS] append transition experience\n",
      "==== EXPERT MODE ENABLE BC: False ENABLE RL: False FULL TRAIN: False ====\n",
      "==== low level action taken: 10 N_pickable_item: 2 ====\n",
      "[SUCCESS] get raw data\n",
      "Qx: tensor([[0.5676, 0.6024, 0.4904]], device='cuda:0')\n",
      "Qy: tensor([[0.4771, 0.4064, 0.5369]], device='cuda:0')\n",
      "Qz: tensor([[0.5566, 0.4843, 0.4826]], device='cuda:0')\n",
      "Qyaw: tensor([[0.4086, 0.3949, 0.2856]], device='cuda:0')\n",
      "[MAX OUTPUT]\n",
      "[Q net] x: 1, y: 2, z: 0, yaw_ind: 0\n",
      "[SUCCESS] estimate actions from network\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[GRASP DISTANCE] 0.0\n",
      "[GRASP REWARD] R: 0.0\n",
      "[OVERALL REWARD] 0.0\n",
      "[ACTION TYPE]: 0\n",
      "[MOVE] move: [ 0.     0.015 -0.02   0.   ] move index: [1, 2, 0, 1]\n",
      "next move: tensor([ 0.0000,  0.0150, -0.0200,  0.0000], device='cuda:0') next move index: [1, 2, 0, 1]\n",
      "target_Qx: 0.7067442536354065 Qx: 0.6024160981178284\n",
      "target_Qy: 0.6088443994522095 Qy: 0.536925196647644\n",
      "target_Qz: 0.6227827668190002 Qz: 0.5566093325614929\n",
      "target_Qyaw: 0.47708192467689514 Qyaw: 0.3948589563369751\n",
      "loss_rank: 0.02272609993815422\n",
      "priority: 0.029525168240070343\n",
      "[SUCCESS] append transition experience\n",
      "==== EXPERT MODE ENABLE BC: False ENABLE RL: False FULL TRAIN: False ====\n",
      "==== low level action taken: 10 N_pickable_item: 2 ====\n",
      "[SUCCESS] get raw data\n",
      "Qx: tensor([[0.5140, 0.7161, 0.4855]], device='cuda:0')\n",
      "Qy: tensor([[0.4786, 0.4581, 0.6183]], device='cuda:0')\n",
      "Qz: tensor([[0.6260, 0.4655, 0.5395]], device='cuda:0')\n",
      "Qyaw: tensor([[0.5172, 0.4769, 0.3904]], device='cuda:0')\n",
      "[MAX OUTPUT]\n",
      "[Q net] x: 1, y: 2, z: 0, yaw_ind: 0\n",
      "[SUCCESS] estimate actions from network\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[GRASP DISTANCE] 0.0\n",
      "[GRASP REWARD] R: 0.0\n",
      "[OVERALL REWARD] 0.0\n",
      "[ACTION TYPE]: 0\n",
      "[MOVE] move: [ 0.     0.015 -0.02   0.   ] move index: [1, 2, 0, 1]\n",
      "next move: tensor([0.0000, 0.0150, 0.0000, 0.0000], device='cuda:0') next move index: [1, 2, 1, 1]\n",
      "target_Qx: 0.7265267968177795 Qx: 0.716064989566803\n",
      "target_Qy: 0.6942339539527893 Qy: 0.6182783842086792\n",
      "target_Qz: 0.38049355149269104 Qz: 0.6259814500808716\n",
      "target_Qyaw: 0.5022813081741333 Qyaw: 0.4769473671913147\n",
      "loss_rank: 0.013935931958258152\n",
      "priority: 0.030632134526968002\n",
      "[SUCCESS] append transition experience\n",
      "==== EXPERT MODE ENABLE BC: False ENABLE RL: False FULL TRAIN: False ====\n",
      "==== low level action taken: 10 N_pickable_item: 2 ====\n",
      "[SUCCESS] get raw data\n",
      "Qx: tensor([[0.5001, 0.7437, 0.4599]], device='cuda:0')\n",
      "Qy: tensor([[0.3899, 0.3846, 0.7040]], device='cuda:0')\n",
      "Qz: tensor([[0.7663, 0.4051, 0.4634]], device='cuda:0')\n",
      "Qyaw: tensor([[0.5204, 0.5099, 0.2771]], device='cuda:0')\n",
      "[MAX OUTPUT]\n",
      "[Q net] x: 1, y: 2, z: 0, yaw_ind: 0\n",
      "[SUCCESS] estimate actions from network\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[GRASP DISTANCE] 0.0\n",
      "[GRASP REWARD] R: 0.0\n",
      "[OVERALL REWARD] 0.0\n",
      "[ACTION TYPE]: 0\n",
      "[MOVE] move: [0.    0.015 0.    0.   ] move index: [1, 2, 1, 1]\n",
      "next move: tensor([ 0.0000,  0.0150, -0.0200,  0.0000], device='cuda:0') next move index: [1, 2, 0, 1]\n",
      "target_Qx: 0.8227092623710632 Qx: 0.7437407970428467\n",
      "target_Qy: 0.7307283878326416 Qy: 0.7040008902549744\n",
      "target_Qz: 0.7825939059257507 Qz: 0.40514686703681946\n",
      "target_Qyaw: 0.6100642085075378 Qyaw: 0.5098810195922852\n",
      "loss_rank: 0.060822099447250366\n",
      "priority: 0.10068543255329132\n",
      "[SUCCESS] append transition experience\n",
      "==== EXPERT MODE ENABLE BC: False ENABLE RL: False FULL TRAIN: False ====\n",
      "==== low level action taken: 10 N_pickable_item: 2 ====\n",
      "[SUCCESS] get raw data\n",
      "Qx: tensor([[0.6381, 0.7942, 0.6401]], device='cuda:0')\n",
      "Qy: tensor([[0.5463, 0.5929, 0.7342]], device='cuda:0')\n",
      "Qz: tensor([[0.7930, 0.4931, 0.4928]], device='cuda:0')\n",
      "Qyaw: tensor([[0.7950, 0.6093, 0.5071]], device='cuda:0')\n",
      "[MAX OUTPUT]\n",
      "[Q net] x: 1, y: 2, z: 0, yaw_ind: 0\n",
      "[SUCCESS] estimate actions from network\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[GRASP DISTANCE] 0.0\n",
      "[GRASP REWARD] R: 0.0\n",
      "[OVERALL REWARD] 0.0\n",
      "[ACTION TYPE]: 0\n",
      "[MOVE] move: [ 0.     0.015 -0.02   0.   ] move index: [1, 2, 0, 1]\n",
      "next move: tensor([ 0.0000,  0.0150, -0.0200,  0.0000], device='cuda:0') next move index: [1, 2, 0, 1]\n",
      "target_Qx: 0.7661129236221313 Qx: 0.7942430377006531\n",
      "target_Qy: 0.7046124935150146 Qy: 0.7342275381088257\n",
      "target_Qz: 0.8223857283592224 Qz: 0.7929831147193909\n",
      "target_Qyaw: 0.6801249980926514 Qyaw: 0.609325647354126\n",
      "loss_rank: 0.02380627766251564\n",
      "priority: 0.025692632421851158\n",
      "[SUCCESS] append transition experience\n",
      "==== EXPERT MODE ENABLE BC: False ENABLE RL: False FULL TRAIN: False ====\n",
      "==== low level action taken: 10 N_pickable_item: 2 ====\n",
      "[SUCCESS] get raw data\n",
      "Qx: tensor([[0.6463, 0.7626, 0.5924]], device='cuda:0')\n",
      "Qy: tensor([[0.6153, 0.4846, 0.7091]], device='cuda:0')\n",
      "Qz: tensor([[0.8072, 0.5574, 0.5547]], device='cuda:0')\n",
      "Qyaw: tensor([[0.6405, 0.6706, 0.5879]], device='cuda:0')\n",
      "[MAX OUTPUT]\n",
      "[Q net] x: 1, y: 2, z: 0, yaw_ind: 1\n",
      "[SUCCESS] estimate actions from network\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[GRASP DISTANCE] 0.0\n",
      "[GRASP REWARD] R: 0.0\n",
      "[OVERALL REWARD] 0.0\n",
      "[ACTION TYPE]: 0\n",
      "[MOVE] move: [ 0.     0.015 -0.02   0.   ] move index: [1, 2, 0, 1]\n",
      "next move: tensor([ 0.0000,  0.0150, -0.0200,  0.0000], device='cuda:0') next move index: [1, 2, 0, 1]\n",
      "target_Qx: 0.777435302734375 Qx: 0.7626420259475708\n",
      "target_Qy: 0.784426212310791 Qy: 0.709088921546936\n",
      "target_Qz: 0.8208200931549072 Qz: 0.8071786165237427\n",
      "target_Qyaw: 0.7785948514938354 Qyaw: 0.6705834865570068\n",
      "loss_rank: 0.007784559857100248\n",
      "priority: 0.012221332639455795\n",
      "[SUCCESS] append transition experience\n",
      "==== EXPERT MODE ENABLE BC: False ENABLE RL: False FULL TRAIN: False ====\n",
      "==== low level action taken: 10 N_pickable_item: 2 ====\n",
      "[SUCCESS] get raw data\n",
      "Qx: tensor([[0.7170, 0.8075, 0.7066]], device='cuda:0')\n",
      "Qy: tensor([[0.6135, 0.5532, 0.8092]], device='cuda:0')\n",
      "Qz: tensor([[0.7989, 0.5550, 0.5820]], device='cuda:0')\n",
      "Qyaw: tensor([[0.6676, 0.7851, 0.7414]], device='cuda:0')\n",
      "[MAX OUTPUT]\n",
      "[Q net] x: 1, y: 2, z: 0, yaw_ind: 1\n",
      "[SUCCESS] estimate actions from network\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[GRASP DISTANCE] 0.0\n",
      "[GRASP REWARD] R: 0.0\n",
      "[OVERALL REWARD] 0.0\n",
      "[ACTION TYPE]: 0\n",
      "[MOVE] move: [ 0.     0.015 -0.02   0.   ] move index: [1, 2, 0, 1]\n",
      "next move: tensor([ 0.0000,  0.0150, -0.0200,  0.0000], device='cuda:0') next move index: [1, 2, 0, 1]\n",
      "target_Qx: 0.8476377129554749 Qx: 0.8075050711631775\n",
      "target_Qy: 0.9343684911727905 Qy: 0.8091635704040527\n",
      "target_Qz: 0.8818082809448242 Qz: 0.7989465594291687\n",
      "target_Qyaw: 0.8634929060935974 Qyaw: 0.7851388454437256\n",
      "loss_rank: 0.005477797240018845\n",
      "priority: 0.013050878420472145\n",
      "[SUCCESS] append transition experience\n",
      "==== EXPERT MODE ENABLE BC: False ENABLE RL: False FULL TRAIN: False ====\n",
      "==== low level action taken: 10 N_pickable_item: 2 ====\n",
      "[SUCCESS] get raw data\n",
      "Qx: tensor([[0.6469, 0.8316, 0.7639]], device='cuda:0')\n",
      "Qy: tensor([[0.6960, 0.8269, 0.9714]], device='cuda:0')\n",
      "Qz: tensor([[0.8824, 0.6272, 0.7140]], device='cuda:0')\n",
      "Qyaw: tensor([[0.6644, 0.8595, 0.7072]], device='cuda:0')\n",
      "[MAX OUTPUT]\n",
      "[Q net] x: 1, y: 2, z: 0, yaw_ind: 1\n",
      "[SUCCESS] estimate actions from network\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[GRASP DISTANCE] 0.0\n",
      "[GRASP REWARD] R: 0.0\n",
      "[OVERALL REWARD] 0.0\n",
      "[ACTION TYPE]: 0\n",
      "[MOVE] move: [ 0.     0.015 -0.02   0.   ] move index: [1, 2, 0, 1]\n",
      "next move: tensor([ 0.0000,  0.0000, -0.0200,  0.0000], device='cuda:0') next move index: [1, 1, 0, 1]\n",
      "target_Qx: 0.9768090844154358 Qx: 0.8316407799720764\n",
      "target_Qy: 1.021673321723938 Qy: 0.9713674783706665\n",
      "target_Qz: 0.9697437882423401 Qz: 0.8823580741882324\n",
      "target_Qyaw: 0.939119815826416 Qyaw: 0.859484076499939\n",
      "loss_rank: 0.0026920619420707226\n",
      "priority: 0.012087719514966011\n",
      "[SUCCESS] append transition experience\n",
      "==== EXPERT MODE ENABLE BC: False ENABLE RL: False FULL TRAIN: False ====\n",
      "==== low level action taken: 10 N_pickable_item: 2 ====\n",
      "[SUCCESS] get raw data\n",
      "Qx: tensor([[0.9294, 0.9815, 0.8536]], device='cuda:0')\n",
      "Qy: tensor([[0.8335, 1.0204, 0.8813]], device='cuda:0')\n",
      "Qz: tensor([[0.9682, 0.8541, 0.8529]], device='cuda:0')\n",
      "Qyaw: tensor([[0.7820, 0.9412, 0.8884]], device='cuda:0')\n",
      "[MAX OUTPUT]\n",
      "[Q net] x: 1, y: 1, z: 0, yaw_ind: 1\n",
      "[SUCCESS] estimate actions from network\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[GRASP DISTANCE] 0.0\n",
      "[GRASP REWARD] R: 0.0\n",
      "[OVERALL REWARD] 0.0\n",
      "[ACTION TYPE]: 0\n",
      "[MOVE] move: [ 0.    0.   -0.02  0.  ] move index: [1, 1, 0, 1]\n",
      "next move: tensor([ 0.0000,  0.0000, -0.0200,  0.0000], device='cuda:0') next move index: [1, 1, 0, 1]\n",
      "target_Qx: 1.0217351913452148 Qx: 0.9815275073051453\n",
      "target_Qy: 0.9978453516960144 Qy: 1.0204380750656128\n",
      "target_Qz: 0.9626931548118591 Qz: 0.9682064652442932\n",
      "target_Qyaw: 0.9961521625518799 Qyaw: 0.9411814212799072\n",
      "loss_rank: 0.007920867763459682\n",
      "priority: 0.009215684607625008\n",
      "[SUCCESS] append transition experience\n",
      "==== EXPERT MODE ENABLE BC: False ENABLE RL: False FULL TRAIN: False ====\n",
      "==== low level action taken: 10 N_pickable_item: 2 ====\n",
      "[SUCCESS] get raw data\n",
      "Qx: tensor([[0.9060, 1.0266, 0.8203]], device='cuda:0')\n",
      "Qy: tensor([[0.7894, 1.0133, 0.8924]], device='cuda:0')\n",
      "Qz: tensor([[0.9706, 0.8847, 0.8803]], device='cuda:0')\n",
      "Qyaw: tensor([[0.8097, 0.9913, 0.8617]], device='cuda:0')\n",
      "[MAX OUTPUT]\n",
      "[Q net] x: 1, y: 1, z: 0, yaw_ind: 1\n",
      "[SUCCESS] estimate actions from network\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[GRASP] grasp something\n",
      "[GRIPPER STATUS] NON_CLOSE_NON_OPEN\n",
      "[GRASP DISTANCE] 0.0\n",
      "[SUCESS] picked an item!\n",
      "[GRASP REWARD] R: 1.0\n",
      "[OVERALL REWARD] 1.0\n",
      "[ACTION TYPE]: 0\n",
      "[MOVE] move: [ 0.    0.   -0.02  0.  ] move index: [1, 1, 0, 1]\n",
      "next move: tensor([ 0.0000,  0.0000, -0.0200,  0.0000], device='cuda:0') next move index: [1, 1, 0, 1]\n",
      "target_Qx: 1.0 Qx: 1.0266419649124146\n",
      "target_Qy: 1.0 Qy: 1.0133312940597534\n",
      "target_Qz: 1.0 Qz: 0.9706355929374695\n",
      "target_Qyaw: 1.0 Qyaw: 0.9913281202316284\n",
      "loss_rank: 0.001976685132831335\n",
      "priority: 0.0024329321458935738\n",
      "[SUCCESS] append transition experience\n",
      "[SUCCESS] grasp an item successfully\n",
      "=== end of action ===\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[SUCCESS] return home\n",
      "[SUCCESS] return home\n",
      "[SUCCESS] GRASP OR PUSH ACTION\n",
      "[GRASP SUCCESS RATE] 0.8%/-inf% [500]\n",
      "[SUCCESS] save agent data\n",
      "==== episode: 0 ====\n",
      "[MIN_DISTANCE] inf\n",
      "N_step_x: 8.0, N_step_y: 10.0, N_step_z: 10.0, N_step_yaw: 1.0, N_step: 10.0\n",
      "N_step_low_level: 12\n",
      "==== EXPERT MODE ENABLE BC: False ENABLE RL: False FULL TRAIN: False ====\n",
      "==== low level action taken: 11 N_pickable_item: 1 ====\n",
      "[SUCCESS] get raw data\n",
      "Qx: tensor([[0.5175, 0.4308, 0.5315]], device='cuda:0')\n",
      "Qy: tensor([[0.3938, 0.4421, 0.4100]], device='cuda:0')\n",
      "Qz: tensor([[0.5078, 0.2543, 0.1310]], device='cuda:0')\n",
      "Qyaw: tensor([[0.2391, 0.2812, 0.2560]], device='cuda:0')\n",
      "[RANDOM OUTPUT]\n",
      "[Q net] x: 1, y: 2, z: 1, yaw_ind: 0\n",
      "[SUCCESS] estimate actions from network\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[GRASP DISTANCE] 0.0\n",
      "[GRASP REWARD] R: 0.0\n",
      "[OVERALL REWARD] 0.0\n",
      "[ACTION TYPE]: 0\n",
      "[MOVE] move: [ 0.015      0.015     -0.02       0.1308997] move index: [2, 2, 0, 2]\n",
      "next move: tensor([ 0.0150,  0.0150, -0.0200,  0.0000], device='cuda:0') next move index: [2, 2, 0, 1]\n",
      "target_Qx: 0.5568354725837708 Qx: 0.5314877033233643\n",
      "target_Qy: 0.5132280588150024 Qy: 0.4100187420845032\n",
      "target_Qz: 0.5208219885826111 Qz: 0.507829487323761\n",
      "target_Qyaw: 0.3270634114742279 Qyaw: 0.2560081481933594\n",
      "loss_rank: 0.04250606149435043\n",
      "priority: 0.04663414508104324\n",
      "[SUCCESS] append transition experience\n",
      "==== EXPERT MODE ENABLE BC: False ENABLE RL: False FULL TRAIN: False ====\n",
      "==== low level action taken: 11 N_pickable_item: 1 ====\n",
      "[SUCCESS] get raw data\n",
      "Qx: tensor([[0.5116, 0.5805, 0.5821]], device='cuda:0')\n",
      "Qy: tensor([[0.3349, 0.4789, 0.5080]], device='cuda:0')\n",
      "Qz: tensor([[0.5670, 0.4183, 0.2923]], device='cuda:0')\n",
      "Qyaw: tensor([[0.2352, 0.3147, 0.1892]], device='cuda:0')\n",
      "[MAX OUTPUT]\n",
      "[Q net] x: 2, y: 2, z: 0, yaw_ind: 1\n",
      "[SUCCESS] estimate actions from network\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[GRASP DISTANCE] 0.0\n",
      "[GRASP REWARD] R: 0.0\n",
      "[OVERALL REWARD] 0.0\n",
      "[ACTION TYPE]: 0\n",
      "[MOVE] move: [ 0.015  0.015 -0.02   0.   ] move index: [2, 2, 0, 1]\n",
      "next move: tensor([ 0.0150,  0.0150, -0.0200,  0.0000], device='cuda:0') next move index: [2, 2, 0, 1]\n",
      "target_Qx: 0.6222279667854309 Qx: 0.5820656418800354\n",
      "target_Qy: 0.5432651042938232 Qy: 0.5079813599586487\n",
      "target_Qz: 0.6199222207069397 Qz: 0.567039430141449\n",
      "target_Qyaw: 0.3743800222873688 Qyaw: 0.3146873712539673\n",
      "loss_rank: 0.018286583945155144\n",
      "priority: 0.020591024309396744\n",
      "[SUCCESS] append transition experience\n",
      "==== EXPERT MODE ENABLE BC: False ENABLE RL: False FULL TRAIN: False ====\n",
      "==== low level action taken: 11 N_pickable_item: 1 ====\n",
      "[SUCCESS] get raw data\n",
      "Qx: tensor([[0.5946, 0.5727, 0.6333]], device='cuda:0')\n",
      "Qy: tensor([[0.3496, 0.5182, 0.5337]], device='cuda:0')\n",
      "Qz: tensor([[0.6630, 0.4052, 0.3936]], device='cuda:0')\n",
      "Qyaw: tensor([[0.4216, 0.3888, 0.2590]], device='cuda:0')\n",
      "[RANDOM OUTPUT]\n",
      "[Q net] x: 2, y: 1, z: 2, yaw_ind: 0\n",
      "[SUCCESS] estimate actions from network\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[GRASP DISTANCE] 0.0\n",
      "[GRASP REWARD] R: 0.0\n",
      "[OVERALL REWARD] 0.0\n",
      "[ACTION TYPE]: 0\n",
      "[MOVE] move: [ 0.015  0.015 -0.02   0.   ] move index: [2, 2, 0, 1]\n",
      "next move: tensor([ 0.0000,  0.0150, -0.0200,  0.0000], device='cuda:0') next move index: [1, 2, 0, 1]\n",
      "target_Qx: 0.6246083974838257 Qx: 0.6333168148994446\n",
      "target_Qy: 0.5840142369270325 Qy: 0.5337395071983337\n",
      "target_Qz: 0.6622686982154846 Qz: 0.6630322337150574\n",
      "target_Qyaw: 0.467492014169693 Qyaw: 0.3888372778892517\n",
      "loss_rank: 0.026493078097701073\n",
      "priority: 0.02869071252644062\n",
      "[SUCCESS] append transition experience\n",
      "==== EXPERT MODE ENABLE BC: False ENABLE RL: False FULL TRAIN: False ====\n",
      "==== low level action taken: 11 N_pickable_item: 1 ====\n",
      "[SUCCESS] get raw data\n",
      "Qx: tensor([[0.6259, 0.6284, 0.5884]], device='cuda:0')\n",
      "Qy: tensor([[0.4549, 0.4432, 0.5883]], device='cuda:0')\n",
      "Qz: tensor([[0.6887, 0.4284, 0.4828]], device='cuda:0')\n",
      "Qyaw: tensor([[0.3884, 0.4765, 0.4228]], device='cuda:0')\n",
      "[MAX OUTPUT]\n",
      "[Q net] x: 1, y: 2, z: 0, yaw_ind: 1\n",
      "[SUCCESS] estimate actions from network\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[GRASP DISTANCE] 0.0\n",
      "[GRASP REWARD] R: 0.0\n",
      "[OVERALL REWARD] 0.0\n",
      "[ACTION TYPE]: 0\n",
      "[MOVE] move: [ 0.     0.015 -0.02   0.   ] move index: [1, 2, 0, 1]\n",
      "next move: tensor([ 0.0150,  0.0150, -0.0200,  0.0000], device='cuda:0') next move index: [2, 2, 0, 1]\n",
      "target_Qx: 0.5337121486663818 Qx: 0.6284433007240295\n",
      "target_Qy: 0.611970841884613 Qy: 0.5883215665817261\n",
      "target_Qz: 0.5857126712799072 Qz: 0.6887103319168091\n",
      "target_Qyaw: 0.4165586531162262 Qyaw: 0.47646045684814453\n",
      "loss_rank: 0.017972830682992935\n",
      "priority: 0.02390533685684204\n",
      "[SUCCESS] append transition experience\n",
      "==== EXPERT MODE ENABLE BC: False ENABLE RL: False FULL TRAIN: False ====\n",
      "==== low level action taken: 11 N_pickable_item: 1 ====\n",
      "[SUCCESS] get raw data\n",
      "Qx: tensor([[0.5697, 0.6084, 0.5589]], device='cuda:0')\n",
      "Qy: tensor([[0.3446, 0.4091, 0.6275]], device='cuda:0')\n",
      "Qz: tensor([[0.6334, 0.4563, 0.3841]], device='cuda:0')\n",
      "Qyaw: tensor([[0.3570, 0.4343, 0.3199]], device='cuda:0')\n",
      "[MAX OUTPUT]\n",
      "[Q net] x: 1, y: 2, z: 0, yaw_ind: 1\n",
      "[SUCCESS] estimate actions from network\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[GRASP DISTANCE] 0.0\n",
      "[GRASP REWARD] R: 0.0\n",
      "[OVERALL REWARD] 0.0\n",
      "[ACTION TYPE]: 0\n",
      "[MOVE] move: [ 0.015  0.015 -0.02   0.   ] move index: [2, 2, 0, 1]\n",
      "next move: tensor([ 0.0150,  0.0150, -0.0200,  0.0000], device='cuda:0') next move index: [2, 2, 0, 1]\n",
      "target_Qx: 0.649222195148468 Qx: 0.5589061379432678\n",
      "target_Qy: 0.6340203285217285 Qy: 0.6274688839912415\n",
      "target_Qz: 0.6487429738044739 Qz: 0.6334467530250549\n",
      "target_Qyaw: 0.5503941178321838 Qyaw: 0.4342682361602783\n",
      "loss_rank: 0.023588139563798904\n",
      "priority: 0.02906791679561138\n",
      "[SUCCESS] append transition experience\n",
      "==== EXPERT MODE ENABLE BC: False ENABLE RL: False FULL TRAIN: False ====\n",
      "==== low level action taken: 11 N_pickable_item: 1 ====\n",
      "[SUCCESS] get raw data\n",
      "Qx: tensor([[0.6092, 0.5471, 0.6227]], device='cuda:0')\n",
      "Qy: tensor([[0.6228, 0.5681, 0.6322]], device='cuda:0')\n",
      "Qz: tensor([[0.6979, 0.5809, 0.6147]], device='cuda:0')\n",
      "Qyaw: tensor([[0.4488, 0.5237, 0.3799]], device='cuda:0')\n",
      "[MAX OUTPUT]\n",
      "[Q net] x: 2, y: 2, z: 0, yaw_ind: 1\n",
      "[SUCCESS] estimate actions from network\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[GRASP DISTANCE] 0.0\n",
      "[GRASP REWARD] R: 0.0\n",
      "[OVERALL REWARD] 0.0\n",
      "[ACTION TYPE]: 0\n",
      "[MOVE] move: [ 0.015  0.015 -0.02   0.   ] move index: [2, 2, 0, 1]\n",
      "next move: tensor([ 0.0000,  0.0150, -0.0200,  0.0000], device='cuda:0') next move index: [1, 2, 0, 1]\n",
      "target_Qx: 0.7848066687583923 Qx: 0.6226723790168762\n",
      "target_Qy: 0.7519954442977905 Qy: 0.6321846842765808\n",
      "target_Qz: 0.832496166229248 Qz: 0.6978737711906433\n",
      "target_Qyaw: 0.6812321543693542 Qyaw: 0.5237413644790649\n",
      "loss_rank: 0.023283949121832848\n",
      "priority: 0.044176120311021805\n",
      "[SUCCESS] append transition experience\n",
      "==== EXPERT MODE ENABLE BC: False ENABLE RL: False FULL TRAIN: False ====\n",
      "==== low level action taken: 11 N_pickable_item: 1 ====\n",
      "[SUCCESS] get raw data\n",
      "Qx: tensor([[0.6595, 0.7833, 0.6334]], device='cuda:0')\n",
      "Qy: tensor([[0.6027, 0.6659, 0.7592]], device='cuda:0')\n",
      "Qz: tensor([[0.8231, 0.6281, 0.5726]], device='cuda:0')\n",
      "Qyaw: tensor([[0.5920, 0.6692, 0.6361]], device='cuda:0')\n",
      "[MAX OUTPUT]\n",
      "[Q net] x: 1, y: 2, z: 0, yaw_ind: 1\n",
      "[SUCCESS] estimate actions from network\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[GRASP DISTANCE] 0.0\n",
      "[GRASP REWARD] R: 0.0\n",
      "[OVERALL REWARD] 0.0\n",
      "[ACTION TYPE]: 0\n",
      "[MOVE] move: [ 0.     0.015 -0.02   0.   ] move index: [1, 2, 0, 1]\n",
      "next move: tensor([ 0.0150,  0.0150, -0.0200,  0.0000], device='cuda:0') next move index: [2, 2, 0, 1]\n",
      "target_Qx: 0.7868954539299011 Qx: 0.7833094596862793\n",
      "target_Qy: 0.7691071033477783 Qy: 0.7591665387153625\n",
      "target_Qz: 0.7701265215873718 Qz: 0.8230738043785095\n",
      "target_Qyaw: 0.6813175678253174 Qyaw: 0.6692310571670532\n",
      "loss_rank: 0.00803920067846775\n",
      "priority: 0.008804493583738804\n",
      "[SUCCESS] append transition experience\n",
      "==== EXPERT MODE ENABLE BC: False ENABLE RL: False FULL TRAIN: False ====\n",
      "==== low level action taken: 11 N_pickable_item: 1 ====\n",
      "[SUCCESS] get raw data\n",
      "Qx: tensor([[0.5647, 0.5882, 0.7449]], device='cuda:0')\n",
      "Qy: tensor([[0.5743, 0.6587, 0.7257]], device='cuda:0')\n",
      "Qz: tensor([[0.7695, 0.5106, 0.4354]], device='cuda:0')\n",
      "Qyaw: tensor([[0.6030, 0.6980, 0.4737]], device='cuda:0')\n",
      "[MAX OUTPUT]\n",
      "[Q net] x: 2, y: 2, z: 0, yaw_ind: 1\n",
      "[SUCCESS] estimate actions from network\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[GRASP DISTANCE] 0.0\n",
      "[GRASP REWARD] R: 0.0\n",
      "[OVERALL REWARD] 0.0\n",
      "[ACTION TYPE]: 0\n",
      "[MOVE] move: [ 0.015  0.015 -0.02   0.   ] move index: [2, 2, 0, 1]\n",
      "next move: tensor([ 0.0150,  0.0150, -0.0200,  0.0000], device='cuda:0') next move index: [2, 2, 0, 1]\n",
      "target_Qx: 0.864682137966156 Qx: 0.7449082136154175\n",
      "target_Qy: 0.8762573003768921 Qy: 0.7256782054901123\n",
      "target_Qz: 0.906945526599884 Qz: 0.7695078253746033\n",
      "target_Qyaw: 0.9197157025337219 Qyaw: 0.6979996562004089\n",
      "loss_rank: 0.003170143812894821\n",
      "priority: 0.02943689003586769\n",
      "[SUCCESS] append transition experience\n",
      "==== EXPERT MODE ENABLE BC: False ENABLE RL: False FULL TRAIN: False ====\n",
      "==== low level action taken: 11 N_pickable_item: 1 ====\n",
      "[SUCCESS] get raw data\n",
      "Qx: tensor([[0.5190, 0.7412, 0.8250]], device='cuda:0')\n",
      "Qy: tensor([[0.8250, 0.8592, 0.8791]], device='cuda:0')\n",
      "Qz: tensor([[0.9049, 0.6787, 0.7501]], device='cuda:0')\n",
      "Qyaw: tensor([[0.6391, 0.9146, 0.7313]], device='cuda:0')\n",
      "[MAX OUTPUT]\n",
      "[Q net] x: 2, y: 2, z: 0, yaw_ind: 1\n",
      "[SUCCESS] estimate actions from network\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[GRASP DISTANCE] 0.0\n",
      "[GRASP REWARD] R: 0.0\n",
      "[OVERALL REWARD] 0.0\n",
      "[ACTION TYPE]: 0\n",
      "[MOVE] move: [ 0.015  0.015 -0.02   0.   ] move index: [2, 2, 0, 1]\n",
      "next move: tensor([ 0.0000,  0.0000, -0.0200,  0.0000], device='cuda:0') next move index: [1, 1, 0, 1]\n",
      "target_Qx: 0.8867290616035461 Qx: 0.825029194355011\n",
      "target_Qy: 0.9097074270248413 Qy: 0.8790779113769531\n",
      "target_Qz: 0.899516224861145 Qz: 0.9048709869384766\n",
      "target_Qyaw: 0.9121443033218384 Qyaw: 0.9146288633346558\n",
      "loss_rank: 0.011853913776576519\n",
      "priority: 0.013048885390162468\n",
      "[SUCCESS] append transition experience\n",
      "==== EXPERT MODE ENABLE BC: False ENABLE RL: False FULL TRAIN: False ====\n",
      "==== low level action taken: 11 N_pickable_item: 1 ====\n",
      "[SUCCESS] get raw data\n",
      "Qx: tensor([[0.5809, 0.8772, 0.7978]], device='cuda:0')\n",
      "Qy: tensor([[0.6842, 0.9126, 0.7278]], device='cuda:0')\n",
      "Qz: tensor([[0.9024, 0.6424, 0.6632]], device='cuda:0')\n",
      "Qyaw: tensor([[0.5423, 0.9220, 0.5793]], device='cuda:0')\n",
      "[MAX OUTPUT]\n",
      "[Q net] x: 1, y: 1, z: 0, yaw_ind: 1\n",
      "[SUCCESS] estimate actions from network\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[GRASP DISTANCE] 0.0\n",
      "[GRASP REWARD] R: 0.0\n",
      "[OVERALL REWARD] 0.0\n",
      "[ACTION TYPE]: 0\n",
      "[MOVE] move: [ 0.    0.   -0.02  0.  ] move index: [1, 1, 0, 1]\n",
      "next move: tensor([ 0.0000,  0.0000, -0.0200,  0.0000], device='cuda:0') next move index: [1, 1, 0, 1]\n",
      "target_Qx: 0.7364395260810852 Qx: 0.8771942257881165\n",
      "target_Qy: 0.6439060568809509 Qy: 0.912619948387146\n",
      "target_Qz: 0.8218632340431213 Qz: 0.9023850560188293\n",
      "target_Qyaw: 0.6945126056671143 Qyaw: 0.9220222234725952\n",
      "loss_rank: 0.0017178388079628348\n",
      "priority: 0.03928369656205177\n",
      "[SUCCESS] append transition experience\n",
      "==== EXPERT MODE ENABLE BC: False ENABLE RL: False FULL TRAIN: False ====\n",
      "==== low level action taken: 11 N_pickable_item: 1 ====\n",
      "[SUCCESS] get raw data\n",
      "Qx: tensor([[0.1356, 0.6837, 0.4432]], device='cuda:0')\n",
      "Qy: tensor([[0.3518, 0.6075, 0.4008]], device='cuda:0')\n",
      "Qz: tensor([[0.7880, 0.5332, 0.5770]], device='cuda:0')\n",
      "Qyaw: tensor([[0.1967, 0.6316, 0.2371]], device='cuda:0')\n",
      "[MAX OUTPUT]\n",
      "[Q net] x: 1, y: 1, z: 0, yaw_ind: 1\n",
      "[SUCCESS] estimate actions from network\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[GRASP] grasp something\n",
      "[GRIPPER STATUS] NON_CLOSE_NON_OPEN\n",
      "[GRASP DISTANCE] 0.0\n",
      "[SUCESS] picked an item!\n",
      "[GRASP REWARD] R: 1.0\n",
      "[OVERALL REWARD] 1.0\n",
      "[ACTION TYPE]: 0\n",
      "[MOVE] move: [ 0.    0.   -0.02  0.  ] move index: [1, 1, 0, 1]\n",
      "next move: tensor([ 0.0000,  0.0000, -0.0200,  0.0000], device='cuda:0') next move index: [1, 1, 0, 1]\n",
      "target_Qx: 1.0 Qx: 0.6836796402931213\n",
      "target_Qy: 1.0 Qy: 0.6074686646461487\n",
      "target_Qz: 1.0 Qz: 0.787973165512085\n",
      "target_Qyaw: 1.0 Qyaw: 0.6316418051719666\n",
      "loss_rank: 0.0\n",
      "priority: 0.10869564116001129\n",
      "[SUCCESS] append transition experience\n",
      "[SUCCESS] finish one episode\n",
      "=== end of action ===\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[SUCCESS] return home\n",
      "[SUCCESS] return home\n",
      "[SUCCESS] GRASP OR PUSH ACTION\n",
      "[GRASP SUCCESS RATE] 1.0%/-inf% [500]\n",
      "[SUCCESS] save agent data\n",
      "=== end of episode ===\n",
      "[SUCCESS] save agent data\n",
      "[SUCCESS] save agent data\n"
     ]
    }
   ],
   "source": [
    "agent.interact(max_episode = 10, \n",
    "               lla_mode = constants.BC_RL,\n",
    "               hld_mode = constants.DEMO_MODE,\n",
    "               is_eval = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.hld_mode = constants.DEMO_MODE\n",
    "agent.is_eval  = True\n",
    "agent.load_agent_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "completion rate (eval): 1.0%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x753529ad4640>]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "CR_rate = np.sum(agent.CR_eval)/agent.max_result_window_eval\n",
    "print(f\"completion rate (eval): {CR_rate*100.}%\")\n",
    "\n",
    "plt.plot(agent.CR_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AGS rate (eval): 100.0%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x753529b614f0>]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "AGS_rate = np.sum(np.array(agent.AGS_eval)*np.array(agent.CR_eval))/(np.array(agent.CR_eval) > 0).sum()\n",
    "print(f\"AGS rate (eval): {AGS_rate*100.}%\")\n",
    "\n",
    "plt.plot(agent.AGS_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ATC rate (eval): 12.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.0, 25.0)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAh8AAAGiCAYAAABH4aTnAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/TGe4hAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAiR0lEQVR4nO3df3RU1d3v8c/k15AAmZjEZIgkEJCKVUHlR4xQqw88QrQoaPtcuWjBevVqgwVpa0VFrFZj9d5etVJsu1qoFaT6XMHKUvpgwFBu+S2IqCC/lKAkKDQzEGAImX3/QMZGEiXJZB+S/X6tddbynLMz55u9XJkP+5yzt88YYwQAAGBJgtcFAAAAtxA+AACAVYQPAABgFeEDAABYRfgAAABWET4AAIBVhA8AAGAV4QMAAFhF+AAAAFYRPgAAgFXNCh9lZWUaNGiQunbtqpycHI0ePVpbtmxp0Obyyy+Xz+drsN1+++1xLRoAALRfzQofFRUVKi0t1cqVK7V48WLV1dXpyiuvVG1tbYN2t956q/bs2RPbHn/88bgWDQAA2q+k5jRetGhRg/3Zs2crJydH69at02WXXRY7npaWpmAwGJ8KAQBAh9Ks8PFloVBIkpSZmdng+Jw5c/T8888rGAxq1KhRmjZtmtLS0hr9jEgkokgkEtuPRqPav3+/srKy5PP5WlMeAACwxBijAwcOKC8vTwkJX3NjxbRQfX29ufrqq82QIUMaHP/tb39rFi1aZDZu3Gief/55c9ZZZ5kxY8Y0+TnTp083ktjY2NjY2Ng6wFZZWfm1GcJnjDFqgTvuuEOvv/66li9fru7duzfZbsmSJRo2bJi2bdum3r17n3T+yyMfoVBIBQUFqqysVHp6ektKAwAAloXDYeXn56umpkaBQOAr27botsvEiRO1cOFCLVu27CuDhyQVFRVJUpPhw+/3y+/3n3Q8PT2d8AEAQDtzKo9MNCt8GGN05513av78+XrzzTdVWFj4tT+zYcMGSVK3bt2acykAANBBNSt8lJaWau7cuXrllVfUtWtXVVVVSZICgYBSU1O1fft2zZ07V1dddZWysrK0ceNG3XXXXbrsssvUr1+/NvkFAABA+9KsZz6aGkqZNWuWJkyYoMrKSt14443atGmTamtrlZ+frzFjxuj+++8/5Vso4XBYgUBAoVCI2y4AALQTzfn+bvZtl6+Sn5+vioqK5nwkAABwDGu7AAAAqwgfAADAKsIHAACwivABAACsInwAAACrCB8AAMAqwgcAALCK8AEAAKwifAAAAKsIHwAAwCrCBwAAsIrwAQAArCJ8AAAAqwgfAADAKsIHAACwivABAACsInwAAACrCB8AAMAqwgcAALCK8AEAAKwifAAAAKsIHwAAwCrCBwAAsIrwAQAArCJ8AAAAqwgfAADAKsIHAACwivABAACsInwAAACrCB8AAMAqwgcAALCK8AEAAKwifAAAAKsIHwAAwCrCBwAAsIrwAQAArCJ8AAAAqwgfAADAKsIHAACwivABAACsInwAAACrCB8AAMAqwgcAALCK8AEAAKwifAAAAKsIHwAAwCrCBwAAsIrwAQAArCJ8AAAAqwgfAADAKsIHAACwivABAACsInwAAACrCB8AAMAqwgcAALCK8AEAAKwifAAAAKsIHwAAwCrCBwAAsIrwAQAArCJ8AAAAqwgfAADAKsIHAACwqlnho6ysTIMGDVLXrl2Vk5Oj0aNHa8uWLQ3aHDlyRKWlpcrKylKXLl10/fXXq7q6Oq5FAwCA9qtZ4aOiokKlpaVauXKlFi9erLq6Ol155ZWqra2Ntbnrrrv06quv6qWXXlJFRYU++eQTXXfddXEvHAAAtE8+Y4xp6Q9/+umnysnJUUVFhS677DKFQiGdeeaZmjt3rr773e9KkjZv3qxzzz1XK1as0CWXXPK1nxkOhxUIBBQKhZSent7S0gAAgEXN+f5u1TMfoVBIkpSZmSlJWrdunerq6jR8+PBYm759+6qgoEArVqxo9DMikYjC4XCDDQAAdFwtDh/RaFSTJ0/WkCFDdP7550uSqqqqlJKSooyMjAZtc3NzVVVV1ejnlJWVKRAIxLb8/PyWlgQAANqBFoeP0tJSbdq0SfPmzWtVAVOnTlUoFIptlZWVrfo8AABwektqyQ9NnDhRCxcu1LJly9S9e/fY8WAwqKNHj6qmpqbB6Ed1dbWCwWCjn+X3++X3+1tSBgAAaIeaNfJhjNHEiRM1f/58LVmyRIWFhQ3ODxgwQMnJySovL48d27Jli3bt2qXi4uL4VAwAANq1Zo18lJaWau7cuXrllVfUtWvX2HMcgUBAqampCgQCuuWWWzRlyhRlZmYqPT1dd955p4qLi0/pTRcAANDxNetVW5/P1+jxWbNmacKECZKOTzL24x//WC+88IIikYhGjBih3/zmN03edvkyXrUFAKD9ac73d6vm+WgLhA8AANofa/N8AAAANBfhAwAAWEX4AAAAVhE+AACAVYQPAABgFeEDAABYRfgAAABWET4AAIBVhA8AAGAV4QMAAFhF+AAAAFYRPgAAgFWEDwAAYBXhAwAAWEX4AAAAVhE+AACAVYQPAABgFeEDAABYRfgAAABWET4AAIBVhA8AAGAV4QMAAFhF+AAAAFYRPgAAgFWEDwAAYBXhAwAAWEX4AAAAVhE+AACAVYQPAABgFeEDAABYRfgAAABWET4AAIBVhA8AAGAV4QMAAFhF+AAAAFYRPgAAgFWEDwAAYBXhAwAAWEX4AAAAVhE+AACAVYQPAABgFeEDAABYRfgAAABWET4AAIBVhA8AAGAV4QMAAFhF+AAAAFYRPgAAgFWEDwAAYBXhAwAAWEX4AAAAVhE+AACAVYQPAABgFeEDAABYRfgAAABWET4AAIBVhA8AAGAV4QMAAFhF+AAAAFYRPgAAgFWEDwAAYBXhAwAAWEX4AAAAVjU7fCxbtkyjRo1SXl6efD6fFixY0OD8hAkT5PP5GmwjR46MV70AAKCda3b4qK2tVf/+/TVjxowm24wcOVJ79uyJbS+88EKrigQAAB1HUnN/oKSkRCUlJV/Zxu/3KxgMtrgoAADQcbXJMx9vvvmmcnJydM455+iOO+7Qvn37mmwbiUQUDocbbAAAoOOKe/gYOXKknnvuOZWXl+uXv/ylKioqVFJSovr6+kbbl5WVKRAIxLb8/Px4lwQAAE4jPmOMafEP+3yaP3++Ro8e3WSbHTt2qHfv3nrjjTc0bNiwk85HIhFFIpHYfjgcVn5+vkKhkNLT01taGgAAsCgcDisQCJzS93ebv2rbq1cvZWdna9u2bY2e9/v9Sk9Pb7ABAICOq83Dx+7du7Vv3z5169atrS8FAADagWa/7XLw4MEGoxg7d+7Uhg0blJmZqczMTP385z/X9ddfr2AwqO3bt+vuu+/W2WefrREjRsS1cAAA0D41O3ysXbtWV1xxRWx/ypQpkqTx48dr5syZ2rhxo/70pz+ppqZGeXl5uvLKK/Xwww/L7/fHr2oAANButeqB07bQnAdWAADA6eG0euAUAADgXxE+AACAVYQPAABgFeEDAABYRfgAAABWET4AAIBVhA8AAGAV4QMAAFhF+AAAAFYRPgAAgFWEDwAAYBXhAwAAWEX4AAAAVhE+AACAVYQPAABgFeEDAABYRfgAAABWET4AAIBVhA8AAGAV4QMAAFhF+AAAAFYRPgAAgFWEDwAAYBXhAwAAWEX4AAAAVhE+AACAVYQPAABgFeEDAABYRfgAAABWET4AAIBVhA8AAGAV4QMAAFhF+AAAAFYRPgAAgFWEDwAAYBXhAwAAWJXkdQG2HKuPqip8RPVRox5Znb0uBwAAZzkz8vFxzWEN/eVSXfXU370uBQAApzkTPlKSjv+qR+ujHlcCAIDbnAkfyYnHf9W6eiNjjMfVAADgLmfCx4mRD4nRDwAAvORO+Ej8l/BxjPABAIBXnAwfdfXcdgEAwCvOhI+EBJ+SEnySGPkAAMBLzszzIUk3XtJDktQp2ZnMBQDAacep8PHgNed5XQIAAM5jCAAAAFjl1MhHzaGjihyLKiMtWf6kRK/LAQDASU6NfIye8f9U9Gi5Nu4OeV0KAADOcip8nJhorI63XQAA8IxT4ePEFOsRZjgFAMAzToUPRj4AAPCeU+HjxMgHa7sAAOAdp8KH/8TIB+EDAADPOBU+TqzvwvTqAAB4x6l5Pob2yVZ2F796ZnX2uhQAAJzlVPi4eUih1yUAAOA8p267AAAA7zk18nH0WFRHjtUrOSFBqSlMrw4AgBecGvl49LX31e/B/9KMpdu8LgUAAGc5FT6SE32SeNUWAAAvORU+TsxwGuFVWwAAPONW+Eg8/pwHM5wCAOCdZoePZcuWadSoUcrLy5PP59OCBQsanDfG6IEHHlC3bt2Umpqq4cOHa+vWrfGqt1WSkz6/7cLIBwAAnml2+KitrVX//v01Y8aMRs8//vjjevrpp/Xss89q1apV6ty5s0aMGKEjR460utjWSmFtFwAAPNfsV21LSkpUUlLS6DljjJ588kndf//9uvbaayVJzz33nHJzc7VgwQLdcMMNJ/1MJBJRJBKJ7YfD4eaWdMpSWNsFAADPxfWZj507d6qqqkrDhw+PHQsEAioqKtKKFSsa/ZmysjIFAoHYlp+fH8+SGuiV3UXf6ddNA3pkttk1AADAV4vrJGNVVVWSpNzc3AbHc3NzY+e+bOrUqZoyZUpsPxwOt1kAGdonW0P7ZLfJZwMAgFPj+Qynfr9ffr/f6zIAAIAlcb3tEgwGJUnV1dUNjldXV8fOea0+ahQ5Vu91GQAAOCuu4aOwsFDBYFDl5eWxY+FwWKtWrVJxcXE8L9UiS7fsVe97X9N3Zzb+/AkAAGh7zb7tcvDgQW3b9sXaKDt37tSGDRuUmZmpgoICTZ48Wb/4xS/Up08fFRYWatq0acrLy9Po0aPjWXeLxF61ZZ4PAAA80+zwsXbtWl1xxRWx/RMPi44fP16zZ8/W3XffrdraWt12222qqanR0KFDtWjRInXq1Cl+VbdQciKv2gIA4LVmh4/LL79cxpgmz/t8Pj300EN66KGHWlVYW2BtFwAAvOfU2i6sagsAgPecCh/+JKZXBwDAa06Fj9gzH9x2AQDAM55PMmZTF3+ShvXNUWpKotelAADgLKfCR1YXv/4wYZDXZQAA4DSnbrsAAADvORk+jDFf+bowAABoO06Fj2jU6Bv3v65e976mfx6q87ocAACc5FT4SEjwKRo1MoYp1gEA8IpT4UNiinUAALzmXPhginUAALzlbPjgtgsAAN5wL3xw2wUAAE+5Fz5Y3wUAAE85NcOpJA3scYbyM9PUxe/crw4AwGnBuW/gJ77X3+sSAABwmnO3XQAAgLcIHwAAwCrnwscP56zTeQ8s0isbPva6FAAAnORc+IjURVV7tF5H6uq9LgUAACc5Fz6YZAwAAG85Fz5OrO1ytN54XAkAAG5yLnww8gEAgLecCx+sagsAgLecCx9+Rj4AAPCUc+GjR1aaBvY4Q90yOnldCgAATvIZY06rJy/D4bACgYBCoZDS09O9LgcAAJyC5nx/OzfyAQAAvEX4AAAAVjkXPuat3qVBj7yhqS+/43UpAAA4ybnwcbQ+qk8PRBQ6fNTrUgAAcJJz4SMlkVdtAQDwknvh4/N5PiKEDwAAPOFc+GCGUwAAvOVc+GBtFwAAvOVe+IiNfJxWc6sBAOAM58JHIC1Z53ZLV2F2Z69LAQDASUleF2DbxQVn6PVJ3/K6DAAAnOXcyAcAAPAW4QMAAFjlXPjYte+QLn9iqUY+uczrUgAAcJJzz3xI0of7DiktJdHrMgAAcJJzIx/JST5JzPMBAIBXnAsfJ+b5OBY1ikaZ6wMAANucCx/JSV/8ykeZYh0AAOucCx8nRj4k1ncBAMALTocPnvsAAMA+5952SUjwqfeZnZWY4BOPfAAAYJ9z4UOSyn98udclAADgLOduuwAAAG8RPgAAgFVOho8Js1Zr+K8q9P6esNelAADgHCef+fho3yHt/KxWByPHvC4FAADnODnykZx4fIr1Ol61BQDAOifDR8rns5xGmGQMAADr3Awfn080xiRjAADY52T4SP48fDC9OgAA9jkZPk7cdmHkAwAA+5wMHzldO+msjFT5kxK9LgUAAOc4+art//6P/l6XAACAs5wc+QAAAN4hfAAAAKucDB+/rdiua55ZrjmrPvK6FAAAnBP38PHggw/K5/M12Pr27Rvvy7TKntARbdwd0ic1h70uBQAA57TJA6fnnXee3njjjS8uknR6Pdd64lXbunrjcSUAALinTVJBUlKSgsHgKbWNRCKKRCKx/XC47VeaZYZTAAC80ybPfGzdulV5eXnq1auXxo0bp127djXZtqysTIFAILbl5+e3RUkNnJjh9CgznAIAYF3cw0dRUZFmz56tRYsWaebMmdq5c6e+9a1v6cCBA422nzp1qkKhUGyrrKyMd0knYYZTAAC8E/fbLiUlJbH/7tevn4qKitSjRw+9+OKLuuWWW05q7/f75ff7413GVyJ8AADgnTZ/1TYjI0Pf+MY3tG3btra+1CnrnJKoM9KSlZbC9OoAANjW5q+hHDx4UNu3b9dNN93U1pc6ZTcMLtANgwu8LgMAACfFfeTjJz/5iSoqKvThhx/qH//4h8aMGaPExESNHTs23pcCAADtUNxHPnbv3q2xY8dq3759OvPMMzV06FCtXLlSZ555ZrwvBQAA2qG4h4958+bF+yPjbt1H+/XE37aoMLuzyq7r53U5AAA45fSaetSS8OFjWrljvw5GjnldCgAAznFyYTletQUAwDtOhw/WdgEAwD4nw0cya7sAAOAZJ8NHCmu7AADgGTfDR5JPklRH+AAAwDonw4c/KVEpSQmxERAAAGCPk6/a5mem6YNflHx9QwAAEHf80x8AAFhF+AAAAFY5GT7q6qO6ZfYa3fSHVaplllMAAKxy8pmPRJ9P5Zv3SpKO1NWrs9/JbgAAwBNOjnwkJPiUnHj8dVvm+gAAwC4nw4f0xSyndceYYh0AAJucDR+xxeXq6z2uBAAAt7gbPmLruzDyAQCATc6Gj2TWdwEAwBPOhg9/UoISE3w6RvgAAMAqZ98xXTzl20pM8HldBgAAznF25IPgAQCAN5wNHwAAwBvOho+n3tiq//nntVq1Y5/XpQAA4BRnw8faj/brb+9Wa/c/D3tdCgAATnE2fJyY56OOt10AALDK3fCRxDwfAAB4gfBxjPABAIBNzoaP2MJy9UyvDgCATc6GD0Y+AADwhrvhgwdOAQDwhM8Yc1rddwiHwwoEAgqFQkpPT2+z6xypq5d0PIQkMNspAACt0pzvb2fXdumUnOh1CQAAOMnZ2y4AAMAbzoaPpVv26q6/bNCfV3zodSkAADjF2fCx49NazV//sdZ8+E+vSwEAwCnOho+UxOMPmfK2CwAAdrkbPpjnAwAATzgbPk7McMraLgAA2OVs+GDkAwAAb7gbPhj5AADAE86Gj+QkplcHAMALzs5wemnvLL017d/VKdnZ/AUAgCecDR/+pET5k5hiHQAA2/hnPwAAsMrZkY/q8BE9Xb5V/qREPTDqm16XAwCAM5wd+Thw5JjmrNql//vWbq9LAQDAKc6GDz/zfAAA4Alnw8eJGU551RYAALucDR8nZjg9FjWKRo3H1QAA4A5nw0fy56vaSsxyCgCATc6GjxMjHxK3XgAAsMnZ8JGc8MWvzkOnAADY4+w8HwkJPi3/2RVKSUpQRlqK1+UAAOAMZ8OHJHU/I83rEgAAcI6zt10AAIA3nB75eLp8q/bXHtVtl/VSXkaq1+UAAOAEp0c+/rKmUrP/8aH2Hoh4XQoAAM5wOnwwxToAAPY5HT6YYh0AAPucDh8pjHwAAGAd4UNMrw4AgE1Oh48T67sw8gEAgD1Oh4+UpERJhA8AAGxyep6PsusuUN2xqLK7+r0uBQAAZ7TZyMeMGTPUs2dPderUSUVFRVq9enVbXarFzspIVc/szuridzqDAQBgVZuEj7/85S+aMmWKpk+frrfeekv9+/fXiBEjtHfv3ra4HAAAaEd8xhgT7w8tKirSoEGD9Mwzz0iSotGo8vPzdeedd+qee+5p0DYSiSgS+WKG0VAopIKCAlVWVio9PT3epTVQ/n61Xn37E73x/l5dlJ9x0vnbL++tIWdnS5LW7tyvp8q3NvlZE4b01LBzcyVJ7+yu0eOLtjTZdtwlBRp5fjdJ0gfVYT386vtNtr1+QHeNvugsSdKH+2o1bf6mJtt+58Ju+m8DCyRJe2oO6+7/3Nhk2yvPy9VNxT0lSftrj2rSC+ubbPvtc87U//hWL0nSwcgx3fHndU22vaRXlkr/7WxJ0rH6qG6etabJthf3OEN3/fs3YvsT/rha9dHG/3f85lnpmlpybmz/tj+v1eFIfaNtz87toumjzovt3/nCW6qprWu0bX5mmh697oLY/k9feltVoSONts1N9+t//ceFsf17X35HlfsPNdo2kJasZ/77xbH9h159V1urDzbaNtWfqN/dNDC2/9jrm/Xux6FG2yYk+PSnHwyO7f+fxR/orY/+2WhbSfrjzYNi89n8Zuk2rdi+r8m2M28aEBsF/MPyHXpz86dNtn3qhguV2eX47co/r/hQ//VudZNtf/ndfrHlC15cW6lXN3zSZNuHx5yvnlmdJUmvrP9Y/7lud5Nt7//OuTonePxvxKJNezRn5a4m2/505Dnq1z1DkrRk817NWr6zybaThvXRwMJMSdI/tn+mmUu3N9mWvxHH8TfiuPbyN6JndpoeHn3Bl3+01cLhsPLz81VTU6NAIPDVjU2cRSIRk5iYaObPn9/g+Pe//31zzTXXnNR++vTpRhIbGxsbGxtbB9gqKyu/NivE/WGHzz77TPX19crNzW1wPDc3V5s3bz6p/dSpUzVlypTYfjQa1f79+5WVlSWfzxfX2k6kMhujKq6jr+2hr+2hr+2hr+2JV18bY3TgwAHl5eV9bVvPn7T0+/3y+xu+bZKRkdGm10xPT+d/Zkvoa3voa3voa3voa3vi0ddfe7vlc3F/4DQ7O1uJiYmqrm54D7i6ulrBYDDelwMAAO1M3MNHSkqKBgwYoPLy8tixaDSq8vJyFRcXx/tyAACgnWmT2y5TpkzR+PHjNXDgQA0ePFhPPvmkamtrdfPNN7fF5U6Z3+/X9OnTT7rNg/ijr+2hr+2hr+2hr+3xoq/b5FVbSXrmmWf0xBNPqKqqShdeeKGefvppFRUVtcWlAABAO9Jm4QMAAKAxTi8sBwAA7CN8AAAAqwgfAADAKsIHAACwypnwMWPGDPXs2VOdOnVSUVGRVq9e7XVJ7V5ZWZkGDRqkrl27KicnR6NHj9aWLQ0Xyzpy5IhKS0uVlZWlLl266Prrrz9pAjo032OPPSafz6fJkyfHjtHX8fPxxx/rxhtvVFZWllJTU3XBBRdo7dq1sfPGGD3wwAPq1q2bUlNTNXz4cG3d2vSicmhcfX29pk2bpsLCQqWmpqp37956+OGH9a/vQdDXLbds2TKNGjVKeXl58vl8WrBgQYPzp9K3+/fv17hx45Senq6MjAzdcsstOniw8QXwmqWV68i1C/PmzTMpKSnmj3/8o3n33XfNrbfeajIyMkx1dbXXpbVrI0aMMLNmzTKbNm0yGzZsMFdddZUpKCgwBw8ejLW5/fbbTX5+vikvLzdr1641l1xyibn00ks9rLr9W716tenZs6fp16+fmTRpUuw4fR0f+/fvNz169DATJkwwq1atMjt27DB/+9vfzLZt22JtHnvsMRMIBMyCBQvM22+/ba655hpTWFhoDh8+7GHl7c8jjzxisrKyzMKFC83OnTvNSy+9ZLp06WKeeuqpWBv6uuVee+01c99995mXX37ZSDppwddT6duRI0ea/v37m5UrV5q///3v5uyzzzZjx45tdW1OhI/Bgweb0tLS2H59fb3Jy8szZWVlHlbV8ezdu9dIMhUVFcYYY2pqakxycrJ56aWXYm3ef/99I8msWLHCqzLbtQMHDpg+ffqYxYsXm29/+9ux8EFfx8/PfvYzM3To0CbPR6NREwwGzRNPPBE7VlNTY/x+v3nhhRdslNhhXH311eYHP/hBg2PXXXedGTdunDGGvo6nL4ePU+nb9957z0gya9asibV5/fXXjc/nMx9//HGr6unwt12OHj2qdevWafjw4bFjCQkJGj58uFasWOFhZR1PKBSSJGVmZkqS1q1bp7q6ugZ937dvXxUUFND3LVRaWqqrr766QZ9K9HU8/fWvf9XAgQP1ve99Tzk5Obrooov0+9//PnZ+586dqqqqatDXgUBARUVF9HUzXXrppSovL9cHH3wgSXr77be1fPlylZSUSKKv29Kp9O2KFSuUkZGhgQMHxtoMHz5cCQkJWrVqVauu7/mqtm3ts88+U319vXJzcxscz83N1ebNmz2qquOJRqOaPHmyhgwZovPPP1+SVFVVpZSUlJNWKc7NzVVVVZUHVbZv8+bN01tvvaU1a9acdI6+jp8dO3Zo5syZmjJliu69916tWbNGP/rRj5SSkqLx48fH+rOxvyn0dfPcc889CofD6tu3rxITE1VfX69HHnlE48aNkyT6ug2dSt9WVVUpJyenwfmkpCRlZma2uv87fPiAHaWlpdq0aZOWL1/udSkdUmVlpSZNmqTFixerU6dOXpfToUWjUQ0cOFCPPvqoJOmiiy7Spk2b9Oyzz2r8+PEeV9exvPjii5ozZ47mzp2r8847Txs2bNDkyZOVl5dHX3dwHf62S3Z2thITE0966r+6ulrBYNCjqjqWiRMnauHChVq6dKm6d+8eOx4MBnX06FHV1NQ0aE/fN9+6deu0d+9eXXzxxUpKSlJSUpIqKir09NNPKykpSbm5ufR1nHTr1k3f/OY3Gxw799xztWvXLkmK9Sd/U1rvpz/9qe655x7dcMMNuuCCC3TTTTfprrvuUllZmST6ui2dSt8Gg0Ht3bu3wfljx45p//79re7/Dh8+UlJSNGDAAJWXl8eORaNRlZeXq7i42MPK2j9jjCZOnKj58+dryZIlKiwsbHB+wIABSk5ObtD3W7Zs0a5du+j7Zho2bJjeeecdbdiwIbYNHDhQ48aNi/03fR0fQ4YMOemV8Q8++EA9evSQJBUWFioYDDbo63A4rFWrVtHXzXTo0CElJDT8GkpMTFQ0GpVEX7elU+nb4uJi1dTUaN26dbE2S5YsUTQabf1Csa16XLWdmDdvnvH7/Wb27NnmvffeM7fddpvJyMgwVVVVXpfWrt1xxx0mEAiYN9980+zZsye2HTp0KNbm9ttvNwUFBWbJkiVm7dq1pri42BQXF3tYdcfxr2+7GENfx8vq1atNUlKSeeSRR8zWrVvNnDlzTFpamnn++edjbR577DGTkZFhXnnlFbNx40Zz7bXX8vpnC4wfP96cddZZsVdtX375ZZOdnW3uvvvuWBv6uuUOHDhg1q9fb9avX28kmV/96ldm/fr15qOPPjLGnFrfjhw50lx00UVm1apVZvny5aZPnz68atscv/71r01BQYFJSUkxgwcPNitXrvS6pHZPUqPbrFmzYm0OHz5sfvjDH5ozzjjDpKWlmTFjxpg9e/Z4V3QH8uXwQV/Hz6uvvmrOP/984/f7Td++fc3vfve7Buej0aiZNm2ayc3NNX6/3wwbNsxs2bLFo2rbr3A4bCZNmmQKCgpMp06dTK9evcx9991nIpFIrA193XJLly5t9G/0+PHjjTGn1rf79u0zY8eONV26dDHp6enm5ptvNgcOHGh1bT5j/mUqOQAAgDbW4Z/5AAAApxfCBwAAsIrwAQAArCJ8AAAAqwgfAADAKsIHAACwivABAACsInwAAACrCB8AAMAqwgcAALCK8AEAAKz6/5Lq8LSA1wlmAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ATC_rate = np.sum(np.array(agent.ATC_eval)*np.array(agent.CR_eval))/(np.array(agent.CR_eval) > 0).sum()\n",
    "print(f\"ATC rate (eval): {ATC_rate}\")\n",
    "\n",
    "plt.plot(agent.ATC_eval, '--')\n",
    "plt.ylim([0, 25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(np.array(agent.CR_eval) > 0).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
