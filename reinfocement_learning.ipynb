{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import utils\n",
    "import torch\n",
    "import constants\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from env import Env\n",
    "from agent import Agent\n",
    "from torchsummary import summary\n",
    "from torch.distributions import Normal, Categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialise Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "workspace space: \n",
      "[[-0.26   0.04 ]\n",
      " [ 0.435  0.685]\n",
      " [ 0.     0.4  ]]\n"
     ]
    }
   ],
   "source": [
    "#initialise environment\n",
    "min_x, max_x =  -0.110 - 0.150,   -0.110 + 0.150\n",
    "min_y, max_y =   0.560 - 0.125,    0.560 + 0.125\n",
    "min_z, max_z =               0,              0.4 \n",
    "\n",
    "workspace_lim = np.asarray([[min_x, max_x], \n",
    "                            [min_y, max_y],\n",
    "                            [min_z, max_z]])\n",
    "\n",
    "print(f\"workspace space: \\n{workspace_lim}\")\n",
    "\n",
    "obj_dir = 'objects/blocks/'\n",
    "N_obj   = 2\n",
    "\n",
    "env = Env(obj_dir, N_obj, workspace_lim, cluttered_mode=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialise Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n",
      "[SUCCESS] initialise environment\n",
      "[SUCCESS] initialise networks\n",
      "[LOAD BUFFER] data_length: 3743\n",
      "[SUCCESS] load low-level buffer\n",
      "[LOAD BUFFER] data_length: 3665\n",
      "[SUCCESS] load low-level buffer\n",
      "[SUCCESS] load previous buffer\n",
      "[SUCCESS] initialise memory buffer\n"
     ]
    }
   ],
   "source": [
    "agent = Agent(env, \n",
    "              max_memory_size = 20000, \n",
    "              is_debug        = True, \n",
    "              N_batch         = 512, \n",
    "              N_batch_hld     = 256, \n",
    "              lr              = 1e-4, \n",
    "              hld_lr          = 1e-3,\n",
    "              tau             = 0.01,\n",
    "              tau_hld         = 0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gather RL Experience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SUCCESS] load agent data\n",
      "[SUCCESS] load grasp model\n",
      "[SUCCESS] load push model\n",
      "[SUCCESS] restart environment\n",
      "[SUCCESS] setup rgbd camera\n",
      "[SUCCESS] load item paths\n",
      "[SUCCESS] randomly choose items\n",
      "[SUCCESS] randomly choose item colors\n",
      "item 0: shape_0, path index: 6, pose: [-0.1005661750664203, 0.47927098638600674, 0.025, 3.1717983576868245, 2.977489530834028, 2.7946967954356317]\n",
      "item 1: shape_1, path index: 5, pose: [-0.09945951945514, 0.45262636142024315, 0.025, 5.6517914157396305, 1.9181682233971276, 1.3173230659254145]\n",
      "[SUCCESS] reset item 1 to working space\n",
      "[SUCCESS] reset items to workplaces if any\n",
      "[SUCCESS] add items to simulation\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "==== episode: 0 ====\n",
      "[SUCCESS] home already\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "axis_norm_list[i]: 0.050002956941481776/0.07\n",
      "axis_norm_list[i]: 0.05000295694148173/0.07\n",
      "[GRASP GUIDANCE] home2grasp- linear: 0.028966948632817815, angular -3.4255902629902604\n",
      "[SUCCESS] home already\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[ACTION CHOICE] grasp!\n",
      "N_step_low_level: 7\n",
      "==== expert mode: False ====\n",
      "==== step: 0 N_pickable_item: 2 ====\n",
      "[SUCCESS] get raw data\n",
      "[SUCCESS] preprocess raw data\n",
      "[SUCCESS] estimate actions from network\n",
      "[SUCCESS] estimate actions from guidance\n",
      "[SUCCESS] normalise guidance action\n",
      "[SUCCESS] compute current q value\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[GRASP REWARD] R: 0.0\n",
      "[SUCCESS] interact with enviornment\n",
      "[STEP]: 1 [ACTION TYPE]: 0\n",
      "[REWARD]: 0.0\n",
      "[MOVE]: linear: [ 0.00348494  0.00021504 -0.02810332]\n",
      "[MOVE]: angular: 2.076512798666954\n",
      "[SUCCESS] get next raw data\n",
      "[SUCCESS] preprocess next raw data\n",
      "[SUCCESS] estimate next actions from network\n",
      "[SUCCESS] estimate next actions from guidance\n",
      "[SUCCESS] normalise next actions from guidance\n",
      "[LOSS] actor_loss: None, critic_loss: 0.003761667525395751\n",
      "[SUCCESS] append transition experience\n",
      "==== expert mode: False ====\n",
      "==== step: 1 N_pickable_item: 2 ====\n",
      "[SUCCESS] get raw data\n",
      "[SUCCESS] preprocess raw data\n",
      "[SUCCESS] estimate actions from network\n",
      "[SUCCESS] estimate actions from guidance\n",
      "[SUCCESS] normalise guidance action\n",
      "[SUCCESS] compute current q value\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[GRASP REWARD] R: 0.0\n",
      "[SUCCESS] interact with enviornment\n",
      "[STEP]: 2 [ACTION TYPE]: 0\n",
      "[REWARD]: 0.0\n",
      "[MOVE]: linear: [ 0.00553599  0.00771109 -0.02374701]\n",
      "[MOVE]: angular: 8.333481252193451\n",
      "[SUCCESS] get next raw data\n",
      "[SUCCESS] preprocess next raw data\n",
      "[SUCCESS] estimate next actions from network\n",
      "[SUCCESS] estimate next actions from guidance\n",
      "[SUCCESS] normalise next actions from guidance\n",
      "[LOSS] actor_loss: None, critic_loss: 0.0005362788215279579\n",
      "[SUCCESS] append transition experience\n",
      "==== expert mode: False ====\n",
      "==== step: 2 N_pickable_item: 2 ====\n",
      "[SUCCESS] get raw data\n",
      "[SUCCESS] preprocess raw data\n",
      "[SUCCESS] estimate actions from network\n",
      "[SUCCESS] estimate actions from guidance\n",
      "[SUCCESS] normalise guidance action\n",
      "[SUCCESS] compute current q value\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[GRASP REWARD] R: 0.0\n",
      "[SUCCESS] interact with enviornment\n",
      "[STEP]: 3 [ACTION TYPE]: 0\n",
      "[REWARD]: 0.0\n",
      "[MOVE]: linear: [ 0.01093847  0.00941967 -0.01562964]\n",
      "[MOVE]: angular: 3.1138879805803295\n",
      "[SUCCESS] get next raw data\n",
      "[SUCCESS] preprocess next raw data\n",
      "[SUCCESS] estimate next actions from network\n",
      "[SUCCESS] estimate next actions from guidance\n",
      "[SUCCESS] normalise next actions from guidance\n",
      "[LOSS] actor_loss: None, critic_loss: 0.01348665077239275\n",
      "[SUCCESS] append transition experience\n",
      "==== expert mode: False ====\n",
      "==== step: 3 N_pickable_item: 2 ====\n",
      "[SUCCESS] get raw data\n",
      "[SUCCESS] preprocess raw data\n",
      "[SUCCESS] estimate actions from network\n",
      "[SUCCESS] estimate actions from guidance\n",
      "[SUCCESS] normalise guidance action\n",
      "[SUCCESS] compute current q value\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[GRASP REWARD] R: 0.0\n",
      "[SUCCESS] interact with enviornment\n",
      "[STEP]: 4 [ACTION TYPE]: 0\n",
      "[REWARD]: 0.0\n",
      "[MOVE]: linear: [-0.0026995   0.00920082 -0.02426255]\n",
      "[MOVE]: angular: 3.8061904907226562\n",
      "[SUCCESS] get next raw data\n",
      "[SUCCESS] preprocess next raw data\n",
      "[SUCCESS] estimate next actions from network\n",
      "[SUCCESS] estimate next actions from guidance\n",
      "[SUCCESS] normalise next actions from guidance\n",
      "[LOSS] actor_loss: None, critic_loss: 0.003837841097265482\n",
      "[SUCCESS] append transition experience\n",
      "==== expert mode: False ====\n",
      "==== step: 4 N_pickable_item: 2 ====\n",
      "[SUCCESS] get raw data\n",
      "[SUCCESS] preprocess raw data\n",
      "[SUCCESS] estimate actions from network\n",
      "[SUCCESS] estimate actions from guidance\n",
      "[SUCCESS] normalise guidance action\n",
      "[SUCCESS] compute current q value\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[GRASP REWARD] R: 0.0\n",
      "[SUCCESS] interact with enviornment\n",
      "[STEP]: 5 [ACTION TYPE]: 0\n",
      "[REWARD]: 0.0\n",
      "[MOVE]: linear: [-0.00825174  0.00716546 -0.02659751]\n",
      "[MOVE]: angular: 4.16131004691124\n",
      "[SUCCESS] get next raw data\n",
      "[SUCCESS] preprocess next raw data\n",
      "[SUCCESS] estimate next actions from network\n",
      "[SUCCESS] estimate next actions from guidance\n",
      "[SUCCESS] normalise next actions from guidance\n",
      "[LOSS] actor_loss: None, critic_loss: 0.012467402964830399\n",
      "[SUCCESS] append transition experience\n",
      "==== expert mode: False ====\n",
      "==== step: 5 N_pickable_item: 2 ====\n",
      "[SUCCESS] get raw data\n",
      "[SUCCESS] preprocess raw data\n",
      "[SUCCESS] estimate actions from network\n",
      "[SUCCESS] estimate actions from guidance\n",
      "[SUCCESS] normalise guidance action\n",
      "[SUCCESS] compute current q value\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[GRASP REWARD] R: 0.0\n",
      "[SUCCESS] interact with enviornment\n",
      "[STEP]: 6 [ACTION TYPE]: 0\n",
      "[REWARD]: 0.0\n",
      "[MOVE]: linear: [-0.00674981  0.01542273 -0.02709167]\n",
      "[MOVE]: angular: 5.757598131895064\n",
      "[SUCCESS] get next raw data\n",
      "[SUCCESS] preprocess next raw data\n",
      "[SUCCESS] estimate next actions from network\n",
      "[SUCCESS] estimate next actions from guidance\n",
      "[SUCCESS] normalise next actions from guidance\n",
      "[LOSS] actor_loss: None, critic_loss: 0.010839186608791351\n",
      "[SUCCESS] append transition experience\n",
      "==== expert mode: False ====\n",
      "==== step: 6 N_pickable_item: 2 ====\n",
      "[SUCCESS] get raw data\n",
      "[SUCCESS] preprocess raw data\n",
      "[SUCCESS] estimate actions from network\n",
      "[SUCCESS] estimate actions from guidance\n",
      "[SUCCESS] normalise guidance action\n",
      "[SUCCESS] compute current q value\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[GRASP REWARD] R: 0.0\n",
      "[SUCCESS] interact with enviornment\n",
      "[STEP]: 7 [ACTION TYPE]: 0\n",
      "[REWARD]: 0.0\n",
      "[MOVE]: linear: [ 0.00274873  0.01992218 -0.02954066]\n",
      "[MOVE]: angular: 11.377597153186796\n",
      "[SUCCESS] get next raw data\n",
      "[SUCCESS] preprocess next raw data\n",
      "[SUCCESS] estimate next actions from network\n",
      "[SUCCESS] estimate next actions from guidance\n",
      "[SUCCESS] normalise next actions from guidance\n",
      "[LOSS] actor_loss: None, critic_loss: 0.0005499091348610818\n",
      "[SUCCESS] append transition experience\n",
      "[SUCCESS] finish actions or grasp an item\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[SUCCESS] return home position\n",
      "=== end of action ===\n",
      "[BUFFER] data saved 3744/20000\n",
      "[SUCCESS] store transition experience for low-level action\n",
      "[BUFFER] data saved 3745/20000\n",
      "[SUCCESS] store transition experience for low-level action\n",
      "[BUFFER] data saved 3746/20000\n",
      "[SUCCESS] store transition experience for low-level action\n",
      "[BUFFER] data saved 3747/20000\n",
      "[SUCCESS] store transition experience for low-level action\n",
      "[BUFFER] data saved 3748/20000\n",
      "[SUCCESS] store transition experience for low-level action\n",
      "[BUFFER] data saved 3749/20000\n",
      "[SUCCESS] store transition experience for low-level action\n",
      "[BUFFER] data saved 3750/20000\n",
      "[SUCCESS] store transition experience for low-level action\n",
      "[CRITIC UPDATE] critic1_loss: 0.016317199915647507, critic2_loss: 0.010799798183143139\n",
      "[ACTOR UPDATE] bc_lambda: 0.9985010869926714\n",
      "[ACTOR UPDATE] min Q mean: -0.07466797530651093 exploration loss: 0.0943215861916542\n",
      "[ACTOR UPDATE] actor_loss: 0.02053409442305565, rl_loss: 0.019653603434562683, bc_loss: 0.02053537219762802 ce_loss: 0.012087970972061157\n",
      "[SUCCESS] update experience priorities\n",
      "[ERROR] len(actor_loss.shape) != len(critic_loss.shape)\n",
      "actor_loss.shape:  (512,) critic_loss.shape:  (512, 1)\n",
      "[SUCCESS] update experience priorities\n",
      "[SUCCESS] online update\n",
      "[SUCCESS] reset items to workplaces if any\n",
      "[SUCCESS] save models\n",
      "[SUCCESS] save agent data\n",
      "==== episode: 0 ====\n",
      "[SUCCESS] home already\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "axis_norm_list[i]: 0.05000295694148175/0.07\n",
      "axis_norm_list[i]: 0.05000295694148171/0.07\n",
      "[GRASP GUIDANCE] home2grasp- linear: 0.029821671447582687, angular -3.3512955022115922\n",
      "[SUCCESS] home already\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[ACTION CHOICE] grasp!\n",
      "N_step_low_level: 7\n",
      "==== expert mode: True ====\n",
      "==== step: 7 N_pickable_item: 2 ====\n",
      "[SUCCESS] get raw data\n",
      "[SUCCESS] preprocess raw data\n",
      "[SUCCESS] estimate actions from network\n",
      "[SUCCESS] estimate actions from guidance\n",
      "[SUCCESS] normalise guidance action\n",
      "[SUCCESS] compute current q value\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[GRASP REWARD] R: 0.0\n",
      "[SUCCESS] interact with enviornment\n",
      "[STEP]: 8 [ACTION TYPE]: 0\n",
      "[REWARD]: 0.0\n",
      "[MOVE]: linear: [-0.00265533  0.00269691 -0.02958053]\n",
      "[MOVE]: angular: -3.351295232772827\n",
      "[SUCCESS] get next raw data\n",
      "[SUCCESS] preprocess next raw data\n",
      "[SUCCESS] estimate next actions from network\n",
      "[SUCCESS] estimate next actions from guidance\n",
      "[SUCCESS] normalise next actions from guidance\n",
      "[LOSS] ce_loss : 0.0028539663180708885\n",
      "[LOSS] actor_loss: 0.013295566663146019, critic_loss: 0.006978141143918037\n",
      "[SUCCESS] append transition experience\n",
      "==== expert mode: True ====\n",
      "==== step: 8 N_pickable_item: 2 ====\n",
      "[SUCCESS] get raw data\n",
      "[SUCCESS] preprocess raw data\n",
      "[SUCCESS] estimate actions from network\n",
      "[SUCCESS] estimate actions from guidance\n",
      "[SUCCESS] normalise guidance action\n",
      "[SUCCESS] compute current q value\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[GRASP REWARD] R: 0.0\n",
      "[SUCCESS] interact with enviornment\n",
      "[STEP]: 9 [ACTION TYPE]: 0\n",
      "[REWARD]: 0.0\n",
      "[MOVE]: linear: [-0.00265533  0.00269691 -0.02958053]\n",
      "[MOVE]: angular: -3.351295232772827\n",
      "[SUCCESS] get next raw data\n",
      "[SUCCESS] preprocess next raw data\n",
      "[SUCCESS] estimate next actions from network\n",
      "[SUCCESS] estimate next actions from guidance\n",
      "[SUCCESS] normalise next actions from guidance\n",
      "[LOSS] ce_loss : 0.000608854868914932\n",
      "[LOSS] actor_loss: 0.008144150488078594, critic_loss: 0.006584902293980122\n",
      "[SUCCESS] append transition experience\n",
      "==== expert mode: True ====\n",
      "==== step: 9 N_pickable_item: 2 ====\n",
      "[SUCCESS] get raw data\n",
      "[SUCCESS] preprocess raw data\n",
      "[SUCCESS] estimate actions from network\n",
      "[SUCCESS] estimate actions from guidance\n",
      "[SUCCESS] normalise guidance action\n",
      "[SUCCESS] compute current q value\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[GRASP REWARD] R: 0.0\n",
      "[SUCCESS] interact with enviornment\n",
      "[STEP]: 10 [ACTION TYPE]: 0\n",
      "[REWARD]: 0.0\n",
      "[MOVE]: linear: [-0.00265533  0.00269691 -0.02958053]\n",
      "[MOVE]: angular: -3.351295232772827\n",
      "[SUCCESS] get next raw data\n",
      "[SUCCESS] preprocess next raw data\n",
      "[SUCCESS] estimate next actions from network\n",
      "[SUCCESS] estimate next actions from guidance\n",
      "[SUCCESS] normalise next actions from guidance\n",
      "[LOSS] ce_loss : 0.00040892345714382827\n",
      "[LOSS] actor_loss: 0.002177200512960553, critic_loss: 0.005181736312806606\n",
      "[SUCCESS] append transition experience\n",
      "==== expert mode: True ====\n",
      "==== step: 10 N_pickable_item: 2 ====\n",
      "[SUCCESS] get raw data\n",
      "[SUCCESS] preprocess raw data\n",
      "[SUCCESS] estimate actions from network\n",
      "[SUCCESS] estimate actions from guidance\n",
      "[SUCCESS] normalise guidance action\n",
      "[SUCCESS] compute current q value\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[GRASP REWARD] R: 0.0\n",
      "[SUCCESS] interact with enviornment\n",
      "[STEP]: 11 [ACTION TYPE]: 0\n",
      "[REWARD]: 0.0\n",
      "[MOVE]: linear: [-0.00265533  0.00269691 -0.02958053]\n",
      "[MOVE]: angular: -3.351295232772827\n",
      "[SUCCESS] get next raw data\n",
      "[SUCCESS] preprocess next raw data\n",
      "[SUCCESS] estimate next actions from network\n",
      "[SUCCESS] estimate next actions from guidance\n",
      "[SUCCESS] normalise next actions from guidance\n",
      "[LOSS] ce_loss : 0.0014323461800813675\n",
      "[LOSS] actor_loss: 0.007028491701930761, critic_loss: 0.009180539287626743\n",
      "[SUCCESS] append transition experience\n",
      "==== expert mode: True ====\n",
      "==== step: 11 N_pickable_item: 2 ====\n",
      "[SUCCESS] get raw data\n",
      "[SUCCESS] preprocess raw data\n",
      "[SUCCESS] estimate actions from network\n",
      "[SUCCESS] estimate actions from guidance\n",
      "[SUCCESS] normalise guidance action\n",
      "[SUCCESS] compute current q value\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[GRASP REWARD] R: 0.0\n",
      "[SUCCESS] interact with enviornment\n",
      "[STEP]: 12 [ACTION TYPE]: 0\n",
      "[REWARD]: 0.0\n",
      "[MOVE]: linear: [-0.00265533  0.00269691 -0.02958053]\n",
      "[MOVE]: angular: -3.351295232772827\n",
      "[SUCCESS] get next raw data\n",
      "[SUCCESS] preprocess next raw data\n",
      "[SUCCESS] estimate next actions from network\n",
      "[SUCCESS] estimate next actions from guidance\n",
      "[SUCCESS] normalise next actions from guidance\n",
      "[LOSS] ce_loss : 0.11585582047700882\n",
      "[LOSS] actor_loss: 0.13148081302642822, critic_loss: 0.020984167233109474\n",
      "[SUCCESS] append transition experience\n",
      "==== expert mode: True ====\n",
      "==== step: 12 N_pickable_item: 2 ====\n",
      "[SUCCESS] get raw data\n",
      "[SUCCESS] preprocess raw data\n",
      "[SUCCESS] estimate actions from network\n",
      "[SUCCESS] estimate actions from guidance\n",
      "[SUCCESS] normalise guidance action\n",
      "[SUCCESS] compute current q value\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[GRASP REWARD] R: 0.0\n",
      "[SUCCESS] interact with enviornment\n",
      "[STEP]: 13 [ACTION TYPE]: 0\n",
      "[REWARD]: 0.0\n",
      "[MOVE]: linear: [-0.00265533  0.00269691 -0.02958053]\n",
      "[MOVE]: angular: -3.351295232772827\n",
      "[SUCCESS] get next raw data\n",
      "[SUCCESS] preprocess next raw data\n",
      "[SUCCESS] estimate next actions from network\n",
      "[SUCCESS] estimate next actions from guidance\n",
      "[SUCCESS] normalise next actions from guidance\n",
      "[LOSS] ce_loss : 2.903444766998291\n",
      "[LOSS] actor_loss: 2.9337408542633057, critic_loss: 0.08649753034114838\n",
      "[SUCCESS] append transition experience\n",
      "==== expert mode: True ====\n",
      "==== step: 13 N_pickable_item: 2 ====\n",
      "[SUCCESS] get raw data\n",
      "[SUCCESS] preprocess raw data\n",
      "[SUCCESS] estimate actions from network\n",
      "[SUCCESS] estimate actions from guidance\n",
      "[SUCCESS] normalise guidance action\n",
      "[SUCCESS] compute current q value\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ryan/github_repository/vision_based_synergies_between_closed_loop_grasping_and_pushing/env.py:445: RuntimeWarning: invalid value encountered in divide\n",
      "  unit_dir  = move_vector/move_norm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[GRASP] grasp something\n",
      "[GRIPPER STATUS] NON_CLOSE_NON_OPEN\n",
      "[SUCESS] picked an item!\n",
      "[GRASP REWARD] R: 1.0\n",
      "[SUCCESS] interact with enviornment\n",
      "[STEP]: 14 [ACTION TYPE]: 0\n",
      "[REWARD]: 1.0\n",
      "[MOVE]: linear: [ 0.     0.    -0.063]\n",
      "[MOVE]: angular: 0.0\n",
      "[SUCCESS] get next raw data\n",
      "[SUCCESS] preprocess next raw data\n",
      "[SUCCESS] estimate next actions from network\n",
      "[SUCCESS] estimate next actions from guidance\n",
      "[SUCCESS] normalise next actions from guidance\n",
      "[LOSS] ce_loss : 0.002650797599926591\n",
      "[LOSS] actor_loss: 0.006091370712965727, critic_loss: 0.1599891185760498\n",
      "[SUCCESS] append transition experience\n",
      "[SUCCESS] finish actions or grasp an item\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[SUCCESS] return home position\n",
      "=== end of action ===\n",
      "[BUFFER] data saved 3666/20000\n",
      "[SUCCESS] store transition experience for low-level action\n",
      "[BUFFER] data saved 3667/20000\n",
      "[SUCCESS] store transition experience for low-level action\n",
      "[BUFFER] data saved 3668/20000\n",
      "[SUCCESS] store transition experience for low-level action\n",
      "[BUFFER] data saved 3669/20000\n",
      "[SUCCESS] store transition experience for low-level action\n",
      "[BUFFER] data saved 3670/20000\n",
      "[SUCCESS] store transition experience for low-level action\n",
      "[BUFFER] data saved 3671/20000\n",
      "[SUCCESS] store transition experience for low-level action\n",
      "[BUFFER] data saved 3672/20000\n",
      "[SUCCESS] store transition experience for low-level action\n",
      "[CRITIC UPDATE] critic1_loss: 0.0208527073264122, critic2_loss: 0.01718652993440628\n",
      "[ACTOR UPDATE] bc_lambda: 0.9984511619383217\n",
      "[ACTOR UPDATE] min Q mean: -0.0845593512058258 exploration loss: 0.09146419912576675\n",
      "[ACTOR UPDATE] actor_loss: 0.008064291439950466, rl_loss: 0.006904846057295799, bc_loss: 0.008066032081842422 ce_loss: 0.0005973934894427657\n",
      "[SUCCESS] update experience priorities\n",
      "[ERROR] len(actor_loss.shape) != len(critic_loss.shape)\n",
      "actor_loss.shape:  (512,) critic_loss.shape:  (512, 1)\n",
      "[SUCCESS] update experience priorities\n",
      "[SUCCESS] online update\n",
      "[SUCCESS] reset items to workplaces if any\n",
      "[SUCCESS] save models\n",
      "[SUCCESS] save agent data\n",
      "==== episode: 0 ====\n",
      "[SUCCESS] home already\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "axis_norm_list[i]: 0.10000011089379837/0.07\n",
      "axis_norm_list[i]: 0.025001500718551214/0.07\n",
      "[GRASP GUIDANCE] home2grasp- linear: 0.031213750013132705, angular -0.8601187261628407\n",
      "[SUCCESS] home already\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[ACTION CHOICE] grasp!\n",
      "N_step_low_level: 7\n",
      "==== expert mode: False ====\n",
      "==== step: 14 N_pickable_item: 1 ====\n",
      "[SUCCESS] get raw data\n",
      "[SUCCESS] preprocess raw data\n",
      "[SUCCESS] estimate actions from network\n",
      "[SUCCESS] estimate actions from guidance\n",
      "[SUCCESS] normalise guidance action\n",
      "[SUCCESS] compute current q value\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[GRASP REWARD] R: 0.0\n",
      "[SUCCESS] interact with enviornment\n",
      "[STEP]: 15 [ACTION TYPE]: 0\n",
      "[REWARD]: 0.0\n",
      "[MOVE]: linear: [ 0.01154368  0.00086574 -0.02634092]\n",
      "[MOVE]: angular: -6.638391315937041\n",
      "[SUCCESS] get next raw data\n",
      "[SUCCESS] preprocess next raw data\n",
      "[SUCCESS] estimate next actions from network\n",
      "[SUCCESS] estimate next actions from guidance\n",
      "[SUCCESS] normalise next actions from guidance\n",
      "[LOSS] actor_loss: None, critic_loss: 0.005557764787226915\n",
      "[SUCCESS] append transition experience\n",
      "==== expert mode: False ====\n",
      "==== step: 15 N_pickable_item: 1 ====\n",
      "[SUCCESS] get raw data\n",
      "[SUCCESS] preprocess raw data\n",
      "[SUCCESS] estimate actions from network\n",
      "[SUCCESS] estimate actions from guidance\n",
      "[SUCCESS] normalise guidance action\n",
      "[SUCCESS] compute current q value\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[GRASP REWARD] R: 0.0\n",
      "[SUCCESS] interact with enviornment\n",
      "[STEP]: 16 [ACTION TYPE]: 0\n",
      "[REWARD]: 0.0\n",
      "[MOVE]: linear: [ 0.00399441  0.00444933 -0.02963047]\n",
      "[MOVE]: angular: -2.958388477563858\n",
      "[SUCCESS] get next raw data\n",
      "[SUCCESS] preprocess next raw data\n",
      "[SUCCESS] estimate next actions from network\n",
      "[SUCCESS] estimate next actions from guidance\n",
      "[SUCCESS] normalise next actions from guidance\n",
      "[LOSS] actor_loss: None, critic_loss: 0.004968166816979647\n",
      "[SUCCESS] append transition experience\n",
      "==== expert mode: False ====\n",
      "==== step: 16 N_pickable_item: 1 ====\n",
      "[SUCCESS] get raw data\n",
      "[SUCCESS] preprocess raw data\n",
      "[SUCCESS] estimate actions from network\n",
      "[SUCCESS] estimate actions from guidance\n",
      "[SUCCESS] normalise guidance action\n",
      "[SUCCESS] compute current q value\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[GRASP REWARD] R: 0.0\n",
      "[SUCCESS] interact with enviornment\n",
      "[STEP]: 17 [ACTION TYPE]: 0\n",
      "[REWARD]: 0.0\n",
      "[MOVE]: linear: [ 0.00613632  0.00696379 -0.03184339]\n",
      "[MOVE]: angular: -5.648816674947738\n",
      "[SUCCESS] get next raw data\n",
      "[SUCCESS] preprocess next raw data\n",
      "[SUCCESS] estimate next actions from network\n",
      "[SUCCESS] estimate next actions from guidance\n",
      "[SUCCESS] normalise next actions from guidance\n",
      "[LOSS] actor_loss: None, critic_loss: 0.00873679481446743\n",
      "[SUCCESS] append transition experience\n",
      "==== expert mode: False ====\n",
      "==== step: 17 N_pickable_item: 1 ====\n",
      "[SUCCESS] get raw data\n",
      "[SUCCESS] preprocess raw data\n",
      "[SUCCESS] estimate actions from network\n",
      "[SUCCESS] estimate actions from guidance\n",
      "[SUCCESS] normalise guidance action\n",
      "[SUCCESS] compute current q value\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[GRASP REWARD] R: 0.0\n",
      "[SUCCESS] interact with enviornment\n",
      "[STEP]: 18 [ACTION TYPE]: 0\n",
      "[REWARD]: 0.0\n",
      "[MOVE]: linear: [ 0.00508341  0.01066203 -0.04069927]\n",
      "[MOVE]: angular: -2.741117253899574\n",
      "[SUCCESS] get next raw data\n",
      "[SUCCESS] preprocess next raw data\n",
      "[SUCCESS] estimate next actions from network\n",
      "[SUCCESS] estimate next actions from guidance\n",
      "[SUCCESS] normalise next actions from guidance\n",
      "[LOSS] actor_loss: None, critic_loss: 0.06678949296474457\n",
      "[SUCCESS] append transition experience\n",
      "==== expert mode: False ====\n",
      "==== step: 18 N_pickable_item: 1 ====\n",
      "[SUCCESS] get raw data\n",
      "[SUCCESS] preprocess raw data\n",
      "[SUCCESS] estimate actions from network\n",
      "[SUCCESS] estimate actions from guidance\n",
      "[SUCCESS] normalise guidance action\n",
      "[SUCCESS] compute current q value\n",
      "[GRIPPER STATUS] FULL cLOSE\n",
      "[GRASP REWARD] R: 0.0\n",
      "[SUCCESS] interact with enviornment\n",
      "[STEP]: 19 [ACTION TYPE]: 0\n",
      "[REWARD]: 0.0\n",
      "[MOVE]: linear: [-0.00122326  0.00821736 -0.04582108]\n",
      "[MOVE]: angular: 0.8153011836111544\n",
      "[SUCCESS] get next raw data\n",
      "[SUCCESS] preprocess next raw data\n",
      "[SUCCESS] estimate next actions from network\n",
      "[SUCCESS] estimate next actions from guidance\n",
      "[SUCCESS] normalise next actions from guidance\n",
      "[LOSS] actor_loss: None, critic_loss: 0.08491495996713638\n",
      "[SUCCESS] append transition experience\n",
      "==== expert mode: False ====\n",
      "==== step: 19 N_pickable_item: 1 ====\n",
      "[SUCCESS] get raw data\n",
      "[SUCCESS] preprocess raw data\n",
      "[SUCCESS] estimate actions from network\n",
      "[SUCCESS] estimate actions from guidance\n",
      "[SUCCESS] normalise guidance action\n",
      "[SUCCESS] compute current q value\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[GRASP REWARD] R: 0.0\n",
      "[SUCCESS] interact with enviornment\n",
      "[STEP]: 20 [ACTION TYPE]: 0\n",
      "[REWARD]: 0.0\n",
      "[MOVE]: linear: [ 0.01132166  0.01556476 -0.02492167]\n",
      "[MOVE]: angular: -12.098741233348845\n",
      "[SUCCESS] get next raw data\n",
      "[SUCCESS] preprocess next raw data\n",
      "[SUCCESS] estimate next actions from network\n",
      "[SUCCESS] estimate next actions from guidance\n",
      "[SUCCESS] normalise next actions from guidance\n",
      "[LOSS] actor_loss: None, critic_loss: 0.0006644871318712831\n",
      "[SUCCESS] append transition experience\n",
      "==== expert mode: False ====\n",
      "==== step: 20 N_pickable_item: 1 ====\n",
      "[SUCCESS] get raw data\n",
      "[SUCCESS] preprocess raw data\n",
      "[SUCCESS] estimate actions from network\n",
      "[SUCCESS] estimate actions from guidance\n",
      "[SUCCESS] normalise guidance action\n",
      "[SUCCESS] compute current q value\n",
      "[WARNING] adnomral gripper motion\n",
      "[GRASP REWARD] R: 0.0\n",
      "[SUCCESS] interact with enviornment\n",
      "[STEP]: 21 [ACTION TYPE]: 0\n",
      "[REWARD]: 0.0\n",
      "[MOVE]: linear: [ 0.00906802  0.01769943 -0.05433681]\n",
      "[MOVE]: angular: 0.3878458030521869\n",
      "[SUCCESS] get next raw data\n",
      "[SUCCESS] preprocess next raw data\n",
      "[SUCCESS] estimate next actions from network\n",
      "[SUCCESS] estimate next actions from guidance\n",
      "[SUCCESS] normalise next actions from guidance\n",
      "[LOSS] actor_loss: None, critic_loss: 0.055291056632995605\n",
      "[SUCCESS] append transition experience\n",
      "[SUCCESS] finish actions or grasp an item\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[SUCCESS] return home position\n",
      "=== end of action ===\n",
      "[BUFFER] data saved 3751/20000\n",
      "[SUCCESS] store transition experience for low-level action\n",
      "[BUFFER] data saved 3752/20000\n",
      "[SUCCESS] store transition experience for low-level action\n",
      "[BUFFER] data saved 3753/20000\n",
      "[SUCCESS] store transition experience for low-level action\n",
      "[BUFFER] data saved 3754/20000\n",
      "[SUCCESS] store transition experience for low-level action\n",
      "[BUFFER] data saved 3755/20000\n",
      "[SUCCESS] store transition experience for low-level action\n",
      "[BUFFER] data saved 3756/20000\n",
      "[SUCCESS] store transition experience for low-level action\n",
      "[BUFFER] data saved 3757/20000\n",
      "[SUCCESS] store transition experience for low-level action\n",
      "[CRITIC UPDATE] critic1_loss: 0.020333144813776016, critic2_loss: 0.013178990222513676\n",
      "[ACTOR UPDATE] bc_lambda: 0.9984012393802248\n",
      "[ACTOR UPDATE] min Q mean: -0.07218773663043976 exploration loss: 0.09205708652734756\n",
      "[ACTOR UPDATE] actor_loss: 0.05617750063538551, rl_loss: 0.019869346171617508, bc_loss: 0.05623382329940796 ce_loss: 0.04696826636791229\n",
      "[SUCCESS] update experience priorities\n",
      "[ERROR] len(actor_loss.shape) != len(critic_loss.shape)\n",
      "actor_loss.shape:  (512,) critic_loss.shape:  (512, 1)\n",
      "[SUCCESS] update experience priorities\n",
      "[SUCCESS] online update\n",
      "[SUCCESS] reset items to workplaces if any\n",
      "[SUCCESS] save models\n",
      "[SUCCESS] save agent data\n",
      "==== episode: 0 ====\n",
      "[SUCCESS] home already\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "axis_norm_list[i]: 0.10000011089379834/0.07\n",
      "axis_norm_list[i]: 0.025001500718551204/0.07\n",
      "[GRASP GUIDANCE] home2grasp- linear: 0.031683868126140234, angular -0.8776928739746537\n",
      "[SUCCESS] home already\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[ACTION CHOICE] grasp!\n",
      "N_step_low_level: 7\n",
      "==== expert mode: True ====\n",
      "==== step: 21 N_pickable_item: 1 ====\n",
      "[SUCCESS] get raw data\n",
      "[SUCCESS] preprocess raw data\n",
      "[SUCCESS] estimate actions from network\n",
      "[SUCCESS] estimate actions from guidance\n",
      "[SUCCESS] normalise guidance action\n",
      "[SUCCESS] compute current q value\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[GRASP REWARD] R: 0.0\n",
      "[SUCCESS] interact with enviornment\n",
      "[STEP]: 22 [ACTION TYPE]: 0\n",
      "[REWARD]: 0.0\n",
      "[MOVE]: linear: [ 0.00858755 -0.00599225 -0.02990342]\n",
      "[MOVE]: angular: -0.8776928186416626\n",
      "[SUCCESS] get next raw data\n",
      "[SUCCESS] preprocess next raw data\n",
      "[SUCCESS] estimate next actions from network\n",
      "[SUCCESS] estimate next actions from guidance\n",
      "[SUCCESS] normalise next actions from guidance\n",
      "[LOSS] ce_loss : 0.004040531348437071\n",
      "[LOSS] actor_loss: 0.009572017937898636, critic_loss: 0.022776097059249878\n",
      "[SUCCESS] append transition experience\n",
      "==== expert mode: True ====\n",
      "==== step: 22 N_pickable_item: 1 ====\n",
      "[SUCCESS] get raw data\n",
      "[SUCCESS] preprocess raw data\n",
      "[SUCCESS] estimate actions from network\n",
      "[SUCCESS] estimate actions from guidance\n",
      "[SUCCESS] normalise guidance action\n",
      "[SUCCESS] compute current q value\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[GRASP REWARD] R: 0.0\n",
      "[SUCCESS] interact with enviornment\n",
      "[STEP]: 23 [ACTION TYPE]: 0\n",
      "[REWARD]: 0.0\n",
      "[MOVE]: linear: [ 0.00858755 -0.00599225 -0.02990342]\n",
      "[MOVE]: angular: -0.8776928186416626\n",
      "[SUCCESS] get next raw data\n",
      "[SUCCESS] preprocess next raw data\n",
      "[SUCCESS] estimate next actions from network\n",
      "[SUCCESS] estimate next actions from guidance\n",
      "[SUCCESS] normalise next actions from guidance\n",
      "[LOSS] ce_loss : 0.003408101387321949\n",
      "[LOSS] actor_loss: 0.013802115805447102, critic_loss: 0.02453485317528248\n",
      "[SUCCESS] append transition experience\n",
      "==== expert mode: True ====\n",
      "==== step: 23 N_pickable_item: 1 ====\n",
      "[SUCCESS] get raw data\n",
      "[SUCCESS] preprocess raw data\n",
      "[SUCCESS] estimate actions from network\n",
      "[SUCCESS] estimate actions from guidance\n",
      "[SUCCESS] normalise guidance action\n",
      "[SUCCESS] compute current q value\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[GRASP REWARD] R: 0.0\n",
      "[SUCCESS] interact with enviornment\n",
      "[STEP]: 24 [ACTION TYPE]: 0\n",
      "[REWARD]: 0.0\n",
      "[MOVE]: linear: [ 0.00858755 -0.00599225 -0.02990342]\n",
      "[MOVE]: angular: -0.8776928186416626\n",
      "[SUCCESS] get next raw data\n",
      "[SUCCESS] preprocess next raw data\n",
      "[SUCCESS] estimate next actions from network\n",
      "[SUCCESS] estimate next actions from guidance\n",
      "[SUCCESS] normalise next actions from guidance\n",
      "[LOSS] ce_loss : 0.001568279112689197\n",
      "[LOSS] actor_loss: 0.009598080068826675, critic_loss: 0.024507474154233932\n",
      "[SUCCESS] append transition experience\n",
      "==== expert mode: True ====\n",
      "==== step: 24 N_pickable_item: 1 ====\n",
      "[SUCCESS] get raw data\n",
      "[SUCCESS] preprocess raw data\n",
      "[SUCCESS] estimate actions from network\n",
      "[SUCCESS] estimate actions from guidance\n",
      "[SUCCESS] normalise guidance action\n",
      "[SUCCESS] compute current q value\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[GRASP REWARD] R: 0.0\n",
      "[SUCCESS] interact with enviornment\n",
      "[STEP]: 25 [ACTION TYPE]: 0\n",
      "[REWARD]: 0.0\n",
      "[MOVE]: linear: [ 0.00858755 -0.00599225 -0.02990342]\n",
      "[MOVE]: angular: -0.8776928186416626\n",
      "[SUCCESS] get next raw data\n",
      "[SUCCESS] preprocess next raw data\n",
      "[SUCCESS] estimate next actions from network\n",
      "[SUCCESS] estimate next actions from guidance\n",
      "[SUCCESS] normalise next actions from guidance\n",
      "[LOSS] ce_loss : 0.0018077236600220203\n",
      "[LOSS] actor_loss: 0.014612548053264618, critic_loss: 0.022375011816620827\n",
      "[SUCCESS] append transition experience\n",
      "==== expert mode: True ====\n",
      "==== step: 25 N_pickable_item: 1 ====\n",
      "[SUCCESS] get raw data\n",
      "[SUCCESS] preprocess raw data\n",
      "[SUCCESS] estimate actions from network\n",
      "[SUCCESS] estimate actions from guidance\n",
      "[SUCCESS] normalise guidance action\n",
      "[SUCCESS] compute current q value\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[GRASP REWARD] R: 0.0\n",
      "[SUCCESS] interact with enviornment\n",
      "[STEP]: 26 [ACTION TYPE]: 0\n",
      "[REWARD]: 0.0\n",
      "[MOVE]: linear: [ 0.00858755 -0.00599225 -0.02990342]\n",
      "[MOVE]: angular: -0.8776928186416626\n",
      "[SUCCESS] get next raw data\n",
      "[SUCCESS] preprocess next raw data\n",
      "[SUCCESS] estimate next actions from network\n",
      "[SUCCESS] estimate next actions from guidance\n",
      "[SUCCESS] normalise next actions from guidance\n",
      "[LOSS] ce_loss : 0.037910234183073044\n",
      "[LOSS] actor_loss: 0.053075022995471954, critic_loss: 0.02061239816248417\n",
      "[SUCCESS] append transition experience\n",
      "==== expert mode: True ====\n",
      "==== step: 26 N_pickable_item: 1 ====\n",
      "[SUCCESS] get raw data\n",
      "[SUCCESS] preprocess raw data\n",
      "[SUCCESS] estimate actions from network\n",
      "[SUCCESS] estimate actions from guidance\n",
      "[SUCCESS] normalise guidance action\n",
      "[SUCCESS] compute current q value\n",
      "[GRIPPER STATUS] FULL OPEN\n",
      "[GRASP REWARD] R: 0.0\n",
      "[SUCCESS] interact with enviornment\n",
      "[STEP]: 27 [ACTION TYPE]: 0\n",
      "[REWARD]: 0.0\n",
      "[MOVE]: linear: [ 0.00858755 -0.00599225 -0.02990342]\n",
      "[MOVE]: angular: -0.8776928186416626\n",
      "[SUCCESS] get next raw data\n",
      "[SUCCESS] preprocess next raw data\n",
      "[SUCCESS] estimate next actions from network\n",
      "[SUCCESS] estimate next actions from guidance\n",
      "[SUCCESS] normalise next actions from guidance\n",
      "[LOSS] ce_loss : 3.451699733734131\n",
      "[LOSS] actor_loss: 3.50207257270813, critic_loss: 0.15065787732601166\n",
      "[SUCCESS] append transition experience\n",
      "==== expert mode: True ====\n",
      "==== step: 27 N_pickable_item: 1 ====\n",
      "[SUCCESS] get raw data\n",
      "[SUCCESS] preprocess raw data\n",
      "[SUCCESS] estimate actions from network\n",
      "[SUCCESS] estimate actions from guidance\n",
      "[SUCCESS] normalise guidance action\n",
      "[SUCCESS] compute current q value\n",
      "[GRASP] grasp something\n",
      "[GRIPPER STATUS] NON_CLOSE_NON_OPEN\n",
      "[SUCESS] picked an item!\n",
      "[GRASP REWARD] R: 1.0\n",
      "[SUCCESS] interact with enviornment\n",
      "[STEP]: 28 [ACTION TYPE]: 0\n",
      "[REWARD]: 1.0\n",
      "[MOVE]: linear: [ 0.     0.    -0.063]\n",
      "[MOVE]: angular: 0.0\n",
      "[SUCCESS] get next raw data\n",
      "[SUCCESS] preprocess next raw data\n",
      "[SUCCESS] estimate next actions from network\n",
      "[SUCCESS] estimate next actions from guidance\n",
      "[SUCCESS] normalise next actions from guidance\n",
      "[LOSS] ce_loss : 0.0005535738891921937\n",
      "[LOSS] actor_loss: 0.002584561938419938, critic_loss: 0.21615853905677795\n",
      "[SUCCESS] append transition experience\n",
      "[SUCCESS] finish one episode\n",
      "=== end of action ===\n",
      "[BUFFER] data saved 3673/20000\n",
      "[SUCCESS] store transition experience for low-level action\n",
      "[BUFFER] data saved 3674/20000\n",
      "[SUCCESS] store transition experience for low-level action\n",
      "[BUFFER] data saved 3675/20000\n",
      "[SUCCESS] store transition experience for low-level action\n",
      "[BUFFER] data saved 3676/20000\n",
      "[SUCCESS] store transition experience for low-level action\n",
      "[BUFFER] data saved 3677/20000\n",
      "[SUCCESS] store transition experience for low-level action\n",
      "[BUFFER] data saved 3678/20000\n",
      "[SUCCESS] store transition experience for low-level action\n",
      "[BUFFER] data saved 3679/20000\n",
      "[SUCCESS] store transition experience for low-level action\n",
      "[CRITIC UPDATE] critic1_loss: 0.01368732564151287, critic2_loss: 0.011483429931104183\n",
      "[ACTOR UPDATE] bc_lambda: 0.9983513193182558\n",
      "[ACTOR UPDATE] min Q mean: -0.07216339558362961 exploration loss: 0.09385257214307785\n",
      "[ACTOR UPDATE] actor_loss: 0.011300401762127876, rl_loss: 0.021689170971512794, bc_loss: 0.011283766478300095 ce_loss: 0.003564657876268029\n",
      "[SUCCESS] update experience priorities\n",
      "[ERROR] len(actor_loss.shape) != len(critic_loss.shape)\n",
      "actor_loss.shape:  (512,) critic_loss.shape:  (512, 1)\n",
      "[SUCCESS] update experience priorities\n",
      "[SUCCESS] online update\n",
      "[SUCCESS] reset items to workplaces if any\n",
      "[SUCCESS] save models\n",
      "[SUCCESS] save agent data\n"
     ]
    }
   ],
   "source": [
    "agent.interact_test(max_episode = 1, grasp_train = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
