{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import utils\n",
    "import torch\n",
    "import constants\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from env import Env\n",
    "from agent import Agent\n",
    "from torchsummary import summary\n",
    "from torch.distributions import Normal, Categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialise Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "workspace space: \n",
      "[[-0.26   0.04 ]\n",
      " [ 0.435  0.685]\n",
      " [ 0.     0.4  ]]\n"
     ]
    }
   ],
   "source": [
    "#initialise environment\n",
    "min_x, max_x =  -0.110 - 0.150,   -0.110 + 0.150\n",
    "min_y, max_y =   0.560 - 0.125,    0.560 + 0.125\n",
    "min_z, max_z =               0,              0.4 \n",
    "\n",
    "workspace_lim = np.asarray([[min_x, max_x], \n",
    "                            [min_y, max_y],\n",
    "                            [min_z, max_z]])\n",
    "\n",
    "print(f\"workspace space: \\n{workspace_lim}\")\n",
    "\n",
    "obj_dir = 'objects/blocks/'\n",
    "N_obj   = 2\n",
    "\n",
    "env = Env(obj_dir, N_obj, workspace_lim, cluttered_mode=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialise Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n",
      "[SUCCESS] initialise environment\n",
      "[SUCCESS] initialise networks\n",
      "[LOAD BUFFER] data_length: 10003\n",
      "[SUCCESS] load low-level buffer\n",
      "[SUCCESS] load previous buffer\n",
      "[SUCCESS] initialise memory buffer\n"
     ]
    }
   ],
   "source": [
    "agent = Agent(env, \n",
    "              max_memory_size = 10000, \n",
    "              is_debug        = True, \n",
    "              N_batch         = 256, \n",
    "              N_batch_hld     = 256, \n",
    "              lr              = 1e-3, \n",
    "              hld_lr          = 1e-3,\n",
    "              tau             = 0.01,\n",
    "              tau_hld         = 0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gather Demonstration Experience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SUCCESS] buffer is full\n"
     ]
    }
   ],
   "source": [
    "agent.gather_guidance_experience(train_grasp = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HLD-net clone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hld_exp = agent.buffer_replay_hld.get_experience()\n",
    "# print(f\"N_hld_exp: {len(hld_exp[0])}\")\n",
    "# hld_train_loader, hld_test_loader = agent.get_train_test_dataloader_hld_net(hld_exp, train_ratio=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# agent.behaviour_cloning_hld(hld_train_loader, hld_test_loader, agent.hld_net, agent.hld_net_target, num_epochs = 1000)\n",
    "\n",
    "# # hld_exp          = None\n",
    "# # hld_train_loader = None\n",
    "# # hld_test_loader  = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def behaviour_cloning_eval_hld(test_loader, hld_net, hld_net_target):\n",
    "    \n",
    "    #set network in evaluation mode\n",
    "    hld_net.eval()\n",
    "    \n",
    "    #set target q network in eval mode\n",
    "    hld_net_target.eval()\n",
    "\n",
    "    #initialise loss funciton\n",
    "    mse_loss  = nn.MSELoss()\n",
    "\n",
    "    #initialise mean loss value\n",
    "    mean_hld_loss = 0.\n",
    "\n",
    "    #initialise batch counter\n",
    "    batch_cnt = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        for states, next_states, target_action_types, next_target_action_types, rewards, dones in test_loader:            \n",
    "\n",
    "            states                   = states.to(agent.device)\n",
    "            next_states              = next_states.to(agent.device)\n",
    "            target_action_types      = target_action_types.to(agent.device)\n",
    "            next_target_action_types = next_target_action_types.to(agent.device)\n",
    "            rewards                  = rewards.unsqueeze(1).to(agent.device)\n",
    "            dones                    = dones.unsqueeze(1).to(agent.device)\n",
    "\n",
    "            # compute current q - values\n",
    "            batch_indices = torch.arange(states.size(0)).long().to(agent.device)\n",
    "            qs = hld_net(states)[batch_indices, target_action_types.long()]\n",
    "            qs = qs.unsqueeze(1)\n",
    "\n",
    "            # Compute target q - values\n",
    "            with torch.no_grad():\n",
    "                # next_qs   = torch.max(hld_net_target(next_states), dim=1)[0]\n",
    "                next_qs   = hld_net_target(next_states)[batch_indices, next_target_action_types.long()]\n",
    "                next_qs   = next_qs.unsqueeze(1)\n",
    "                target_qs = rewards + agent.gamma * (1 - dones) * next_qs\n",
    "\n",
    "            if agent.is_debug:\n",
    "                if len(qs.shape) != len(target_qs.shape):\n",
    "                    print(\"[ERROR] len(qs.shape) != len(target_qs.shape)\")\n",
    "                    break\n",
    "                else:\n",
    "                    if qs.shape[0] != target_qs.shape[0]:\n",
    "                        print(\"[ERROR] qs.shape[0] != target_qs.sahpe[0]\")\n",
    "                        break\n",
    "                    elif qs.shape[1] != target_qs.shape[1]:\n",
    "                        print(\"[ERROR] qs.shape[1] != target_qs.sahpe[1]\")\n",
    "                        break\n",
    "            \n",
    "            all_qs = hld_net(states)\n",
    "            print(\"=====\")\n",
    "            for i in range(all_qs.shape[0]):\n",
    "                print(all_qs[i].argmax(dim = 0).item(), \n",
    "                      int(target_action_types[i].item()), \n",
    "                      rewards[i].item(),\n",
    "                      dones[i].item(), \n",
    "                      qs[i] - target_qs[i])\n",
    "\n",
    "            hld_net_loss = mse_loss(qs, target_qs)\n",
    "\n",
    "            mean_hld_loss += hld_net_loss.item()\n",
    "        \n",
    "            batch_cnt += 1\n",
    "\n",
    "    mean_hld_loss /= batch_cnt\n",
    "\n",
    "    return mean_hld_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# agent.hld_net.load_checkpoint()\n",
    "# agent.hld_net_target.load_checkpoint()\n",
    "# behaviour_cloning_eval_hld(hld_train_loader, agent.hld_net, agent.hld_net_target)\n",
    "\n",
    "# # hld_exp          = None\n",
    "# # hld_train_loader = None\n",
    "# # hld_test_loader  = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Push Clone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N_push_exp: 10003\n"
     ]
    }
   ],
   "source": [
    "push_exp  = agent.buffer_replay.get_experience_by_action_type(constants.PUSH)\n",
    "print(f\"N_push_exp: {len(push_exp[0])}\")\n",
    "push_train_loader, push_test_loader = agent.get_train_test_dataloader( push_exp, is_grasp = False, train_ratio=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/100 \n",
      "[TRAIN] critic1 loss: 0.041686, critic2 loss: 0.051378, actor Loss: 0.065225 \n",
      "[EVAL] critic1 loss eval: 0.029747/inf critic2 loss eval: 0.035720/inf \n",
      "[EVAL] actor loss eval: 0.067653/inf \n",
      "[SUCCESS] save critic1 model!\n",
      "[SUCCESS] save critic2 model!\n",
      "[SUCCESS] save actor model!\n",
      "Epoch: 2/100 \n",
      "[TRAIN] critic1 loss: 0.024101, critic2 loss: 0.025333, actor Loss: 0.032747 \n",
      "[EVAL] critic1 loss eval: 0.015256/0.029747 critic2 loss eval: 0.015303/0.035720 \n",
      "[EVAL] actor loss eval: 0.032406/0.067653 \n",
      "[SUCCESS] save critic1 model!\n",
      "[SUCCESS] save critic2 model!\n",
      "[SUCCESS] save actor model!\n",
      "Epoch: 3/100 \n",
      "[TRAIN] critic1 loss: 0.023217, critic2 loss: 0.026433, actor Loss: 0.029535 \n",
      "[EVAL] critic1 loss eval: 0.014563/0.015256 critic2 loss eval: 0.018793/0.015303 \n",
      "[EVAL] actor loss eval: 0.073804/0.032406 \n",
      "[SUCCESS] save critic1 model!\n",
      "Epoch: 4/100 \n",
      "[TRAIN] critic1 loss: 0.021991, critic2 loss: 0.025824, actor Loss: 0.027613 \n",
      "[EVAL] critic1 loss eval: 0.013617/0.014563 critic2 loss eval: 0.021157/0.015303 \n",
      "[EVAL] actor loss eval: 0.030875/0.032406 \n",
      "[SUCCESS] save critic1 model!\n",
      "[SUCCESS] save actor model!\n",
      "Epoch: 5/100 \n",
      "[TRAIN] critic1 loss: 0.019996, critic2 loss: 0.027348, actor Loss: 0.028231 \n",
      "[EVAL] critic1 loss eval: 0.012629/0.013617 critic2 loss eval: 0.022555/0.015303 \n",
      "[EVAL] actor loss eval: 0.034568/0.030875 \n",
      "[SUCCESS] save critic1 model!\n",
      "Epoch: 6/100 \n",
      "[TRAIN] critic1 loss: 0.021891, critic2 loss: 0.024053, actor Loss: 0.024040 \n",
      "[EVAL] critic1 loss eval: 0.016195/0.012629 critic2 loss eval: 0.014115/0.015303 \n",
      "[EVAL] actor loss eval: 0.021152/0.030875 \n",
      "[SUCCESS] save critic2 model!\n",
      "[SUCCESS] save actor model!\n",
      "Epoch: 7/100 \n",
      "[TRAIN] critic1 loss: 0.021222, critic2 loss: 0.022330, actor Loss: 0.022266 \n",
      "[EVAL] critic1 loss eval: 0.036823/0.012629 critic2 loss eval: 0.013230/0.014115 \n",
      "[EVAL] actor loss eval: 0.021547/0.021152 \n",
      "[SUCCESS] save critic2 model!\n",
      "Epoch: 8/100 \n",
      "[TRAIN] critic1 loss: 0.022050, critic2 loss: 0.022748, actor Loss: 0.019969 \n",
      "[EVAL] critic1 loss eval: 0.015614/0.012629 critic2 loss eval: 0.012515/0.013230 \n",
      "[EVAL] actor loss eval: 0.022634/0.021152 \n",
      "[SUCCESS] save critic2 model!\n",
      "Epoch: 9/100 \n",
      "[TRAIN] critic1 loss: 0.021537, critic2 loss: 0.022136, actor Loss: 0.019444 \n",
      "[EVAL] critic1 loss eval: 0.015638/0.012629 critic2 loss eval: 0.012342/0.012515 \n",
      "[EVAL] actor loss eval: 0.023977/0.021152 \n",
      "[SUCCESS] save critic2 model!\n",
      "Epoch: 10/100 \n",
      "[TRAIN] critic1 loss: 0.021569, critic2 loss: 0.025316, actor Loss: 0.020332 \n",
      "[EVAL] critic1 loss eval: 0.018722/0.012629 critic2 loss eval: 0.019131/0.012342 \n",
      "[EVAL] actor loss eval: 0.019899/0.021152 \n",
      "[SUCCESS] save actor model!\n",
      "Epoch: 11/100 \n",
      "[TRAIN] critic1 loss: 0.019636, critic2 loss: 0.021038, actor Loss: 0.018851 \n",
      "[EVAL] critic1 loss eval: 0.010967/0.012629 critic2 loss eval: 0.013783/0.012342 \n",
      "[EVAL] actor loss eval: 0.023186/0.019899 \n",
      "[SUCCESS] save critic1 model!\n",
      "Epoch: 12/100 \n",
      "[TRAIN] critic1 loss: 0.019363, critic2 loss: 0.019266, actor Loss: 0.016941 \n",
      "[EVAL] critic1 loss eval: 0.011053/0.010967 critic2 loss eval: 0.011020/0.012342 \n",
      "[EVAL] actor loss eval: 0.020118/0.019899 \n",
      "[SUCCESS] save critic2 model!\n",
      "Epoch: 13/100 \n",
      "[TRAIN] critic1 loss: 0.019145, critic2 loss: 0.019087, actor Loss: 0.016766 \n",
      "[EVAL] critic1 loss eval: 0.011911/0.010967 critic2 loss eval: 0.012412/0.011020 \n",
      "[EVAL] actor loss eval: 0.026646/0.019899 \n",
      "Epoch: 14/100 \n",
      "[TRAIN] critic1 loss: 0.021835, critic2 loss: 0.022747, actor Loss: 0.017602 \n",
      "[EVAL] critic1 loss eval: 0.053386/0.010967 critic2 loss eval: 0.014369/0.011020 \n",
      "[EVAL] actor loss eval: 0.018650/0.019899 \n",
      "[SUCCESS] save actor model!\n",
      "Epoch: 15/100 \n",
      "[TRAIN] critic1 loss: 0.019841, critic2 loss: 0.019786, actor Loss: 0.016873 \n",
      "[EVAL] critic1 loss eval: 0.017086/0.010967 critic2 loss eval: 0.011611/0.011020 \n",
      "[EVAL] actor loss eval: 0.018884/0.018650 \n",
      "Epoch: 16/100 \n",
      "[TRAIN] critic1 loss: 0.019075, critic2 loss: 0.020327, actor Loss: 0.015630 \n",
      "[EVAL] critic1 loss eval: 0.012910/0.010967 critic2 loss eval: 0.016595/0.011020 \n",
      "[EVAL] actor loss eval: 0.014640/0.018650 \n",
      "[SUCCESS] save actor model!\n",
      "Epoch: 17/100 \n",
      "[TRAIN] critic1 loss: 0.018823, critic2 loss: 0.020169, actor Loss: 0.015555 \n",
      "[EVAL] critic1 loss eval: 0.011402/0.010967 critic2 loss eval: 0.023427/0.011020 \n",
      "[EVAL] actor loss eval: 0.014118/0.014640 \n",
      "[SUCCESS] save actor model!\n",
      "Epoch: 18/100 \n",
      "[TRAIN] critic1 loss: 0.018593, critic2 loss: 0.019317, actor Loss: 0.015778 \n",
      "[EVAL] critic1 loss eval: 0.011972/0.010967 critic2 loss eval: 0.011290/0.011020 \n",
      "[EVAL] actor loss eval: 0.015897/0.014118 \n",
      "Epoch: 19/100 \n",
      "[TRAIN] critic1 loss: 0.018296, critic2 loss: 0.018270, actor Loss: 0.015542 \n",
      "[EVAL] critic1 loss eval: 0.011628/0.010967 critic2 loss eval: 0.011057/0.011020 \n",
      "[EVAL] actor loss eval: 0.015085/0.014118 \n",
      "Epoch: 20/100 \n",
      "[TRAIN] critic1 loss: 0.018553, critic2 loss: 0.019203, actor Loss: 0.015767 \n",
      "[EVAL] critic1 loss eval: 0.011308/0.010967 critic2 loss eval: 0.013098/0.011020 \n",
      "[EVAL] actor loss eval: 0.029380/0.014118 \n",
      "Epoch: 21/100 \n",
      "[TRAIN] critic1 loss: 0.034172, critic2 loss: 0.019211, actor Loss: 0.014801 \n",
      "[EVAL] critic1 loss eval: 0.015641/0.010967 critic2 loss eval: 0.011899/0.011020 \n",
      "[EVAL] actor loss eval: 0.013646/0.014118 \n",
      "[SUCCESS] save actor model!\n",
      "Epoch: 22/100 \n",
      "[TRAIN] critic1 loss: 0.027628, critic2 loss: 0.020275, actor Loss: 0.014131 \n",
      "[EVAL] critic1 loss eval: 0.047807/0.010967 critic2 loss eval: 0.015157/0.011020 \n",
      "[EVAL] actor loss eval: 0.014874/0.013646 \n",
      "Epoch: 23/100 \n",
      "[TRAIN] critic1 loss: 0.022711, critic2 loss: 0.030997, actor Loss: 0.014255 \n",
      "[EVAL] critic1 loss eval: 0.012276/0.010967 critic2 loss eval: 0.013532/0.011020 \n",
      "[EVAL] actor loss eval: 0.015214/0.013646 \n",
      "Epoch: 24/100 \n",
      "[TRAIN] critic1 loss: 0.026193, critic2 loss: 0.019212, actor Loss: 0.015213 \n",
      "[EVAL] critic1 loss eval: 0.012780/0.010967 critic2 loss eval: 0.012983/0.011020 \n",
      "[EVAL] actor loss eval: 0.017439/0.013646 \n",
      "Epoch: 25/100 \n",
      "[TRAIN] critic1 loss: 0.018817, critic2 loss: 0.018667, actor Loss: 0.014379 \n",
      "[EVAL] critic1 loss eval: 0.011052/0.010967 critic2 loss eval: 0.011236/0.011020 \n",
      "[EVAL] actor loss eval: 0.035341/0.013646 \n",
      "Epoch: 26/100 \n",
      "[TRAIN] critic1 loss: 0.018589, critic2 loss: 0.018299, actor Loss: 0.016185 \n",
      "[EVAL] critic1 loss eval: 0.010859/0.010967 critic2 loss eval: 0.017935/0.011020 \n",
      "[EVAL] actor loss eval: 0.016415/0.013646 \n",
      "[SUCCESS] save critic1 model!\n",
      "Epoch: 27/100 \n",
      "[TRAIN] critic1 loss: 0.018452, critic2 loss: 0.018414, actor Loss: 0.015153 \n",
      "[EVAL] critic1 loss eval: 0.012579/0.010859 critic2 loss eval: 0.011469/0.011020 \n",
      "[EVAL] actor loss eval: 0.030314/0.013646 \n",
      "Epoch: 28/100 \n",
      "[TRAIN] critic1 loss: 0.020138, critic2 loss: 0.019157, actor Loss: 0.014992 \n",
      "[EVAL] critic1 loss eval: 0.015230/0.010859 critic2 loss eval: 0.025097/0.011020 \n",
      "[EVAL] actor loss eval: 0.016548/0.013646 \n",
      "Epoch: 29/100 \n",
      "[TRAIN] critic1 loss: 0.018591, critic2 loss: 0.018706, actor Loss: 0.015129 \n",
      "[EVAL] critic1 loss eval: 0.011451/0.010859 critic2 loss eval: 0.011704/0.011020 \n",
      "[EVAL] actor loss eval: 0.019797/0.013646 \n",
      "Epoch: 30/100 \n",
      "[TRAIN] critic1 loss: 0.017548, critic2 loss: 0.017994, actor Loss: 0.014413 \n",
      "[EVAL] critic1 loss eval: 0.010870/0.010859 critic2 loss eval: 0.010649/0.011020 \n",
      "[EVAL] actor loss eval: 0.026583/0.013646 \n",
      "[SUCCESS] save critic2 model!\n",
      "Epoch: 31/100 \n",
      "[TRAIN] critic1 loss: 0.017827, critic2 loss: 0.017586, actor Loss: 0.013857 \n",
      "[EVAL] critic1 loss eval: 0.010744/0.010859 critic2 loss eval: 0.010936/0.010649 \n",
      "[EVAL] actor loss eval: 0.021886/0.013646 \n",
      "[SUCCESS] save critic1 model!\n",
      "Epoch: 32/100 \n",
      "[TRAIN] critic1 loss: 0.017995, critic2 loss: 0.017730, actor Loss: 0.014119 \n",
      "[EVAL] critic1 loss eval: 0.011097/0.010744 critic2 loss eval: 0.012130/0.010649 \n",
      "[EVAL] actor loss eval: 0.015667/0.013646 \n",
      "Epoch: 33/100 \n",
      "[TRAIN] critic1 loss: 0.019373, critic2 loss: 0.018784, actor Loss: 0.013876 \n",
      "[EVAL] critic1 loss eval: 0.010888/0.010744 critic2 loss eval: 0.011091/0.010649 \n",
      "[EVAL] actor loss eval: 0.017173/0.013646 \n",
      "Epoch: 34/100 \n",
      "[TRAIN] critic1 loss: 0.017821, critic2 loss: 0.017749, actor Loss: 0.013029 \n",
      "[EVAL] critic1 loss eval: 0.011099/0.010744 critic2 loss eval: 0.011039/0.010649 \n",
      "[EVAL] actor loss eval: 0.021212/0.013646 \n",
      "Epoch: 35/100 \n",
      "[TRAIN] critic1 loss: 0.017464, critic2 loss: 0.017843, actor Loss: 0.012740 \n",
      "[EVAL] critic1 loss eval: 0.010939/0.010744 critic2 loss eval: 0.011321/0.010649 \n",
      "[EVAL] actor loss eval: 0.015011/0.013646 \n",
      "Epoch: 36/100 \n",
      "[TRAIN] critic1 loss: 0.017668, critic2 loss: 0.017647, actor Loss: 0.012291 \n",
      "[EVAL] critic1 loss eval: 0.010838/0.010744 critic2 loss eval: 0.010919/0.010649 \n",
      "[EVAL] actor loss eval: 0.016270/0.013646 \n",
      "Epoch: 37/100 \n",
      "[TRAIN] critic1 loss: 0.017889, critic2 loss: 0.018061, actor Loss: 0.012955 \n",
      "[EVAL] critic1 loss eval: 0.012249/0.010744 critic2 loss eval: 0.011988/0.010649 \n",
      "[EVAL] actor loss eval: 0.013787/0.013646 \n",
      "Epoch: 38/100 \n",
      "[TRAIN] critic1 loss: 0.018512, critic2 loss: 0.018038, actor Loss: 0.013036 \n",
      "[EVAL] critic1 loss eval: 0.010793/0.010744 critic2 loss eval: 0.012915/0.010649 \n",
      "[EVAL] actor loss eval: 0.017089/0.013646 \n",
      "Epoch: 39/100 \n",
      "[TRAIN] critic1 loss: 0.018150, critic2 loss: 0.017667, actor Loss: 0.012133 \n",
      "[EVAL] critic1 loss eval: 0.010719/0.010744 critic2 loss eval: 0.011247/0.010649 \n",
      "[EVAL] actor loss eval: 0.016271/0.013646 \n",
      "[SUCCESS] save critic1 model!\n",
      "Epoch: 40/100 \n",
      "[TRAIN] critic1 loss: 0.017407, critic2 loss: 0.017510, actor Loss: 0.012239 \n",
      "[EVAL] critic1 loss eval: 0.010703/0.010719 critic2 loss eval: 0.010929/0.010649 \n",
      "[EVAL] actor loss eval: 0.013844/0.013646 \n",
      "[SUCCESS] save critic1 model!\n",
      "Epoch: 41/100 \n",
      "[TRAIN] critic1 loss: 0.017662, critic2 loss: 0.017346, actor Loss: 0.012635 \n",
      "[EVAL] critic1 loss eval: 0.011055/0.010703 critic2 loss eval: 0.010617/0.010649 \n",
      "[EVAL] actor loss eval: 0.017020/0.013646 \n",
      "[SUCCESS] save critic2 model!\n",
      "Epoch: 42/100 \n",
      "[TRAIN] critic1 loss: 0.018175, critic2 loss: 0.017944, actor Loss: 0.012420 \n",
      "[EVAL] critic1 loss eval: 0.012293/0.010703 critic2 loss eval: 0.010889/0.010617 \n",
      "[EVAL] actor loss eval: 0.037643/0.013646 \n",
      "Epoch: 43/100 \n",
      "[TRAIN] critic1 loss: 0.018003, critic2 loss: 0.020172, actor Loss: 0.012058 \n",
      "[EVAL] critic1 loss eval: 0.011786/0.010703 critic2 loss eval: 0.018117/0.010617 \n",
      "[EVAL] actor loss eval: 0.019442/0.013646 \n",
      "Epoch: 44/100 \n",
      "[TRAIN] critic1 loss: 0.018944, critic2 loss: 0.018543, actor Loss: 0.011748 \n",
      "[EVAL] critic1 loss eval: 0.011366/0.010703 critic2 loss eval: 0.014258/0.010617 \n",
      "[EVAL] actor loss eval: 0.017765/0.013646 \n",
      "Epoch: 45/100 \n",
      "[TRAIN] critic1 loss: 0.017953, critic2 loss: 0.017913, actor Loss: 0.010801 \n",
      "[EVAL] critic1 loss eval: 0.010968/0.010703 critic2 loss eval: 0.011244/0.010617 \n",
      "[EVAL] actor loss eval: 0.033115/0.013646 \n",
      "Epoch: 46/100 \n",
      "[TRAIN] critic1 loss: 0.018410, critic2 loss: 0.017488, actor Loss: 0.010761 \n",
      "[EVAL] critic1 loss eval: 0.011200/0.010703 critic2 loss eval: 0.010376/0.010617 \n",
      "[EVAL] actor loss eval: 0.015624/0.013646 \n",
      "[SUCCESS] save critic2 model!\n",
      "Epoch: 47/100 \n",
      "[TRAIN] critic1 loss: 0.018279, critic2 loss: 0.017981, actor Loss: 0.011512 \n",
      "[EVAL] critic1 loss eval: 0.011643/0.010703 critic2 loss eval: 0.011502/0.010376 \n",
      "[EVAL] actor loss eval: 0.024715/0.013646 \n",
      "Epoch: 48/100 \n",
      "[TRAIN] critic1 loss: 0.018498, critic2 loss: 0.020150, actor Loss: 0.010941 \n",
      "[EVAL] critic1 loss eval: 0.011041/0.010703 critic2 loss eval: 0.012914/0.010376 \n",
      "[EVAL] actor loss eval: 0.019126/0.013646 \n",
      "Epoch: 49/100 \n",
      "[TRAIN] critic1 loss: 0.017762, critic2 loss: 0.021236, actor Loss: 0.010152 \n",
      "[EVAL] critic1 loss eval: 0.010995/0.010703 critic2 loss eval: 0.014530/0.010376 \n",
      "[EVAL] actor loss eval: 0.013869/0.013646 \n",
      "Epoch: 50/100 \n",
      "[TRAIN] critic1 loss: 0.017502, critic2 loss: 0.018303, actor Loss: 0.009570 \n",
      "[EVAL] critic1 loss eval: 0.012732/0.010703 critic2 loss eval: 0.011242/0.010376 \n",
      "[EVAL] actor loss eval: 0.016220/0.013646 \n",
      "Epoch: 51/100 \n",
      "[TRAIN] critic1 loss: 0.017388, critic2 loss: 0.019169, actor Loss: 0.008770 \n",
      "[EVAL] critic1 loss eval: 0.010886/0.010703 critic2 loss eval: 0.011882/0.010376 \n",
      "[EVAL] actor loss eval: 0.017179/0.013646 \n",
      "Epoch: 52/100 \n",
      "[TRAIN] critic1 loss: 0.017515, critic2 loss: 0.018007, actor Loss: 0.009592 \n",
      "[EVAL] critic1 loss eval: 0.010942/0.010703 critic2 loss eval: 0.011097/0.010376 \n",
      "[EVAL] actor loss eval: 0.017473/0.013646 \n",
      "Epoch: 53/100 \n",
      "[TRAIN] critic1 loss: 0.017380, critic2 loss: 0.017558, actor Loss: 0.009075 \n",
      "[EVAL] critic1 loss eval: 0.027794/0.010703 critic2 loss eval: 0.010722/0.010376 \n",
      "[EVAL] actor loss eval: 0.051393/0.013646 \n",
      "Epoch: 54/100 \n",
      "[TRAIN] critic1 loss: 0.017989, critic2 loss: 0.017629, actor Loss: 0.011660 \n",
      "[EVAL] critic1 loss eval: 0.011133/0.010703 critic2 loss eval: 0.011896/0.010376 \n",
      "[EVAL] actor loss eval: 0.019625/0.013646 \n",
      "Epoch: 55/100 \n",
      "[TRAIN] critic1 loss: 0.017453, critic2 loss: 0.017516, actor Loss: 0.008884 \n",
      "[EVAL] critic1 loss eval: 0.011153/0.010703 critic2 loss eval: 0.011002/0.010376 \n",
      "[EVAL] actor loss eval: 0.015204/0.013646 \n",
      "Epoch: 56/100 \n",
      "[TRAIN] critic1 loss: 0.018235, critic2 loss: 0.017283, actor Loss: 0.007991 \n",
      "[EVAL] critic1 loss eval: 0.011851/0.010703 critic2 loss eval: 0.010957/0.010376 \n",
      "[EVAL] actor loss eval: 0.015561/0.013646 \n",
      "Epoch: 57/100 \n",
      "[TRAIN] critic1 loss: 0.019467, critic2 loss: 0.017389, actor Loss: 0.007406 \n",
      "[EVAL] critic1 loss eval: 0.014505/0.010703 critic2 loss eval: 0.010817/0.010376 \n",
      "[EVAL] actor loss eval: 0.017175/0.013646 \n",
      "Epoch: 58/100 \n",
      "[TRAIN] critic1 loss: 0.019059, critic2 loss: 0.017926, actor Loss: 0.006892 \n",
      "[EVAL] critic1 loss eval: 0.011336/0.010703 critic2 loss eval: 0.011287/0.010376 \n",
      "[EVAL] actor loss eval: 0.017118/0.013646 \n",
      "Epoch: 59/100 \n",
      "[TRAIN] critic1 loss: 0.017830, critic2 loss: 0.017325, actor Loss: 0.007233 \n",
      "[EVAL] critic1 loss eval: 0.010925/0.010703 critic2 loss eval: 0.010678/0.010376 \n",
      "[EVAL] actor loss eval: 0.016999/0.013646 \n",
      "Epoch: 60/100 \n",
      "[TRAIN] critic1 loss: 0.017405, critic2 loss: 0.017094, actor Loss: 0.006197 \n",
      "[EVAL] critic1 loss eval: 0.011203/0.010703 critic2 loss eval: 0.010592/0.010376 \n",
      "[EVAL] actor loss eval: 0.016957/0.013646 \n",
      "Epoch: 61/100 \n",
      "[TRAIN] critic1 loss: 0.017608, critic2 loss: 0.016991, actor Loss: 0.005397 \n",
      "[EVAL] critic1 loss eval: 0.010901/0.010703 critic2 loss eval: 0.010476/0.010376 \n",
      "[EVAL] actor loss eval: 0.019087/0.013646 \n",
      "Epoch: 62/100 \n",
      "[TRAIN] critic1 loss: 0.017589, critic2 loss: 0.016922, actor Loss: 0.005453 \n",
      "[EVAL] critic1 loss eval: 0.011232/0.010703 critic2 loss eval: 0.011099/0.010376 \n",
      "[EVAL] actor loss eval: 0.018685/0.013646 \n",
      "Epoch: 63/100 \n",
      "[TRAIN] critic1 loss: 0.017486, critic2 loss: 0.016915, actor Loss: 0.004762 \n",
      "[EVAL] critic1 loss eval: 0.011164/0.010703 critic2 loss eval: 0.011507/0.010376 \n",
      "[EVAL] actor loss eval: 0.017457/0.013646 \n",
      "Epoch: 64/100 \n",
      "[TRAIN] critic1 loss: 0.017415, critic2 loss: 0.016822, actor Loss: 0.004642 \n",
      "[EVAL] critic1 loss eval: 0.011529/0.010703 critic2 loss eval: 0.010170/0.010376 \n",
      "[EVAL] actor loss eval: 0.020473/0.013646 \n",
      "[SUCCESS] save critic2 model!\n",
      "Epoch: 65/100 \n",
      "[TRAIN] critic1 loss: 0.018468, critic2 loss: 0.018706, actor Loss: 0.004555 \n",
      "[EVAL] critic1 loss eval: 0.012605/0.010703 critic2 loss eval: 0.015394/0.010170 \n",
      "[EVAL] actor loss eval: 0.018141/0.013646 \n",
      "Epoch: 66/100 \n",
      "[TRAIN] critic1 loss: 0.017484, critic2 loss: 0.017220, actor Loss: 0.004764 \n",
      "[EVAL] critic1 loss eval: 0.011894/0.010703 critic2 loss eval: 0.010773/0.010170 \n",
      "[EVAL] actor loss eval: 0.017006/0.013646 \n",
      "Epoch: 67/100 \n",
      "[TRAIN] critic1 loss: 0.017905, critic2 loss: 0.016652, actor Loss: 0.004766 \n",
      "[EVAL] critic1 loss eval: 0.010941/0.010703 critic2 loss eval: 0.009983/0.010170 \n",
      "[EVAL] actor loss eval: 0.019826/0.013646 \n",
      "[SUCCESS] save critic2 model!\n",
      "Epoch: 68/100 \n",
      "[TRAIN] critic1 loss: 0.018579, critic2 loss: 0.017284, actor Loss: 0.004231 \n",
      "[EVAL] critic1 loss eval: 0.011917/0.010703 critic2 loss eval: 0.012101/0.009983 \n",
      "[EVAL] actor loss eval: 0.014991/0.013646 \n",
      "Epoch: 69/100 \n",
      "[TRAIN] critic1 loss: 0.019373, critic2 loss: 0.017656, actor Loss: 0.003497 \n",
      "[EVAL] critic1 loss eval: 0.012232/0.010703 critic2 loss eval: 0.014746/0.009983 \n",
      "[EVAL] actor loss eval: 0.016512/0.013646 \n",
      "Epoch: 70/100 \n",
      "[TRAIN] critic1 loss: 0.020129, critic2 loss: 0.016796, actor Loss: 0.004043 \n",
      "[EVAL] critic1 loss eval: 0.013957/0.010703 critic2 loss eval: 0.014142/0.009983 \n",
      "[EVAL] actor loss eval: 0.020630/0.013646 \n",
      "Epoch: 71/100 \n",
      "[TRAIN] critic1 loss: 0.019435, critic2 loss: 0.020125, actor Loss: 0.005523 \n",
      "[EVAL] critic1 loss eval: 0.013904/0.010703 critic2 loss eval: 0.014305/0.009983 \n",
      "[EVAL] actor loss eval: 0.019962/0.013646 \n",
      "Epoch: 72/100 \n",
      "[TRAIN] critic1 loss: 0.018619, critic2 loss: 0.016855, actor Loss: 0.004740 \n",
      "[EVAL] critic1 loss eval: 0.012547/0.010703 critic2 loss eval: 0.011301/0.009983 \n",
      "[EVAL] actor loss eval: 0.017259/0.013646 \n",
      "Epoch: 73/100 \n",
      "[TRAIN] critic1 loss: 0.018765, critic2 loss: 0.015614, actor Loss: 0.003426 \n",
      "[EVAL] critic1 loss eval: 0.033682/0.010703 critic2 loss eval: 0.014826/0.009983 \n",
      "[EVAL] actor loss eval: 0.018073/0.013646 \n",
      "Epoch: 74/100 \n",
      "[TRAIN] critic1 loss: 0.017855, critic2 loss: 0.015286, actor Loss: 0.003616 \n",
      "[EVAL] critic1 loss eval: 0.023021/0.010703 critic2 loss eval: 0.013342/0.009983 \n",
      "[EVAL] actor loss eval: 0.017613/0.013646 \n",
      "Epoch: 75/100 \n",
      "[TRAIN] critic1 loss: 0.018669, critic2 loss: 0.015480, actor Loss: 0.003218 \n",
      "[EVAL] critic1 loss eval: 0.011973/0.010703 critic2 loss eval: 0.013877/0.009983 \n",
      "[EVAL] actor loss eval: 0.017180/0.013646 \n",
      "Epoch: 76/100 \n",
      "[TRAIN] critic1 loss: 0.018589, critic2 loss: 0.015838, actor Loss: 0.002578 \n",
      "[EVAL] critic1 loss eval: 0.012107/0.010703 critic2 loss eval: 0.012794/0.009983 \n",
      "[EVAL] actor loss eval: 0.017086/0.013646 \n",
      "Epoch: 77/100 \n",
      "[TRAIN] critic1 loss: 0.018344, critic2 loss: 0.015301, actor Loss: 0.002535 \n",
      "[EVAL] critic1 loss eval: 0.013278/0.010703 critic2 loss eval: 0.013254/0.009983 \n",
      "[EVAL] actor loss eval: 0.017265/0.013646 \n",
      "Epoch: 78/100 \n",
      "[TRAIN] critic1 loss: 0.017722, critic2 loss: 0.015331, actor Loss: 0.002752 \n",
      "[EVAL] critic1 loss eval: 0.015176/0.010703 critic2 loss eval: 0.012968/0.009983 \n",
      "[EVAL] actor loss eval: 0.017841/0.013646 \n",
      "Epoch: 79/100 \n",
      "[TRAIN] critic1 loss: 0.017322, critic2 loss: 0.015258, actor Loss: 0.002720 \n",
      "[EVAL] critic1 loss eval: 0.014567/0.010703 critic2 loss eval: 0.013706/0.009983 \n",
      "[EVAL] actor loss eval: 0.018284/0.013646 \n",
      "Epoch: 80/100 \n",
      "[TRAIN] critic1 loss: 0.016712, critic2 loss: 0.015681, actor Loss: 0.002880 \n",
      "[EVAL] critic1 loss eval: 0.013170/0.010703 critic2 loss eval: 0.011524/0.009983 \n",
      "[EVAL] actor loss eval: 0.017963/0.013646 \n",
      "Epoch: 81/100 \n",
      "[TRAIN] critic1 loss: 0.016753, critic2 loss: 0.015291, actor Loss: 0.003624 \n",
      "[EVAL] critic1 loss eval: 0.012950/0.010703 critic2 loss eval: 0.013109/0.009983 \n",
      "[EVAL] actor loss eval: 0.018758/0.013646 \n",
      "Epoch: 82/100 \n",
      "[TRAIN] critic1 loss: 0.017111, critic2 loss: 0.013974, actor Loss: 0.002985 \n",
      "[EVAL] critic1 loss eval: 0.015568/0.010703 critic2 loss eval: 0.011037/0.009983 \n",
      "[EVAL] actor loss eval: 0.016684/0.013646 \n",
      "Epoch: 83/100 \n",
      "[TRAIN] critic1 loss: 0.015764, critic2 loss: 0.014642, actor Loss: 0.002625 \n",
      "[EVAL] critic1 loss eval: 0.010534/0.010703 critic2 loss eval: 0.018538/0.009983 \n",
      "[EVAL] actor loss eval: 0.017248/0.013646 \n",
      "[SUCCESS] save critic1 model!\n",
      "Epoch: 84/100 \n",
      "[TRAIN] critic1 loss: 0.017659, critic2 loss: 0.017074, actor Loss: 0.002884 \n",
      "[EVAL] critic1 loss eval: 0.012249/0.010534 critic2 loss eval: 0.017623/0.009983 \n",
      "[EVAL] actor loss eval: 0.017860/0.013646 \n",
      "Epoch: 85/100 \n",
      "[TRAIN] critic1 loss: 0.016789, critic2 loss: 0.014739, actor Loss: 0.003084 \n",
      "[EVAL] critic1 loss eval: 0.015159/0.010534 critic2 loss eval: 0.012961/0.009983 \n",
      "[EVAL] actor loss eval: 0.016968/0.013646 \n",
      "Epoch: 86/100 \n",
      "[TRAIN] critic1 loss: 0.016493, critic2 loss: 0.014869, actor Loss: 0.002180 \n",
      "[EVAL] critic1 loss eval: 0.012009/0.010534 critic2 loss eval: 0.012606/0.009983 \n",
      "[EVAL] actor loss eval: 0.016821/0.013646 \n",
      "Epoch: 87/100 \n",
      "[TRAIN] critic1 loss: 0.015432, critic2 loss: 0.014092, actor Loss: 0.002427 \n",
      "[EVAL] critic1 loss eval: 0.012548/0.010534 critic2 loss eval: 0.012689/0.009983 \n",
      "[EVAL] actor loss eval: 0.016872/0.013646 \n",
      "Epoch: 88/100 \n",
      "[TRAIN] critic1 loss: 0.014684, critic2 loss: 0.014758, actor Loss: 0.002013 \n",
      "[EVAL] critic1 loss eval: 0.012440/0.010534 critic2 loss eval: 0.013243/0.009983 \n",
      "[EVAL] actor loss eval: 0.016996/0.013646 \n",
      "Epoch: 89/100 \n",
      "[TRAIN] critic1 loss: 0.016048, critic2 loss: 0.014616, actor Loss: 0.001847 \n",
      "[EVAL] critic1 loss eval: 0.010472/0.010534 critic2 loss eval: 0.010844/0.009983 \n",
      "[EVAL] actor loss eval: 0.017474/0.013646 \n",
      "[SUCCESS] save critic1 model!\n",
      "Epoch: 90/100 \n",
      "[TRAIN] critic1 loss: 0.015394, critic2 loss: 0.013596, actor Loss: 0.002216 \n",
      "[EVAL] critic1 loss eval: 0.010204/0.010472 critic2 loss eval: 0.010367/0.009983 \n",
      "[EVAL] actor loss eval: 0.017550/0.013646 \n",
      "[SUCCESS] save critic1 model!\n",
      "Epoch: 91/100 \n",
      "[TRAIN] critic1 loss: 0.014554, critic2 loss: 0.013016, actor Loss: 0.001835 \n",
      "[EVAL] critic1 loss eval: 0.011127/0.010204 critic2 loss eval: 0.010575/0.009983 \n",
      "[EVAL] actor loss eval: 0.016212/0.013646 \n",
      "Epoch: 92/100 \n",
      "[TRAIN] critic1 loss: 0.013408, critic2 loss: 0.012451, actor Loss: 0.001820 \n",
      "[EVAL] critic1 loss eval: 0.010892/0.010204 critic2 loss eval: 0.010011/0.009983 \n",
      "[EVAL] actor loss eval: 0.015838/0.013646 \n",
      "Epoch: 93/100 \n",
      "[TRAIN] critic1 loss: 0.015467, critic2 loss: 0.013435, actor Loss: 0.002692 \n",
      "[EVAL] critic1 loss eval: 0.010249/0.010204 critic2 loss eval: 0.009996/0.009983 \n",
      "[EVAL] actor loss eval: 0.016434/0.013646 \n",
      "Epoch: 94/100 \n",
      "[TRAIN] critic1 loss: 0.014014, critic2 loss: 0.012966, actor Loss: 0.002093 \n",
      "[EVAL] critic1 loss eval: 0.010469/0.010204 critic2 loss eval: 0.010145/0.009983 \n",
      "[EVAL] actor loss eval: 0.018415/0.013646 \n",
      "Epoch: 95/100 \n",
      "[TRAIN] critic1 loss: 0.013199, critic2 loss: 0.012800, actor Loss: 0.001884 \n",
      "[EVAL] critic1 loss eval: 0.010604/0.010204 critic2 loss eval: 0.009517/0.009983 \n",
      "[EVAL] actor loss eval: 0.016205/0.013646 \n",
      "[SUCCESS] save critic2 model!\n",
      "Epoch: 96/100 \n",
      "[TRAIN] critic1 loss: 0.014168, critic2 loss: 0.012757, actor Loss: 0.001898 \n",
      "[EVAL] critic1 loss eval: 0.010855/0.010204 critic2 loss eval: 0.012214/0.009517 \n",
      "[EVAL] actor loss eval: 0.015831/0.013646 \n",
      "Epoch: 97/100 \n",
      "[TRAIN] critic1 loss: 0.013511, critic2 loss: 0.012603, actor Loss: 0.001872 \n",
      "[EVAL] critic1 loss eval: 0.011680/0.010204 critic2 loss eval: 0.010598/0.009517 \n",
      "[EVAL] actor loss eval: 0.017813/0.013646 \n",
      "Epoch: 98/100 \n",
      "[TRAIN] critic1 loss: 0.013218, critic2 loss: 0.012324, actor Loss: 0.001841 \n",
      "[EVAL] critic1 loss eval: 0.011281/0.010204 critic2 loss eval: 0.010771/0.009517 \n",
      "[EVAL] actor loss eval: 0.016814/0.013646 \n",
      "Epoch: 99/100 \n",
      "[TRAIN] critic1 loss: 0.013446, critic2 loss: 0.012469, actor Loss: 0.001560 \n",
      "[EVAL] critic1 loss eval: 0.010064/0.010204 critic2 loss eval: 0.010912/0.009517 \n",
      "[EVAL] actor loss eval: 0.016342/0.013646 \n",
      "[SUCCESS] save critic1 model!\n",
      "Epoch: 100/100 \n",
      "[TRAIN] critic1 loss: 0.013458, critic2 loss: 0.012351, actor Loss: 0.001772 \n",
      "[EVAL] critic1 loss eval: 0.014342/0.010064 critic2 loss eval: 0.011123/0.009517 \n",
      "[EVAL] actor loss eval: 0.016659/0.013646 \n"
     ]
    }
   ],
   "source": [
    "agent.push_critic1.load_checkpoint()\n",
    "agent.push_critic2.load_checkpoint()\n",
    "agent.push_critic1_target.load_checkpoint()\n",
    "agent.push_critic2_target.load_checkpoint()\n",
    "agent.push_actor.load_checkpoint()\n",
    "\n",
    "agent.behaviour_cloning(push_train_loader, push_test_loader, \n",
    "                        agent.push_critic1, agent.push_critic2, \n",
    "                        agent.push_critic1_target, agent.push_critic2_target, \n",
    "                        agent.push_actor, num_epochs = 100, is_grasp = False)\n",
    "\n",
    "push_exp          = None\n",
    "push_train_loader = None\n",
    "push_test_loader  = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grasp Clone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N_grasp_exp: 0\n"
     ]
    }
   ],
   "source": [
    "grasp_exp = agent.buffer_replay.get_experience_by_action_type(constants.GRASP)\n",
    "print(f\"N_grasp_exp: {len(grasp_exp[0])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "num_samples should be a positive integer value, but got num_samples=0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 7\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# agent.grasp_critic1.load_checkpoint()\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# agent.grasp_critic2.load_checkpoint()\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# agent.grasp_critic1_target.load_checkpoint()\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# agent.grasp_critic2_target.load_checkpoint()\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# agent.grasp_actor.load_checkpoint()\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m grasp_train_loader, grasp_test_loader \u001b[38;5;241m=\u001b[39m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_train_test_dataloader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrasp_exp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_grasp\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_ratio\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.9\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m agent\u001b[38;5;241m.\u001b[39mbehaviour_cloning(grasp_train_loader, grasp_test_loader, \n\u001b[1;32m      9\u001b[0m                         agent\u001b[38;5;241m.\u001b[39mgrasp_critic1, agent\u001b[38;5;241m.\u001b[39mgrasp_critic2, \n\u001b[1;32m     10\u001b[0m                         agent\u001b[38;5;241m.\u001b[39mgrasp_critic1_target, agent\u001b[38;5;241m.\u001b[39mgrasp_critic2_target, \n\u001b[1;32m     11\u001b[0m                         agent\u001b[38;5;241m.\u001b[39mgrasp_actor, num_epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m750\u001b[39m, is_grasp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     13\u001b[0m grasp_exp          \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/github_repository/vision_based_synergies_between_closed_loop_grasping_and_pushing/agent.py:464\u001b[0m, in \u001b[0;36mAgent.get_train_test_dataloader\u001b[0;34m(self, exp, train_ratio, is_grasp)\u001b[0m\n\u001b[1;32m    461\u001b[0m train_subset \u001b[38;5;241m=\u001b[39m Subset(dataset, train_indices)\n\u001b[1;32m    462\u001b[0m test_subset  \u001b[38;5;241m=\u001b[39m Subset(dataset, test_indices)\n\u001b[0;32m--> 464\u001b[0m train_loader \u001b[38;5;241m=\u001b[39m \u001b[43mDataLoader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_subset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mN_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    465\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mdrop_last\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m>\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mN_batch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    466\u001b[0m test_loader  \u001b[38;5;241m=\u001b[39m DataLoader( test_subset, batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mN_batch, shuffle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    468\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m train_loader, test_loader\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_env/lib/python3.9/site-packages/torch/utils/data/dataloader.py:350\u001b[0m, in \u001b[0;36mDataLoader.__init__\u001b[0;34m(self, dataset, batch_size, shuffle, sampler, batch_sampler, num_workers, collate_fn, pin_memory, drop_last, timeout, worker_init_fn, multiprocessing_context, generator, prefetch_factor, persistent_workers, pin_memory_device)\u001b[0m\n\u001b[1;32m    348\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# map-style\u001b[39;00m\n\u001b[1;32m    349\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m shuffle:\n\u001b[0;32m--> 350\u001b[0m         sampler \u001b[38;5;241m=\u001b[39m \u001b[43mRandomSampler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgenerator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgenerator\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m    351\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    352\u001b[0m         sampler \u001b[38;5;241m=\u001b[39m SequentialSampler(dataset)  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_env/lib/python3.9/site-packages/torch/utils/data/sampler.py:143\u001b[0m, in \u001b[0;36mRandomSampler.__init__\u001b[0;34m(self, data_source, replacement, num_samples, generator)\u001b[0m\n\u001b[1;32m    140\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreplacement should be a boolean value, but got replacement=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreplacement\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    142\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_samples, \u001b[38;5;28mint\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_samples \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 143\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_samples should be a positive integer value, but got num_samples=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_samples\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: num_samples should be a positive integer value, but got num_samples=0"
     ]
    }
   ],
   "source": [
    "# agent.grasp_critic1.load_checkpoint()\n",
    "# agent.grasp_critic2.load_checkpoint()\n",
    "# agent.grasp_critic1_target.load_checkpoint()\n",
    "# agent.grasp_critic2_target.load_checkpoint()\n",
    "# agent.grasp_actor.load_checkpoint()\n",
    "\n",
    "grasp_train_loader, grasp_test_loader = agent.get_train_test_dataloader(grasp_exp, is_grasp = True, train_ratio=0.9)\n",
    "agent.behaviour_cloning(grasp_train_loader, grasp_test_loader, \n",
    "                        agent.grasp_critic1, agent.grasp_critic2, \n",
    "                        agent.grasp_critic1_target, agent.grasp_critic2_target, \n",
    "                        agent.grasp_actor, num_epochs = 750, is_grasp = True)\n",
    "\n",
    "grasp_exp          = None\n",
    "grasp_train_loader = None\n",
    "grasp_test_loader  = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
